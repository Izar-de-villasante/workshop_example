[
["index.html", "Learning R through examples Index", " Learning R through examples Xijin Ge 2019-01-16 Index Chapter 1 Step into R program Chapter 2 Visualizing data set Chapter 3 Data importing Chapter 4 Heart attack data set I Chapter 5 Heart attack data set II Chapter 6 Data structures Chapter 7 Vectors Chapter 8 Matrics and arrays Chapter 9 Lists Chapter 10 Data frames Chapter 11 Strings Chapter 12 Advanced topics Chapter 13 State data set Chapter 14 Game sale data set Chapter 15 Employee salary data set "],
["step-into-r-program-analyzing-iris-flower-dataset.html", "Chapter 1 Step into R program-Analyzing iris flower dataset 1.1 Data frames have rows and columns: the Iris flower dataset 1.2 Analyzing one set of numbers 1.3 Student’s t-test 1.4 Test for normal distribution 1.5 Analyzing a column of categorical values 1.6 Analyzing the relationship between two columns of numbers 1.7 Visualizing and testing the differences in groups 1.8 Testing the difference among multiple groups (Analysis of Variance: ANOVA)", " Chapter 1 Step into R program-Analyzing iris flower dataset Getting started Install R from www.R-project.org. Choose the cloud server or any U.S. mirror site. Install RStudio Desktop from www.RStudio.com. Rstudio uses the R software you just installed in the background, but provides a more user-friendly interface. We will use Rstudio. R commands can be typed directly into the “Console” window. Or you can enter them in the “R Script” window and click the “Run” button. Just try all of these commands and guess what’s going on. If it takes a few months to type these 188 characters, try www.RapidTyping.com. 1.1 Data frames have rows and columns: the Iris flower dataset In 1936, Edgar Anderson collected data to quantify the geographic variation of Iris flowers. The data set consists of 50 samples from each of three sub-species ( Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the lengths and the widths of sepals and petals, in centimeters (cm). This data is included in R software. Go to the Wikipedia page for this data set (yes, it is famous!). Have a quick look at the data there, think about what distinguishes the three species? If we have a flower with sepals of 6.5cm long and 3.0cm wide, petals of 6.2cm long, and 2.2cm wide, which species does it most likely belong to? Think (!) for a few minutes while eyeballing the data at Wikipedia. Figure 1.1: Iris flower. Photo from Wikipedia. Figure 1.2: Example of a data frame. To answer these questions, let’s visualize and analyze the data with R. Type these commands without the comments after “#”. iris #This will print the whole dataset, which is included with R dim(iris) # show the dimension of the data frame: 150 rows and 5 columns. head(iris) # show the first few rows; useful for bigger datasets. So the first 4 columns contain numeric values. The last one is species information as character values. This is an important distinction, as we cannot add or subtract character values. This object is a data frame, with both numeric and character columns. A matrix only contains one type of values, often just numbers. To have a look at the data in a spreadsheet, we can use the fix( ) function. #fix(iris) # examine data frame in a spreadsheet. Click on column names to double-check data types (numeric vs. character). Sometimes we need to overwrite data types guessed by R. For example, sometimes we use 1 for male and 0 for female. These are essentially categories; Values like 1.6 make no sense. In this case we need to enforce this column as characters. Note this window needs to be closed before proceeding to the next. View(iris) # this Rstudio function also shows data. Note R is case sensitive. Individual values in a data frame can be accessed using row and column indices. iris[3, 4] # shows the value in 3rd row, 4th column. It is 0.2. iris[3, ] # shows all of row 3 iris[, 4] # shows all of column 4 iris[3, 1:4] # shows row 3, columns 1 to 4. Exercise 1.1 Display data in rows 1 to 10 from columns 2 to 5. Start a new Word document. Copy and paste your R code and the results to the document and save it as PDF. colnames(iris) # Column names. ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [5] &quot;Species&quot; Remember these column names, as we are going to use them in our analysis now. Note that sepal length information is contained in the column named Sepal.Length. Since R is case sensitive, we have to type these column names exactly as above. attach(iris) # attach dataset to R working memory, so that columns can be accessible by name. Petal.Length # after attaching, we can just use column names to represent a column of numbers as a vector R is case-sensitive. “petal.length” will not be recognized. mean(Petal.Length) # mean( ) is a function that operates on Petal.Length, a vector of 150 umbers ## [1] 3.758 Exercise 1.2 Compute average sepal length. Hint: replace Petal.Length with Sepal.Length. The best way to learn about other R functions is Google search. Exercise 1.3 Google “R square root function” to find the R function, and compute the value of \\(\\sqrt(123.456)\\). 1.2 Analyzing one set of numbers x &lt;- Petal.Length # I am just lazy and don’t want to type “Petal.Length”, repeatedly. summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 4.350 3.758 5.100 6.900 The minimum petal length is 1.0, and the maximum is 6.9. Average petal length is 3.758. The mid-point or median is 5.35, as about half of the numbers is smaller than 5.35. Why the median is different from the mean? What happens if there is a typo and one number is entered 340cm instead of 3.40cm? The 3rd quartile, or 75th percentile is 5.1, as 75% of the flowers has petals shorter than 5.1. The 95th percentile for the weight of 2-year-old boy is 37 pounds. If a 2-year-old boy weighs 37 pounds, he is heavier than 95% of his peers. If a student’s GPA ranks 5th in a class of 25, he/she is at 80th percentile. The 1st quartile, or 25th percentile is 1.6. Only 25% of the flowers has petals shorter than 1.6. These summary statistics are graphically represented as a boxplot in the Figure 1.3A. Boxplots are more useful when multiple sets of numbers are compared. boxplot(x) # Figure 1.3A. It graphically represents the spread of the data. boxplot(iris[, 1:4]) # boxplot of several columns at the same time Figure 1.3B. Figure 1.3: Boxplot of petal length (A) and of all 4 columns (B). In Rstudio, you can copy a plot to clipboard using the Export button on top of the plot area. Or you can click zoom, right click on the popup plot and select “Copy Image”. Then you can paste the plot into Word. If you are using R software, instead of Rstudio, you can right click on the plots and copy as meta-file. Exercise 1.4 What can we tell from this boxplot (Figire 1.3B)? Summarize your observations in PLAIN English. Note the differences in median and the spread (tall boxes). What is the most variable characteristics? To quantify the variance, we can compute the standard deviation σ: \\[\\begin{align} σ=\\sqrt{\\frac{1}{N}[(x_{1}-u)^2+(x_{2}-u)^2+...+(x_{N}-u)^2]} \\end{align}\\] where \\[\\begin{align} u=\\frac{1}{N}(x_{1}+x_{2}+...x_{N}) \\end{align}\\] If all the measurements are close to the mean (µ), then standard deviation should be small. sd(x) # sd( ) is a function for standard deviation ## [1] 1.765298 sd(Sepal.Width) ## [1] 0.4358663 As we can see, these flowers have similar sepal width. They differ widely in petal length. This is consistent with the boxplot above. Perhaps changes in petal length lead to better survival in different habitats. With R it is very easy to generate graphs. barplot(x) Figure 1.4: Barplot of petal length As we can see, the first 50 flowers (Iris setosa) have much shorter petals than the other two species. The last 50 flowers (Iris verginica) have slightly longer petals than the middle (iris versicolor). plot(x) # Run sequence plot hist(x) # histogram lag.plot(x) qqnorm(x) # Q-Q plot for normal distribution qqline(x) Figure 1.5: Sequence plot, histogram, lag plot and normal Q-Q plot. Histogram shows the distribution of data. The histogram top right of Figure 1.5 shows that there are more flowers with Petal Length between 1 and 1.5. It also shows that the data does not show a bell-curved distribution. Lag plot is a scatter plot against the same set of number with an offset of 1. Any structure in lag plot indicate non-randomness in the order in which the data is presented. Q-Q plot can help check if data follows a Gaussian distribution, which is widely observed in many situations. Also referred to as normal distribution, it is the pre-requisite for many statistical methods. See Figure 1.6 for an example of normal distribution. Quantiles of the data is compared against those in a normal distribution. If the normal Q-Q plot is close to the reference line produced by qqline( ), then the data has a normal distribution. Exercise 1.5 Generate 500 random numbers from the standard normal distribution and generate sequence plots, histogram, lag plot, and Q-Q plot. You should get plots like those in Figure 1.6. Hint: Run x = rnorm(500) to generate these numbers, and then re-run the last 5 lines of code on this page. They are also shown on Figure 1.5. (ref:1-5) Plots for random generated numbers following a normal distribution. This is for reference. Figure 1.6: (ref:1-5) Exercise 1.6 Investigate Septal Length distribution using these techniques and summarize your observations in PLAIN English. Hint: assign x with the values in the sepal length column (x = Sepal.Length), and re-run all the code in this section. 1.3 Student’s t-test In hypothesis testing, we evaluate how likely the observed data can be generated if a certain hypothesis is true. If this probability (p value) is very small (&lt; 0.05, typically), we reject that hypothesis. Are the petal lengths of iris setosa significantly different from these of iris versicolor? x &lt;- Petal.Length[1:50] # the first 50 values of Sepal.Length are for iris setosa y &lt;- Petal.Length[51:100] # the next 50 values of Sepal.Length are for iris versicolor t.test(x, y) # t.test( ) is an R function for student’s t-test ## ## Welch Two Sample t-test ## ## data: x and y ## t = -39.493, df = 62.14, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.939618 -2.656382 ## sample estimates: ## mean of x mean of y ## 1.462 4.260 The null hypothesis is that the true mean is the same. Since p value is really small, we reject this hypothesis. Iris versicolor has longer sepals than iris setosa. Exercise 1.7 Are iris setosa and iris versicolor significantly different in sepal width? Hint: Replace Petal.Length with something else and re-run above code. We can also do t-test on one set of numbers. This is a one-sample t-test of mean: t.test(Sepal.Length, mu = 5.8) ## ## One Sample t-test ## ## data: Sepal.Length ## t = 0.64092, df = 149, p-value = 0.5226 ## alternative hypothesis: true mean is not equal to 5.8 ## 95 percent confidence interval: ## 5.709732 5.976934 ## sample estimates: ## mean of x ## 5.843333 In this case, our hypothesis is that the true average of sepal length for all iris flowers is 5.8. Since p value is quite big, we accept this hypothesis. This function also tells us the 95% confidence interval on the mean. Based on our sample of 150 iris flowers, we are 95% confident that the true mean is between 5.71 and 5.98. Exercise 1.8 Compute 95% confidence interval of petal length. 1.4 Test for normal distribution We can perform hypothesis testing on whether a set of numbers derived from normal distribution. The null hypothesis is that the data is from a normal distribution. shapiro.test(Petal.Length) ## ## Shapiro-Wilk normality test ## ## data: Petal.Length ## W = 0.87627, p-value = 7.412e-10 If petal length is normally distributed, there is only 7.412×10-10 chance of getting a test statistic of 0.87627, which is observed in our sample of 150 flowers. In other words, it is highly unlikely that petal length follows a normal distribution. We reject the normal distribution hypothesis. Exercise 1.9 Is sepal width normally distributed? Run Shapiro’s test and also generate histogram and normal Q-Q plot. 1.5 Analyzing a column of categorical values In the iris dataset, the last column contains the species information. These are “string” values or categorical values. counts &lt;- table(Species) # tabulate the frequencies counts ## Species ## setosa versicolor virginica ## 50 50 50 pie(counts) # See Figure 1.7A barplot(counts) # See Figure 1.7B Figure 1.7: Frequencies of categorical values visualized by Pie chart (A) and bar chart (B). Pie charts are very effective in showing proportions. We can see that the three species are each represented with 50 observations. 1.6 Analyzing the relationship between two columns of numbers Scatter plot is very effective in visualizing correlation between two columns of numbers. attach(iris) # attach the data set x &lt;- Petal.Width # just lazy y &lt;- Petal.Length plot(x, y) # scatterplot, refined version in Figure 1.9 Figure 1.8: Scatter plot of petal width and petal length. Figure 1.8 shows that there is a positive correlation between petal length and petal width. In other words, flowers with longer petals are often wider. So the petals are getting bigger substantially, when both dimensions increase. Another unusual feature is that there seems to be two clusters of points. Do the points in the small cluster represent one particular species of Iris? We need to further investigate this. The following will produce a plot with the species information color-coded. The resultant Figure 1.9 clearly shows that indeed one particular species, I. setosa constitutes the smaller cluster in the low left. The other two species also show difference in this plot, even though they are not easily separated. This is a very important insight into this dataset. plot(x, y, col = rainbow(3)[Species]) # change colors based on another column (Species). legend(&quot;topleft&quot;, levels(Species), fill = rainbow(3)) # add legends on topleft. Figure 1.9: Scatter plot shows the correlation of petal width and petal length. The rainbow( ) function generates 3 colors and Species information is used to choose colors. Note that Species column is a factor, which is a good way to encode columns with multiple levels. Internally, it is coded as 1, 2, 3. str(iris) # show the structure of data object ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Perhaps due to adaption to environment, change in petal length lead to better survival. With the smallest petals, Iris Setosa is found in Arctic regions. Iris versicolor is often found in the Eastern United States and Eastern Canada. Iris virginica “is common along the coastal plain from Florida to Georgia in the Southeastern United States [Wikipedia].” It appears the iris flowers in warmer places are much larger than those in colder ones. With R, it is very easy to generate lots of graphics. But we still have to do the thinking. It requires to put the plots in context. We can quantitatively characterize the strength of the correlation using several types of correlation coefficients, such as Pearson’s correlation coefficient, r. It ranges from -1 to 1. cor(x, y) ## [1] 0.9628654 This means the petal width and petal length are strongly and positively correlated. cor.test(x, y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 43.387, df = 148, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9490525 0.9729853 ## sample estimates: ## cor ## 0.9628654 Through hypothesis testing of the correlation, we reject the null hypothesis that the true correlation is zero. That means the correlation is statistically significant. Note that Pearson’s correlation coefficient is not robust against outliers and other methods such as Spearman’s exists. See help info: ?cor # show help info on cor ( ) We can also determine the equation that links petal length and petal width. This is so called regression analysis. We assume Petal.Length = a × Petal.Width + c + e, where a is the slope parameter, c is a constant, and e is some random error. This linear model can be determined by a method that minimizes the least squared-error: model &lt;- lm(y ~ x) # Linear Model (lm): petal length as a function of petal width summary(model) # shows the details ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.33542 -0.30347 -0.02955 0.25776 1.39453 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.08356 0.07297 14.85 &lt;2e-16 *** ## x 2.22994 0.05140 43.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4782 on 148 degrees of freedom ## Multiple R-squared: 0.9271, Adjusted R-squared: 0.9266 ## F-statistic: 1882 on 1 and 148 DF, p-value: &lt; 2.2e-16 As we can see, we estimated that a=2.22944 and c=1.08356. Both parameters are significantly different from zero as the p values are &lt;2×10-16 in both cases. In other words, we can reliably predict Petal.Length = 2.22944 × Petal.Width + 1.08356. This model can be put on the scatter plot as a line. plot(model) abline(model) # add regression line to existing scatter plot. Finishes Figure 1.8. Sometimes, we use this type of regression analysis to investigate whether variables are associated. Exercise 1.10 Investigate the relationship between sepal length and sepal width using scatter plots, correlation coefficients, test of correlation, and linear regression. Again interpret all your results in PLAIN and proper English. 1.7 Visualizing and testing the differences in groups Are boys taller than girls of the same age? Such situations are common. We have measurements of two groups of objects and want to know if the observed differences are real or due to random sampling error. attach(iris) # attach iris data boxplot(Petal.Length ~ Species) # Generate boxplot: Petal length by species, see Figure 1.11 Figure 1.10: Boxplot of petal length, grouped by species. From the boxplot, it is obvious that I. Setosa has much shorter petals. But are there significant differences between I. versicolor and I. virginica? We only had a small sample of 50 flowers for each species. But we want to draw some conclusion about the two species in general. We could measure all the iris flowers across the world; Or we could use statistics to make inference. First we need to extract these data x &lt;- Petal.Length[51:100] # extract Petal Length of iris versicolor, from No.51 to No.100 x # x contain 50 measurements y &lt;- Petal.Length[101:150] # extract Petal length of iris virginica, from No. 101 to No. 150 y # y contain 50 measurements boxplot(x, y) # a boxplot of the two groups of values t.test(x, y) ## ## Welch Two Sample t-test ## ## data: x and y ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean of x mean of y ## 4.260 5.552 In this student’s t-test, our null hypothesis is that the mean petal length is the same for I. versicolor and I. virginica. A small p value of 2.2x10-16 indicates under this hypothesis, it is extremely unlikely to observe the difference of 1.292cm through random sampling. Hence we reject that hypothesis and conclude that the true mean is different. If we measure all I. versicolor and I. virginica flowers in the world and compute their true average petal lengths, it is very likely that the two averages will differ. On the other hand, if p value is larger than a threshold, typically 0.05, we will accept the null hypothesis and conclude that real average petal length is the same. We actually do not need to separate two set of numbers into two data objects in order to do t-test or compare them side-by-side on one plot. We can do it right within the data frame. R can separate data points by another column. x2 &lt;- iris[51:150, ] # Extract rows 51 to 150 t.test(Petal.Length ~ Species, data = x2) # t-test of Petal.Length column, divided by the Species column in x2. boxplot(Petal.Length ~ Species, data = droplevels(x2)) # droplevels( ) removes empty levels in Species Exercise 1.11 Use boxplot and t-test to investigate whether sepal width is different between I. versicolor and I. virginica. Interpret your results. 1.8 Testing the difference among multiple groups (Analysis of Variance: ANOVA) As indicated by Figure 1.10, sepal width has small variation, even across 3 species. We want to know if the mean sepal width is the same across 3 species. This is done through Analysis of Variance (ANOVA). boxplot(Sepal.Width ~ Species) # Figure 1.12 Figure 1.11: Boxplot of sepal width across 3 species. summary(aov(Sepal.Width ~ Species)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since p value is much smaller than 0.05, we reject the null hypothesis. The mean sepal width is not the same for 3 species. This is the only thing we can conclude from this. The boxplot in Figure 1.11 seems to indicate that I. Setosa has wider sepals. Now we demonstrate how to use the lattice package for visualizing data using multiple panels. This package is not included in the base version of R. They need to be downloaded and installed. One of main advantages of R is that it is open, and users can contribute their code as packages. If you are using Rstudio, you can choose Tools-&gt;Install packages from the main menu, and then enter the name of the package. If you are using R software, you can install additional packages, by clicking Packages in the main menu, and select a mirror site. These mirror sites all work the same, but some may be faster. Lately I just use cloud mirror. After choosing a mirror and clicking “OK”, you can scroll down the long list to find your package. Alternatively, you can type this command to install packages. #install.packages (&quot;ggplot2&quot;) # choose the cloud mirror site when asked Packages only need to be installed once. But every time you need to use a package, you need to load it from your hard drive. library(ggplot2) # load the ggplot2 package Figure 1.12A shows the sepal width distribution for each of the 3 species. The 3 histograms are aligned in panels: layout of 1 column and 3 rows. Figure 1.12A shows an interesting feature in how sepal width is distributed. While for I. setosa it is skewed to the right. For the other two species it is skewed to the left. We can also have density plots side-by-side (Figure 1.12B): ggplot(iris, aes(x = Sepal.Width, group = Species, y = c(..count..[..group.. == 1]/sum(..count..[..group.. == 1]), ..count..[..group.. == 2]/sum(..count..[..group.. == 2]), ..count..[..group.. == 3]/sum(..count..[..group.. == 3])) * 100)) + geom_histogram(binwidth = .2, colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + facet_grid(Species ~ .) + labs(y = &quot;Percent of Total&quot;) ggplot(iris, aes(x = Petal.Length, fill = Species)) + geom_density(alpha = .3) Figure 1.12: Visualizing data using the ggplot2 package. Exercise 1.12 Use boxplot, multiple panel histograms and density plots to investigate whether petal width is the same among three subspecies. "],
["visualizing-data-set-analyzing-cars-and-iris-flower-data-sets.html", "Chapter 2 Visualizing data set-Analyzing cars and iris flower data sets 2.1 Basic concepts of R graphics 2.2 Visualizing mtcars dataset 2.3 Visualizing iris data set", " Chapter 2 Visualizing data set-Analyzing cars and iris flower data sets 2.1 Basic concepts of R graphics In addition to the graphics functions in base R, there are many other packages we can use to create graphics. The most widely used are lattice and ggplot2. Together with base R graphics, sometimes these are referred to as the three independent paradigms of R graphics. The lattice package extends base R graphics and enables the creating of graphs in multiple facets. The ggplot2 is developed based on a so-called Grammar of Graphics (hence the “gg”), a modular approach that builds complex graphics using layers. Note the recommended textbook R Graphics Cookbook includes all kinds of R plots and code. Some are online: http://www.cookbook-r.com/Graphs/. There are also websites lists all sorts of R graphics and example codes that you can use. http://www.r-graph-gallery.com/ contains more than 200 such examples. Another one is here: http://bxhorn.com/r-graphics-gallery/ We start with base R graphics. The first import distinction should be made about high- and low-level graphics functions in base R. See this table. Figure 2.1: List of graphics functions in base R. Sometimes we generate many graphics quickly for exploratory data analysis (EDA) to get some sense of how the data looks like. We can achieve this by using plotting functions with default settings to quickly generate a lot of “plain” plots. R is a very powerful EDA tool. However, you have to know what types of graphs are possible for the data. Other times, we want to generate really “cool”-looking graphics for papers, presentations. Making such plots typically requires a bit more coding, as you have to add different parameters easily understood. For me, it usually involves some google searches of example codes, and then I revise it via trial-and-error. If I cannot make it work, I read the help document. 2.2 Visualizing mtcars dataset 2.2.1 scatter plot The mtcars data set is included in base R. It contains various statistics on 32 different types of cars from the 1973-74 model year. The data was obtained from the 1974 Motor Trend US magazine. Our objective is to use this dataset to learn the difference between them, possibly for choosing a car to buy. mtcars # show the mtcars dataset ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ? mtcars # shows the information on this dataset 2.2.1.1 Customize scatterplots We start with a basic scatter plot. First, we attach the mtcars data to memory so that we can refer to the columns directly by their names. attach(mtcars) # attach dataset to memory ## The following object is masked from package:ggplot2: ## ## mpg plot(wt, mpg) # weight (wt) and miles per gallon (mpg), see Figure 2.3 This generates a basic scatter plot with default settings using wt as x and mpg as y. Each data points are represented as an open circle on the plot. As you could see, heavier vehicles are less fuel efficient. We can add a regression line on this scatter plot using a lower-level graphics function abline: abline(lm(mpg ~ wt)) # add regression line Note that lm(mpg ~ wt) generates a linear regression model of mpg as a function of wt, which is then passed on to abline. We add other information about these cars to customize this plot. plot(wt, mpg, pch = am) # am = 1 for automatic transmission “pch” is a parameter that specifies the types of data points on the plot. See Figure 2.2 for a whole list of possible values. The “am” column in mtcars dataset indicates whether the car is automatic transmission (am = 1) or not (am = 0). Figure 2.2: Data point types in base R. am ## [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 So R uses circles or squares according to this sequence for each of the data points. It draws a circle when am value is 1, and square when it is zero. See all the types in Figure 2.2. We add a legend to the top-right corner using a low-level graphics function legend: legend(&quot;topright&quot;, c(&quot;Automatic&quot;, &quot;Manual&quot;), pch = 0:1) This plot shows that heavier cars often use manual transmissions. Always slow down and interpret your plots in plain language. We can be happy about this plot, but we continue to fuss in more information on this graph. Using the same line of thinking, we can change the color of the data points according to other information, i.e. the number of cylinders. plot(wt, mpg, pch = am, col = rainbow(8)[cyl]) The rainbow(8) generates a vector of 8 colors. The values in the cyl column are used to choose the color from these 8 colors. Green, blue, and red, indicates 4, 6, and 8 cylinders, respectively. Now we alter the size of the data points to represent additional information such as horsepower, hp. Since hp is often a big number, we divide it by 50, a ratio determined by trial-and-error. These sometimes are called bubble plots or balloon plots. plot(wt, mpg, pch = am, col = rainbow(8)[cyl], cex = hp / 50) legend(5, 30, c(&quot;4&quot;, &quot;6&quot;, &quot;8&quot;), title = &quot;Cylinders&quot;, pch = 15, col = rainbow(8)[c(4, 6, 8)]) Note that we added this legend at the (5, 30) position on the plot. To see all the options: ? plot This website lists all the parameters for R graphics: http://www.statmethods.net/advgraphs/parameters.html. Now we want to finish up this plot by adding axis labels, and title. We also changed the x-axis range to 1-6 using the xlim parameter to specify the limits. We finally put everything together. Figure 2.3: Enhanced scatter plot of mtcars data. Also called bubble plot or balloon plot. Note that this seemingly complicated chunk of code is built upon many smaller steps, and it involves trial-and-error. Exercise 2.1 Create a scatter plot similar to Figure 2.3 using the mtcars dataset to highlight the correlation between hp and disp (displacement). You should use colors to represent carb ( # of carburetors), types of data points to denote the number of gears, and size of the data points proportional to qsec, the number of seconds the cars need run the first ¼ mile. Add regression line and legends. Note that you should include comments and interpretations. Submit your code and plot in a PDF file. Hint: Go through the above example first. Then start small and add on step by step. Figure 2.3 is perhaps a bit too busy. Let’s develop an elegant version. plot(wt, mpg, pch = 16, cex = hp / 50, col = rainbow(8)[cyl]) Notes: x; y; solid circle; horsepowersize of bubble; color cylinder 4, 6, or 8 Then we use a lower-level graphics function points to draw circles around each data point. Figure 2.4: Scatter plot showing the weight and MPG, colored by the number of cylinders. A line at mpg = 20 separates the 4-cylinder cars from 8 cylinder cars. This line adds a LOWESS smooth line determined by locally-weighted polynomial regression. Exercise 2.2 Generate the bubble plot Figure 2.4 based on the code:“plot(wt, mpg, pch = 16, cex = hp / 50, col = rainbow(8)[cyl])”. Submit your complete code and the generated plot. Give a brief interpretation about your plot. 2.2.1.2 3D scatter plot Even though I’ve never been a fan of 3D plots, it is possible in R using additional packages such as scatterplot3d. Install this package and then try the following. library(scatterplot3d) scatterplot3d(wt, disp, mpg, color = rainbow(8)[cyl], type = &quot;h&quot;, pch = 20) Figure 2.5: 3D scatter plots are rarely useful. 3D plots are hard to interpret. So try to avoid them. However, it is fun when you can interact with them. Using example code at this website, you can create interactive 3D plots: http://www.statmethods.net/graphs/scatterplot.html 1 I like to work directly from my Google Drive, which automatically backs up my files in the cloud and syncs across several of my computers. This is an insurance against disasters like my dog peeing on my computer and ruins my grant proposal just before the deadline. 2 Note that I was trying to avoid having spaces in column names. Instead of “Blood Pressure”, I used “BloodPressure”. This makes the columns easier to reference to. 2.2.2 Barplot with error bars If we are interested in the difference between the cars with different numbers of cylinders. The 32 models are divided into 3 groups as cyl takes the values of 4, 6, or 8. We can use the aggregate function to generate some statistics by group. stats &lt;- aggregate(. ~ cyl, data = mtcars, mean) stats ## cyl mpg disp hp drat wt qsec vs ## 1 4 26.66364 105.1364 82.63636 4.070909 2.285727 19.13727 0.9090909 ## 2 6 19.74286 183.3143 122.28571 3.585714 3.117143 17.97714 0.5714286 ## 3 8 15.10000 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000 ## am gear carb ## 1 0.7272727 4.090909 1.545455 ## 2 0.4285714 3.857143 3.428571 ## 3 0.1428571 3.285714 3.500000 This tells R to divide the cars into different groups by cyl and compute the average of all other columns for each group. The results above indicate many differences. As the number of cylinders increase, fuel efficiency measured by mpg decreases, while displacement and horsepower increases. We can obviously create a basic bar chart to show the difference in mpg. barplot(stats[, 2]) # basic bar plot The labels are missing from this basic plot. It is certainly possible to add the labels, but it is more convenient to use the tapply function, which generates a vector with names. We use tapply to calculate the mean, standard deviation (sd) and the numbers of samples for each group. Means &lt;- tapply(mpg, list(cyl), mean) # Average mpg per group Means ## 4 6 8 ## 26.66364 19.74286 15.10000 Note that it generates a vector and “4”, “6”, and “8” are names of each of the elements in this vector. tapply applies a function to a vector (mpg) according to grouping information defined by a factor (cyl) of the same length. Here it first groups the mpg numbers into 3 groups (cly= 4, 6, 8), and then within each group, the mean is calculated and returned. tapply is a member of a family of functions which includes apply, sapply, and lapply; all are powerful and efficient in computing than loops. Similarly, we can compute the standard deviation for each group. SDs &lt;- tapply(mpg, list(cyl), sd) # SD per group for mpg SDs ## 4 6 8 ## 4.509828 1.453567 2.560048 Now we can have a basic barplot with group names: barplot(Means) Our goal is to generate a graph like Figure 2.6 with both error bars and text annotation on the number of samples per group. We use two low-level graphics functions to add these elements to the plot, namely, text, and arrow. The text function adds any text to a plot to a position specified by x, y coordinates. Let’s try it. text(0.5, 5, &quot;n=11&quot;) # adding text to plot The (0.5, and 5) are the x and y location of the text information. Try to change it to something else within the plotting range, meaning x within 0 to 3 and y between 0 and 25. You can place any text anywhere. We added sample size information for the first group. We can choose to do this for each of bar manually, but obvious there should be a better way to do this. The trick is to find the precise location of all the bars and place the text there, hopefully doing this once for all of them. To achieve this, we use the values returned by the barplot object. xloc &lt;- barplot(Means) # get locations of the bars xloc # the center of each of bars on x ## [,1] ## [1,] 0.7 ## [2,] 1.9 ## [3,] 3.1 Yes, plotting functions not only generate graphs, they can also returns values. These values sometimes are useful in computing or refining the graph. Try this: h &lt;- hist( rnorm(100) ) and then type h to see the values returned by hist function. In our barplot case, we got an object containing the location of the center of the bars on x-axis. So the first bar is located on x=0.7. Since xloc has the location on all bars, we can add the information all at once: Nsamples &lt;- tapply(mpg, list(cyl), length) #number of samples per group Nsamples ## 4 6 8 ## 11 7 14 text(xloc, 2, Nsamples) # add sample size to each group The xloc specifies the center of the bars on the x-axis, namely 07, 1.9, and 3.1. The y coordinates are all 2. Try change the y location from 2 to 10, and see what happens. Looking great! The sample sizes are labeled on all the bars! This method works even if you have 20 bars! Now we want to make it explicit that these numbers represent sample size. For the first bar, we want “n=11”, instead of just “11”. First, we will append “n=” to each number and generate a string vector using the paste function paste(&quot;n=&quot;, Nsamples) # creating the strings to be added to the plot ## [1] &quot;n= 11&quot; &quot;n= 7&quot; &quot;n= 14&quot; text(xloc, 2, paste(&quot;n=&quot;, Nsamples)) # add sample size to each group Following a similar strategy, now we want to add error bars to represent standard deviations (SD) within each group. The plot is more informative as we visualize both the mean and variation within groups. For each bar, we need to draw an error bar from mean – SD to mean + SD. Let’s play with the arrows function, which draws arrows on plots. arrows(1, 15, # x,y of the starting point 1, 25) # x,y of the ending point arrows(2, 15, 2, 25, code = 3) # arrows on both ends arrows(3, 10, 3, 20, code = 3, angle = 90) # bend 90 degrees, flat Now it’s beginning to look a lot like Christmas (error bar)! I learned this clever hack of arrows as error bars from (Beckerman 2017)1. We are ready to add the error bars using the data stored in xloc, Means and SDs. barplot(Means) # re-create bar plot arrows(xloc, Means - SDs, # define 3 beginning points xloc, Means + SDs, # define 3 ending points code = 3, angle = 90, length = 0.1) Yes, we have a bar plot with error bars! We need to add a few refinements now such as colors, labels, and a title. As the first error bar is truncated, we need to adjust the range for y, by changing the ylim parameter. Putting everything together, we get this code chuck. attach(mtcars) # attach data, two columns: numbers and groups ## The following objects are masked from mtcars (pos = 4): ## ## am, carb, cyl, disp, drat, gear, hp, mpg, qsec, vs, wt ## The following object is masked from package:ggplot2: ## ## mpg Means &lt;- tapply(mpg, list(cyl), mean) #compute means by group defined by cyl SDs &lt;- tapply(mpg, list(cyl), sd) # calculate standard deviation by group Nsamples &lt;- tapply(mpg, list(cyl), length) # number of samples per group xloc &lt;- barplot(Means, # bar plot, returning the location of the bars xlab = &quot;Number of Cylinders&quot;, ylab = &quot;Average MPG&quot;, ylim = c(0,35), col = &quot;green&quot;) arrows (xloc, Means - SDs, # add error bars as arrows xloc, Means + SDs, code = 3, angle = 90, length = 0.1) text(xloc, 2, paste(&quot;n=&quot;, Nsamples)) # add sample size to each group Figure 2.6: Bar chart with error bar representing standard deviation. Sometimes we want error bars to represent standard error instead which is given by σ/√n, where σ is standard deviation and n is sample size. In R we can do complex statistical tests or regression analyses with just one line of code, for example, aov, glm. However, we went through all this trouble to generate just a bar chart! What is the point? We could click around in sigmaPlot, or GraphPad and get a barplot in less time. Well, once you figured how to do one, you can easily do this for 10 graphs. More importantly, this code clearly recorded the plotting process, which is essential for reproducible research. Exercise 2.3 Revise the above relative code to generate a bar chart showing the average weight of cars with either automatic or manual transmission. Include error bars, the number of samples and axis labels. Exercise 2.4 Create a bar chart with error bars to show the average illiteracy rate by region, using the illiteracy rate in the state.x77 data and regions defined by state.region in the state data set. Hints: 1, Check the type of data set using class(state.x77); 2, Convert the matrix dataset to data frame using df.state.x77 &lt;- as.data.frame(state.x77); 3, Attach df.state.x77. 2.2.3 Visualizing correlation between categorical variables If we are interested in the correlation between two categorical variables, we can tabulate the frequencies from a data frame: counts &lt;- table(cyl, am) This contingency table gives us the number of cars in each combination. Among the 8-cylinder cars, there are 12 models with manual transmission, and only 2 models have automatic transmission. We obviously can feed this into a fisher’s exact test to test for independence. We could easily visualize this information with a bar chart. barplot(counts) We generated a stacked barplot. Let’s refine it. We have a plot like the left of Figure 2.7. We can also put the bars side-by-side, by setting the beside option to TRUE. barplot(counts, col = rainbow(3), xlab =&quot;Transmission&quot;, ylab = &quot;Number of cars&quot;) legend(&quot;topright&quot;,rownames(counts), pch = 15, title = &quot;Cylinders&quot;, col = rainbow(3)) barplot(counts, col = rainbow(3), xlab = &quot;Transmission&quot;, ylab = &quot;Number of cars&quot;, beside = TRUE) legend(&quot;topright&quot;,rownames(counts), pch = 15, title = &quot;Cylinders&quot;, col = rainbow(3)) Figure 2.7: Bar plots showing the proportion of cars by cylinder and transmission. See the right side of Figure 2.7. Given a large dataset, we can easily tabulate categorical variables and plot these to show the relative frequencies. Another easy plot is the mosaic plot: mosaicplot(counts, col = c(&quot;red&quot;, &quot;green&quot;)) Figure 2.8: Mosaic plot. Vertically, we divide the square into 3 parts, the area of each is proportional to the number of cars with different cylinders. There are more 8-cylinder vehicles than those with 4 or 6. Horizontally, we divide the square according to the am variable, which represents automatic transmission (am =0) or manual transmission. Clearly, these two are not independent. As the number of cylinder increases, more cars are using manual transmission. While the height of the bars in the bar chart in Figure 5 represents absolute totals per category, in mosaic plots the height are equal. Thus we see proportions within each category. Exercise 2.5 Use bar plot and mosaic plot to investigate the correlation between cyl and gear. Interpret your results. 2.2.4 Detecting correlations among variables In ther beginning of this chapter we used scatter plots to study the correlation between two variables, mpg and wt in the mtcars dataset. There are many such pairwise correlations. One simple yet useful plot of the entire dataset is scatter plot matrix (SPM). SPMs can be created by the pairs function, or just run plot on a data frame. plot(mtcars) # scatter plot matrix; same as pairs(mtcars) Figure 2.9: Scatter plot matrix of the mtcars dataset. We can spend all day studying this large plot, as it contains information on all pairs of variables. For example, mpg is negatively correlated with disp, hp, and wt, and positively correlated with drat. There are many variations of scatter plot matrix, for instance, the spm function in the car package. Also try this cool plot using ellipses: http://www.r-graph-gallery.com/97-correlation-ellipses/ library(ellipse) # install.packages(&quot;ellipses&quot;) library(RColorBrewer) # install.packages(&quot;RcolorBrewer&quot;) data &lt;- cor(mtcars) # correlation matrix my_colors &lt;- brewer.pal(5, &quot;Spectral&quot;) # Color Pannel my_colors &lt;- colorRampPalette(my_colors)(100) ord &lt;- order(data[1, ]) # Order the correlation matrix data_ord &lt;- data[ord, ord] plotcorr(data_ord, col = my_colors[data_ord * 50 + 50], mar = c(1, 1, 1, 1)) Figure 2.10: Scatter plot matrix of the mtcars dataset. Exercise 2.6 Generate a scatter plot matrix of the state.x77 data in the state data set included in base R. It includes various statistics on the 50 U.S. states. Type ? state for more information and type state.x77 to see the data. Also, visualize the correlation using the ellipses shown above. Interpret your results. What types of correlation do you find interesting? Hint: The class(state.x77) should be “matrix” not “data frame”, otherewise convert it to a “matrix”. If you examine the above code carefully, the ellipses are drawn just based on a matrix of Pearson’s correlation coefficients. We can easily quantify the relationship between all variables by generating a matrix of Pearson’s correlation coefficient: cor(mtcars) # correlation coefficient of all columns corMatrix &lt;- cor(mtcars[, 1:11]) round(corMatrix, 2) # Round to 2 digits ## mpg cyl disp hp drat wt qsec vs am gear carb ## mpg 1.00 -0.85 -0.85 -0.78 0.68 -0.87 0.42 0.66 0.60 0.48 -0.55 ## cyl -0.85 1.00 0.90 0.83 -0.70 0.78 -0.59 -0.81 -0.52 -0.49 0.53 ## disp -0.85 0.90 1.00 0.79 -0.71 0.89 -0.43 -0.71 -0.59 -0.56 0.39 ## hp -0.78 0.83 0.79 1.00 -0.45 0.66 -0.71 -0.72 -0.24 -0.13 0.75 ## drat 0.68 -0.70 -0.71 -0.45 1.00 -0.71 0.09 0.44 0.71 0.70 -0.09 ## wt -0.87 0.78 0.89 0.66 -0.71 1.00 -0.17 -0.55 -0.69 -0.58 0.43 ## qsec 0.42 -0.59 -0.43 -0.71 0.09 -0.17 1.00 0.74 -0.23 -0.21 -0.66 ## vs 0.66 -0.81 -0.71 -0.72 0.44 -0.55 0.74 1.00 0.17 0.21 -0.57 ## am 0.60 -0.52 -0.59 -0.24 0.71 -0.69 -0.23 0.17 1.00 0.79 0.06 ## gear 0.48 -0.49 -0.56 -0.13 0.70 -0.58 -0.21 0.21 0.79 1.00 0.27 ## carb -0.55 0.53 0.39 0.75 -0.09 0.43 -0.66 -0.57 0.06 0.27 1.00 We used the round function to keep two digits after the decimal point. We can examine the coefficients in this matrix. Note that strong negative correlations are also interesting. For example, wt and mpg have a correlation of r= -0.87, meaning that heavier vehicles tend to have smaller mpg. We can visualize this matrix in other ways besides the ellipses. The most logical thing to do with a matrix of correlation coefficients is to generate a tree using hierarchical clustering. plot(hclust(as.dist(1 - corMatrix))) # hierarchical clustering Figure 2.11: Hierarchical clustering tree. Here we first subtracted the r values from 1 to define a distance measure. So perfectly correlated variables with r = 1 have a distance of 0, while negatively correlated variables with r = -1 have a distance of 2. We did this operation on the entire matrix at once. You can try to run the 1- corMatrix from the command line to see the result. The result is then formatted as a distance matrix using as.dist, which is passed to the hclust function to create a hierarchical clustering tree. See more info on hclust by using ? hclust As we can see from the tree, cyl is most highly correlated with disp and then hp and wt. Broadly, the variables form two groups, with high correlation within each cluster. This is an important insight into the overall correlation structure of this data set. Exercise 2.7 Generate a hierarchical clustering tree for the 32 car models in the mtcars dataset and discuss your results. Hint: You can transpose the dataset using function t(), so that rows becomes columns and columns become rows. Then you should be able to produce a similar tree for the cars. Another straight forward method is to translate the numbers in the correlation matrix into colors using the image functions. image(corMatrix) # translate a matrix into an image Here we are using red and yellow colors to represent the positive and negative numbers, respectively. Since the row and column names are missing, we can use the heatmap function. heatmap(corMatrix, scale = &quot;none&quot;) # Generate heatmap Here a lot more things are going on. The orders are re-arranged and also a tree is drawn to summarize the similarity. We explain heatmap in details later. A more elegant way of show correlation matrix using ggplot2 is available here: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization. So as you can see, coding is not hard when you can steal ideas from others, thanks to Dr. Google and the thousands of contributors, who contribute code examples and answer questions online. R has a fantastic user community. 2.2.5 Hierarchical clustering In the mtcars dataset, we have 32 car models, each characterized by 11 parameters (dimensions, variables). We want to compare or group these cars using information about all of these parameters. We know that Honda Civic is similar to Toyota Corolla but different from a Cadillac. Quantitatively, we need to find a formula to measure the similarity. Given two models, we have 22 numbers. We need to boil them down to one number to measure relative similarity. This is often done by a distance function. The most popular one is Euclidean distance, (it is also the most abused metric):\\(Eucliden Distance D=√((mpg_1-mpg_2)^2+(hp_1-hp_2)^2+(wt_1-wt_2)^2+⋯)\\).If two cars have similar characteristics, they have similar numbers on all of these dimensions; their distance as calculated above is small. Therefore, this is a reasonable formula. Note that we democratically added the squared difference of all dimensions. We treated every dimension with equal weight. However, if we look at the raw data, we know that some characteristics, such as hp(horsepower), have much bigger numerical value than others. In other words, the difference in hp can overwhelm our magic formula and make other dimensions essentially meaningless. Since different columns of data are in very different scale, we need to do some normalization. We want to transform data so that they are comparable on scale. At the same time, we try to preserve as much information as we could. mt = as.matrix(mtcars) heatmap(mt) Figure 2.12: Heatmap with all default settings. This is not correct. Normalization is needed. Do not go out naked. In this basic heat map, data are scaled by row by default. Some numbers are massive (hp and disp), and they are all in bright yellow. This is not democratic or reasonable as these big numbers dominate the clustering. We clearly need to do some normalization or scaling for each column which contains different statistics. Through check the help information by: ? heatmap We can figure out that we need an additional parameter: heatmap(mt, scale = &quot;column&quot;) # Figure 2.13 Scaling is handled by the scale function, which subtracts the mean from each column and then divides by the standard division. Afterward, all the columns have the same mean of approximately 0 and standard deviation of 1. This is called standardization. ? scale We can also handle scaling ourselves. We can use the apply function to subtract the mean from each column and then divide by the standard division of each column. mt &lt;- apply(mt, 2, function(y)(y - mean(y)) / sd(y)) Note that we defined a function, and ‘applied’ the function to each of the columns of mt. For each column, we first calculate the mean and standard deviation. Then the mean is subtracted before being divided by standard deviation. The second parameter “2” refers to do something with the column. Use “1” for row. Sometimes, we have columns with very little real variation. Being divided by standard deviation will amplify noise. In such cases, we just subtract the mean. This is called centering. Centering is less aggressive in transforming our data than standardization. apply(mt, 2, mean) # compute column mean Here mean( ) is the function that applied to each column. The column means are close to zero. colMeans(mt) # same as above apply(mt, 2, sd) # compute sd for each column Here sd( ) is the function that applied to each column. heatmap(mt, scale = &quot;none&quot;) # Figure 2.13 Figure 2.13: Heatmap of mtcars dataset. Yellow- positive number / above average. This produced Figure 2.13. Another function with much better coloring is heatmap.2 in the gplot package. #install.packages(&quot;gplots&quot;) library(gplots) heatmap.2(mt) # plain version This basic heatmap is not very cool. So, we do some fine tuning. This function have a million parameters to tune: ? heatmap.2 heatmap.2(mt, col = greenred(75), density.info = &quot;none&quot;, trace = &quot;none&quot;, scale = &quot;none&quot;, margins = c(5, 10)) Figure 2.14: Fine-tuned heatmap using heatmap.2 in gplots package. Note that the last argument gives a large right margin, so that the long names can show up un-truncated. By default, the heatmap function scales the rows in the data matrix so that it has zero mean. In our case, we already did our scaling, so we use scale=“none” as a parameter. Also, the dendrogram on the left and top are generated using the hclust function. The distance function is Euclidean distance. All of these can be changed. Figure 2.15: Single, complete and average linkage methods for hierarchical clustering. We used the hclust function before. Let’s dive into the details a little bit. First, each of the objects (columns or rows in mtcars data), is treated as a cluster. The algorithm joins the two most similar clusters based on a distance function. This is performed iteratively until there is just a single cluster containing all objects. At each iteration, the distances between clusters are recalculated according to one of the methods—Single linkage, complete linkage, average linkage, and so on. In the single-linkage method, the distance between two clusters is defined by the smallest distance among the object pairs. This approach puts ‘friends of friends’ into a cluster. On the contrary, complete linkage method defines the distance as the largest distance between object pairs. It finds similar clusters. Between these two extremes, there are many options in between. The linkage method I found the most robust is the average linkage method, which uses the average of all distances. However, the default seems to be complete linkage. Thus we need to change that in our final version of the heat map. library(gplots) hclust2 &lt;- function(x, ...) # average linkage method hclust(x, method=&quot;average&quot;, ...) dist2 &lt;- function(x, ...) #distance method as.dist(1-cor(t(x), method=&quot;pearson&quot;)) # Transform data mt &lt;- apply(mt, 2, function(y)(y - mean(y)) / sd(y)) heatmap.2(mt, distfun = dist2, # use 1-Pearson as distance hclustfun = hclust2, # use average linkage col = greenred(75), #color green red density.info = &quot;none&quot;, trace = &quot;none&quot;, scale = &quot;none&quot;, RowSideColors = rainbow(8)[mtcars$cyl], margins = c(5, 10) # bottom and right margins ) legend(&quot;topright&quot;,as.character(c(4, 6, 8)), fill=rainbow(8)[c(4, 6, 8)]) # add legend Figure 2.16: Final version of heatmap for mtcars data. Here we defined and used our custom distance function dist2 and used average linkage method for hclust. We also added a color bar to code for the number of cylinders. We can also add color bars for the columns, as long as we have some information for each of the column. Hierarchical clustering coupled with a heatmap is a very effective method to visualize and explore multidimensional data. It not only visualizes all data points but also highlights the correlation structure in both rows and columns. It is my favorite plot, and you can find such plots in many of my scientific publications! Let’s discuss how to interpret Figure 2.16. First, the colors red and green represent positive and negative numbers, respectively. Bright red represent large positive numbers and bright green means negative numbers with large absolute values. Since we standardized our data, red indicates above average and green below average. The 8 cylinder cars which form a cluster in the bottom have bigger than average horsepower (hp), weight (wt). These cars have smaller than average fuel efficiency (mpg), acceleration(qsec), the number of gears. These cars share similar characteristics and form a tight cluster. The four-cylinder cars, on the other hand, have the opposite. The distance in the above dendrogram between two objects is proportional to some measure of dissimilarity (such as Euclidean distance) between them defined by the original data. This is true for both trees, the one on the top and the one on the left. There are many ways to quantify the similarity between objects. The first step in hierarchical clustering is to define distance or dissimilarity between objects that are characterized by vectors. We have discussed that we can use Pearson’s correlation coefficient (PCC) to measure the correlation between two numerical vectors. We could thus easily generate a measure of dis-similarity/distance by a formula like: \\(Distance(x,y) = 1-PCC(x,y)\\). This score have a maximum of 2 and minimum of 0. Similar distance measure could be defined based on any non-parametric versions of correlation coefficients. In addition to these, there are many ways to quantify dis-similarity: (See: http://www.statsoft.com/textbook/cluster-analysis/) Euclidean distance. This is probably the most commonly chosen type of distance. It simply is the geometric distance in the multidimensional space. It is computed as: \\(Distance(x,y) = √(∑_{i=1}^{m}(x_i-y_i )^2 )\\), where m is the dimension of the vectors. Note that Euclidean (and squared Euclidean) distances are usually computed from raw data, and not from standardized data. This method has certain advantages (e.g., the distance between any two objects is not affected by the addition of new objects to the analysis, which may be outliers). However, the distances can be greatly affected by differences in scale among the dimensions from which the distances are computed. For example, if one of the dimensions denotes a measured length in centimeters, and you then convert it to millimeters, the resulting Euclidean or squared Euclidean distances can be greatly affected, and consequently, the results of cluster analyses may be very different. It is good practice to transform the data, so they have similar scales. Squared Euclidean distance. You may want to square the standard Euclidean distance to place progressively greater weight on objects that are further apart. City-block (Manhattan) distance. This distance is simply the average difference across dimensions. In most cases, this distance measure yields results similar to the simple Euclidean distance. However, note that in this measure, the effect of single large differences (outliers) is dampened (since they are not squared). The city-block distance is computed as: \\(Distance(x,y) = \\frac{1}{m}∑_{i=1}^{m}|x_i-y_i|\\) Chebyshev distance. This distance measure may be appropriate in cases when we want to define two objects as “different” if they are different on any one of the dimensions. The Cheever distance is calculated by: \\(Distance(x,y) = Maximum|x_i-y_i|\\) Percent disagreement. This measure is particularly useful if the data for the dimensions included in the analysis are categorical in nature. This distance is computed as: \\(Distance(x,y) = (Number of xi ≠ yi)/ m\\) Exercise 2.8 Generate a heatmap for the statistics of 50 states in the state.x77 dataset (for information ? state) using heatmap.2 in the gplots package. Normalize your data properly before creating heatmap. Use the default Euclidean distance and complete linkage. Use state.region to color code the states and include an appropriate legend. Interpret your results. Discuss both trees. Exercise 2.9 Change distance function to 1-Pearson’s correlation coefficient. Change linkage method to average linkage. Turn off the clustering of the columns by reading the help information on heatmap.2. Observe what is different in the clustering trees. Exercise 2.10 Generate a heat map for the iris flower dataset. For data normalization, do not use standardization, just use centering (subtract the means). Use the species information in a color bar and interpret your results. 2.2.6 Representing data using faces. Serious scientific research only! Humans are sensitive to facial images. We can use this to visualize data. #install.packages(&quot;TeachingDemos&quot;) library(TeachingDemos) faces(mtcars) Figure 2.17: Using faces to represent data. This is called Chernoff’s faces. Each column of data is used to define a facial feature. The features parameters of this implementation are: 1-height of face (“mpg”), 2-width of face (“cyl”) 3-shape of face (“disp”), 4-height of mouth (“hp”), 5-width of mouth (“drat”), 6-curve of smile (“wt”), 7-height of eyes (“qsec”), 8-width of eyes(“vs”), 9-height of hair(“am”), 10-width of hair (“gear”), 11-styling of hair (“carb”). It turns out that the longer, serious faces represent smaller cars that are environmentally-friendly, while big polluters are shown as cute, baby faces. What an irony! 2.3 Visualizing iris data set 2.3.1 A Matrix only contains numbers While data frames can have a mix of numbers and characters in different columns, a matrix is often only contain numbers. Let’s extract first 4 columns from the data frame iris and convert to a matrix: attach(iris) ## The following objects are masked from iris (pos = 11): ## ## Petal.Length, Petal.Width, Sepal.Length, Sepal.Width, Species ## The following objects are masked from iris (pos = 12): ## ## Petal.Length, Petal.Width, Sepal.Length, Sepal.Width, Species ## The following objects are masked from iris (pos = 13): ## ## Petal.Length, Petal.Width, Sepal.Length, Sepal.Width, Species x &lt;- as.matrix(iris[, 1:4]) # convert to matrix colMeans(x) # column means for matrix ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.843333 3.057333 3.758000 1.199333 colSums(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 876.5 458.6 563.7 179.9 The same thing can be done with rows via rowMeans(x) and rowSums(x). Here is some matrix algebra. y &lt;- iris[1:10, 1:4] # extract the first 10 rows of iris data in columns 1 to 4. y t(y) # transpose z &lt;- y + 5 # add a number to all numbers in a matrix z &lt;- y * 1.5 # multiply a factor z + y # adding corresponding elements y * z # multiplying corresponding elements y &lt;- as.matrix(y) # convert the data.frame y to a matrix z &lt;- as.matrix(z) # convert the data.frame z to a matrix y %*% t(z) # Matrix multiplication 2.3.2 Scatter plot matrix We can generate a matrix of scatter plots simply by: pairs(iris[, 1:4]) pairs(iris[, 1:4], col = rainbow(3)[Species]) # Figure 2.18 Figure 2.18: Scatter plot matrix. Exercise 2.11 Look at this large plot for a moment. What do you see? Provide interpretation of these scatter plots. 2.3.3 Heatmap Heatmaps with hierarchical clustering are my favorite way to visualize data matrices. The rows and columns are kept in place, and the values are coded by colors. Heatmaps can directly visualize millions of numbers in one plot. The hierarchical trees also show the similarity among rows and columns: closely connected rows or columns are similar. library(gplots) hclust2 &lt;- function(x, ...) hclust(x, method=&quot;average&quot;, ...) x &lt;- as.matrix( iris[, 1:4]) x &lt;- apply(x, 2, function(y) (y - mean(y))) heatmap.2(x, hclustfun = hclust2, # use average linkage col = greenred(75), #color green red density.info = &quot;none&quot;, trace = &quot;none&quot;, scale = &quot;none&quot;, labRow = FALSE, # no row names RowSideColors = rainbow(3)[iris$Species], srtCol = 45, # column labels at 45 degree margins = c(10, 10)) # bottom and right margins legend(&quot;topright&quot;, levels(iris$Species), fill = rainbow(3)[1:3]) Figure 2.19: Heatmap for iris flower dataset. 2.3.4 Star plot Star plot uses stars to visualize multidimensional data. Radar chart is a useful way to display multivariate observations with an arbitrary number of variables. Each observation is represented as a star-shaped figure with one ray for each variable. For a given observation, the length of each ray is made proportional to the size of that variable. The star plot is first used by Georg von Mayr in 1877! x = iris [, 1:4] stars(x) # do I see any diamonds in Figure 2.20A? I want the bigger one! stars(x, key.loc = c(17,0)) # What does this tell you? Exercise 2.12 Based on heatmap and the star plot, what is your overall impression regarding the differences among these 3 species of flowers? 2.3.5 Segment diagrams The stars() function can also be used to generate segment diagrams, where each variable is used to generate colorful segments. The sizes of the segments are proportional to the measurements. stars(x, key.loc = c(20,0.5), draw.segments = T ) Figure 2.20: Star plots and segments diagrams. Exercise 2.13 Produce the segments diagram of the state data (state.x77) and offer some interpretation regarding South Dakota compared with other states. Hints: Convert the matrix to data frame using df.state.x77 &lt;- as.data.frame(state.x77),then attach df.state.x77. 2.3.6 Parallel coordinate plot Parallel coordinate plot is a straightforward way of visualizing multivariate data using lines. x = iris[, 1:4] matplot(t(x), type = &#39;l&#39;, #“l” is lower case L for “line”. col = rainbow(3)[iris$Species]) # Species information is color coded legend(&quot;topright&quot;, levels(iris$Species), fill = rainbow(3)) # add legend to figure. text(c(1.2, 2, 3, 3.8), 0, colnames(x)) # manually add names Figure 2.21: Parallel coordinate plots directly visualize high-dimensional data by drawing lines. The result is shown in Figure 2.21. Note that each line represents a flower. The four measurements are used to define the line. We can clearly see that I. setosa have smaller petals. In addition to this, the “lattice” package has something nicer called “parallelplot”. That function can handle columns with different scales. 2.3.7 Box plot boxplot(x) # plain version. Column names may not shown properly par(mar = c(8, 2, 2, 2)) # set figure margins (bottom, left, top, right) boxplot(x, las = 2) # Figure 2.22 Figure 2.22: Box plot of all 4 columns Notice that las = 2 option puts the data labels vertically. The par function sets the bottom, left, top and right margins respectively of the plot region in number of lines of text. Here we set the bottom margins to 8 lines so that the labels can show completely. 2.3.8 Bar plot with error bar Figure 2.23: Bar plot of average petal lengths for 3 species Exercise 2.14 Write R code to generate Figure 2.23, which show the means of petal length for each of the species with error bars corresponding to standard deviations. Bar plot of average petal lengths for 3 species Bar plot of average petal lengths for 3 species. 2.3.9 Combining plots It is possible to combine multiple plots at the same graphics window. op &lt;- par(no.readonly = TRUE) # get old parameters par(mfrow= c(2, 2)) # nrows = 2; ncols= 2 attach(iris) hist(Sepal.Length) hist(Sepal.Width) hist(Petal.Length) hist(Petal.Width) par(op) # restore old parameters; otherwise affect all subsequent plots Figure 2.24: Combine multiple histograms. The result is shown in Figure 2.24. This plot gives a good overview of the distribution of multiple variables. We can see that the overall distributions of petal length and petal width are quite unusual. Exercise 2.15 Create a combined plot for Q-Q plot of the 4 numeric variables in the iris flower data set. Arrange your plots in 1 row and 4 columns. Include straight lines and interpretations. 2.3.10 Plot using principal component analysis (PCA) PCA is a linear projection method. As illustrated in Figure 2.25, it tries to define a new set of orthogonal coordinates to represent the dataset such that the new coordinates can be ranked by the amount of variation or information it captures in the dataset. After running PCA, you get many pieces of information: • How the new coordinates are defined, • The percentage of variances captured by each of the new coordinates, • A representation of all the data points onto the new coordinates. Figure 2.25: Concept of PCA. Here the first component x’ gives a relatively accurate representation of the data. Here’s an example of running PCA in R. Note that “scale=T” in the following command means that the data is normalized before conduction PCA so that each variable has unite variance. ? prcomp pca = prcomp(iris[, 1:4], scale = T) pca # Have a look at the results. ## Standard deviations (1, .., p=4): ## [1] 1.7083611 0.9560494 0.3830886 0.1439265 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.5210659 -0.37741762 0.7195664 0.2612863 ## Sepal.Width -0.2693474 -0.92329566 -0.2443818 -0.1235096 ## Petal.Length 0.5804131 -0.02449161 -0.1421264 -0.8014492 ## Petal.Width 0.5648565 -0.06694199 -0.6342727 0.5235971 Note that the first principal component is positively correlated with Sepal length, petal length, and petal width. Recall that these three variables are highly correlated. Sepal width is the variable that is almost the same across three species with small standard deviation. PC2 is mostly determined by sepal width, less so by sepal length. plot(pca) # plot the amount of variance each principal components captures. str(pca) # this shows the structure of the object, listing all parts. head(pca$x) # the new coordinate values for each of the 150 samples ## PC1 PC2 PC3 PC4 ## [1,] -2.257141 -0.4784238 0.12727962 0.024087508 ## [2,] -2.074013 0.6718827 0.23382552 0.102662845 ## [3,] -2.356335 0.3407664 -0.04405390 0.028282305 ## [4,] -2.291707 0.5953999 -0.09098530 -0.065735340 ## [5,] -2.381863 -0.6446757 -0.01568565 -0.035802870 ## [6,] -2.068701 -1.4842053 -0.02687825 0.006586116 These numbers can be used to plot the distribution of the 150 data points. plot(pca$x[, 1:2], pch = 1, col = iris$Species, xlab = &quot;1st principal component&quot;, ylab = &quot;2nd Principal Component&quot;) legend(&quot;topright&quot;, levels(iris$Species), fill = rainbow(3)) The result (left side of Figure 2.26) is a projection of the 4-dimensional iris flowering data on 2-dimensional space using the first two principal components. From this I observed that the first principal component alone can be used to distinguish the three species. We could use simple rules like this: If PC1 &lt; -1, then Iris setosa. If PC1 &gt; 1.5 then Iris virginica. If -1 &lt; PC1 &lt; 1, then Iris versicolor. Figure 2.26: PCA plot of the iris flower dataset using R base graphics (left) and ggplot2 (right). 2.3.11 Attempt at ggplot2 There are 3 big plotting systems in R: base graphics, lattice, and ggplot2. Now let’s try ggplot2. First, let’s construct a data frame as demanded by ggplot2. pcaData &lt;- as.data.frame(pca$x[, 1:2]) pcaData &lt;- cbind(pcaData, iris$Species) colnames(pcaData) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;Species&quot;) #install.packages(&quot;ggplot2&quot;) library(ggplot2) ggplot(pcaData, aes(PC1, PC2, color = Species, shape = Species)) + # define plot area geom_point(size = 2) # adding data points Now we have a basic plot. As you could see this plot is very different from those from R base graphics. We are adding elements one by one using the “+” sign at the end of the first line. We will add details to this plot. percentVar &lt;- round(100 * summary(pca)$importance[2, 1:2], 0) # compute % variances ggplot(pcaData, aes(PC1, PC2, color = Species, shape = Species)) + # starting ggplot2 geom_point(size = 2) + # add data points xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + # x label ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) + # y label ggtitle(&quot;Principal component analysis (PCA)&quot;) + # title theme(aspect.ratio = 1) # width and height ratio The result is shown in right side of Figure 2.26. You can experiment with each of the additional element by commenting out the corresponding line of code. You can also keep adding code to further customize it. Exercise 2.16 Create PCA plot of the state.x77 data set (convert matrix to data frame). Use the state.region information to color code the states. Interpret your results. Hint: do not forget normalization using the scale option. 2.3.12 Classification: Predicting the odds of binary outcomes It is easy to distinguish I. setosa from the other two species, just based on petal length alone. Here we focus on building a predictive model that can predict between I. versicolor and I. virginica. For this we use the logistic regression to model the odd ratio of being I. virginica as a function of all of the 4 measurements: \\[ln(odds)=ln(\\frac{p}{1-p}) =a×Sepal.Length + b×Sepal.Width + c×Petal.Length + d×Petal.Width+c+e.\\] iris2 &lt;- iris[51:150, ] # removes the first 50 samples, which represent I. setosa iris2 &lt;- droplevels(iris2) # removes setosa, an empty levels of species. model &lt;- glm(Species ~ . , family = binomial(link = &#39;logit&#39;), data = iris2) # Species ~ . species as a function of everything else in the dataset summary(model) ## ## Call: ## glm(formula = Species ~ ., family = binomial(link = &quot;logit&quot;), ## data = iris2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.01105 -0.00541 -0.00001 0.00677 1.78065 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -42.638 25.707 -1.659 0.0972 . ## Sepal.Length -2.465 2.394 -1.030 0.3032 ## Sepal.Width -6.681 4.480 -1.491 0.1359 ## Petal.Length 9.429 4.737 1.991 0.0465 * ## Petal.Width 18.286 9.743 1.877 0.0605 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.629 on 99 degrees of freedom ## Residual deviance: 11.899 on 95 degrees of freedom ## AIC: 21.899 ## ## Number of Fisher Scoring iterations: 10 Sepal length and width are not useful in distinguishing versicolor from virginica. The most significant (P=0.0465) factor is Petal.Length. One unit increase in petal length will increase the log-odd of being virginica by 9.429. Marginally significant effect is found for Petal.Width. If you do not fully understand the mathematics behind linear regression or logistic regression, do not worry about it too much. Me either. In this class, I just want to show you how to do these analysis in R and interpret the results. I do not understand how computers work. Yet I use it every day. Exercise 2.17 So far, we used a variety of techniques to investigate the iris flower dataset. Recall that in the very beginning, I asked you to eyeball the data and answer two questions: • What distinguishes these three species? • If we have a flower with sepals of 6.5cm long and 3.0cm wide, petals of 6.2cm long, and 2.2cm wide, which species does it most likely belong to? Review all the analysis we did, examine the raw data, and answer the above questions. Write a paragraph and provide evidence of your thinking. Do more analysis if needed. References: 1 Beckerman, A. (2017). Getting started with r second edition. New York, NY, Oxford University Press. "],
["data-importing.html", "Chapter 3 Data importing 3.1 Enter data manually 3.2 Reading data from file using Import dataset in Rstudio 3.3 Data manipulation in a data frame 3.4 Reading file using read.table, read.csv, etc. 3.5 General procedure to read data into R: 3.6 Recommended workflow for EVERY project", " Chapter 3 Data importing There are many different ways to get data into R. You can enter data manually (see below), or semi-manually (see below). You can read data into R from a local file or a file on the internet. You can also use R to retrieve data from databases, local or remote. The most import thing is to read data set into R correctly. A dataset not read in correctly will never be analyzed or visualized correctly. 3.1 Enter data manually x &lt;- c(2.1, 3.1, 3.2, 5.4) sum(x) ## [1] 13.8 A &lt;- matrix( c(2, 4, 3, 1, 5, 7), # the data elements nrow = 2, # number of rows ncol = 3) # number of columns A # show the matrix ## [,1] [,2] [,3] ## [1,] 2 3 5 ## [2,] 4 1 7 x &lt;- scan() # Enter values from keyboard, separated by Return key. End by empty line. 2.1 ## [1] 2.1 3.1 ## [1] 3.1 4.1 ## [1] 4.1 Note that you can paste a column of numbers from Excel. 3.2 Reading data from file using Import dataset in Rstudio Before reading files into R, we often need to open the files to take a look. Notepad or WordPad that come with Windows is very limited (and sooo amateur)! Do not even think about using Microsoft Word! I strongly recommend that you install a powerful text editor such as NotePad++ (https://notepad-plus-plus.org/), or TextPad (https://www.textpad.com/). If you are a Mac user, try TextMate, TextWrangler etc. I use NotePad++ almost every day to look into data, and also write R programs, as it can highlight R commands based on R syntax. I even use a tool called NppToR (https://sourceforge.net/projects/npptor/) to send R commands from NotePad++ directly to R, and I love it! Regardless of their extensions in file names, all plain text files can be opened by these text editors. Plain text files only contain text without any formatting, links and images. The file names can be “poems.txt”, “poems.tex”, “students.csv”, or just “data” without extension. I often save my R scripts as text file with names like “code_1-22-2017.R”. You can import text files, regardless of file names, to Microsoft Excel, which can properly parse your file into columns if the correct delimiter is specified. Comma separated values (CSV) files, use comma to separate the columns. CSV files can also be conveniently opened by Excel. And Rstudio likes it too. So let’s try to use CSV files. Another common type is tab-delimitated text files, which uses the tab or \\(\\t\\) as it is invisible character. Other types of files such as Excel .xls or .xlsx files often needed to be saved as CSV files. Probably the most intuitive way to read data into Rstudio is to use the Import dataset function available at File-&gt;Import Dataset from the Rstudio menu. You can also click Import dataset button on the top-right of the Rstudio interface. The preferred format is CSV. But it is not required. You can open a text file with Excel, and save it as a CSV file. 1. Download data and read background information Download the heartatk4R.txt file from this page http://statland.org/R/RC/tables4R.htm. It is a tab-deliminated text file, meaning the different columns are separated by tab, hence the “\\(\\t\\)” above. 2. In Rstudio, click File-&gt;Import Dataset-&gt;From text(readr)…, find the file on your hard drive. You should change the Delimiter to “tab”, and the preview shows that the data is correctly parsed into multiple columns. You can also change the name of your data object by changing the default “heartatk4R” to “x” on the lower left of the import interface. See Figure 3.1. The awesome nerds at Rstudio actually helped you generating these 3 lines of code: library(readr) x &lt;- read_delim(&quot;datasets/heartatk4R.txt&quot;,&quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) View(x) # shows the data, will change if you modify it. Before you click Import, I highly recommend that you select all the codes and copy it to your clipboard. After clicking Import, you can paste the code into a script window. If you do not have a script window open, you can create one by clicking the File + icon on the top left. Copy and paste these code to your script file. You will need it when you want to re-run the analysis without going through the above steps. You can see the data appears as a spreadsheet, which can be sorted by clicking on the column names. This spreadsheet can be closed. To reopen, click on x object, which is a data frame named after the input file. You data is now available as x. Figure 3.1: Importing data into R. Check data type. Most of the times, R can guess the type of data in each column. But we always need to double check using the str command. If not satisfied, we can enforce data type conversion in R using as.numeric, as.factor, or as.character functions. str(x) # structure of data object, data types for each column ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : int 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: int 41041 41041 41091 41081 41091 41091 41091 41091 41041 41041 ... ## $ SEX : chr &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... ## $ DRG : int 122 122 122 122 122 121 121 121 121 123 ... ## $ DIED : int 0 0 0 0 0 0 0 0 0 1 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : chr &quot;0010&quot; &quot;0006&quot; &quot;0005&quot; &quot;0002&quot; ... ## $ AGE : chr &quot;079&quot; &quot;034&quot; &quot;076&quot; &quot;080&quot; ... ## - attr(*, &quot;spec&quot;)=List of 2 ## ..$ cols :List of 8 ## .. ..$ Patient : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ DIAGNOSIS: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ SEX : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ DRG : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ DIED : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ CHARGES : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ LOS : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ AGE : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## ..$ default: list() ## .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; Note that the first column is just patient id number runs from 1 to 12844. It will not be useful in our analysis. The numbers in DIAGNOSIS, DRG, and DIED are integers but they actually code for certain categories. They are not measurements. It does not make sense, for example, to add them or average them. Most of the times, there is no particular order. The same is true for SEX. We need to reformat these columns as factors. We are going to use x$SEX to refer to the SEX column of the data frame x: x$DIAGNOSIS &lt;- as.factor(x$DIAGNOSIS) # convert this column to factor x$SEX &lt;- as.factor(x$SEX) x$DRG &lt;- as.factor(x$DRG) x$DIED &lt;- as.factor(x$DIED) Now the last three columns are actually numeric measurements. But the LOS and AGE were actually read as characters, due the fact that 10 is recorded as 0010. x$LOS &lt;- as.numeric(x$LOS) # convert to numeric x$AGE &lt;- as.numeric(x$AGE) str(x) # double check structure of data ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : int 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41001&quot;,&quot;41011&quot;,..: 5 5 9 8 9 9 9 9 5 5 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : num 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : num 79 34 76 80 55 84 84 70 76 65 ... ## - attr(*, &quot;spec&quot;)=List of 2 ## ..$ cols :List of 8 ## .. ..$ Patient : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ DIAGNOSIS: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ SEX : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ DRG : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ DIED : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ CHARGES : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ LOS : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ AGE : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## ..$ default: list() ## .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; summary(x) # a summary often gives us a lot of useful information ## Patient DIAGNOSIS SEX DRG DIED ## Min. : 1 41091 :5213 F:5065 121:5387 0:11434 ## 1st Qu.: 3212 41041 :2665 M:7779 122:6047 1: 1410 ## Median : 6422 41011 :1824 123:1410 ## Mean : 6422 41071 :1703 ## 3rd Qu.: 9633 41001 : 467 ## Max. :12844 41081 : 287 ## (Other): 685 ## CHARGES LOS AGE ## Min. : 3 Min. : 0.000 Min. : 20.00 ## 1st Qu.: 5422 1st Qu.: 4.000 1st Qu.: 57.00 ## Median : 8445 Median : 7.000 Median : 67.00 ## Mean : 9879 Mean : 7.569 Mean : 66.29 ## 3rd Qu.:12569 3rd Qu.:10.000 3rd Qu.: 77.00 ## Max. :47910 Max. :38.000 Max. :103.00 ## NA&#39;s :699 The summary( ) function is very useful to get basic information about data frames. Note that for numeric columns we are shown mean, median, etc, while for factors the frequencies are shown. This reassured us that the data types are correctly recognized. It also shows missing values for CHARGES. Some people got free treatment for heart attack? Maybe not. Missing does not mean zero. Maybe the data was not entered for some patients. Except enforcing data type conversion by as.factor, as.numeric and so on, We can also reformat the columns before clicking Import: After locating the file, you can click on the automatically detected data type under each of the column names as shown in Figure 3.2. By selecting “Factor” from the drop down and enter all possible levels separated by commas, you can successfully format this column as a factor. Figure 3.2: Changing data types while importing data into Rstudio. But if the column has too many levels, it is trouble to type in manually. We can read the column in as character first and then do as.facter conversion. 3.3 Data manipulation in a data frame We can sort the data by age. Again, type these commands in the script window, instead of directly into the Console window. And save the scripts once a while. x &lt;- x[order(x$AGE), ] # sort by ascending order by AGE Global Environment window contains the names and sizes of all the variables or objects in the computer memory. R programming is all about creating and modifying these objects in the memory with clear, step-by-step instructions. We also can sort the data by clicking on the column names in spreadsheet from Global Environment. Just like in Excel, you can add a new column with computed results: x$pdc &lt;- x$CHARGES / x$LOS Here we created a new column pdc to represent per day cost. We can also create a column to represent age groups using the floor function just returns the integer part. x$ag &lt;- floor(x$AGE/10) * 10 You can now do things like this: boxplot(x$CHARGES ~ x$ag) Each box represents an age group. Older patients tends to stay longer in the hospital after being admitted for heart attack. You can extract a subset of cases: x2 &lt;- subset(x, SEX == &quot;F&quot;) # Only females. “==” is for comparison and “=” is for assign value. x3 &lt;- subset(x, AGE &gt; 80) # only people older than 80 summary(x3) ## Patient DIAGNOSIS SEX DRG DIED ## Min. : 6 41091 :928 F:1263 121:1056 0:1518 ## 1st Qu.: 3527 41071 :347 M: 785 122: 462 1: 530 ## Median : 6834 41011 :280 123: 530 ## Mean : 6633 41041 :275 ## 3rd Qu.: 9734 41001 : 78 ## Max. :12844 41081 : 58 ## (Other): 82 ## CHARGES LOS AGE pdc ## Min. : 92 Min. : 1.000 Min. : 81.00 Min. : 18.4 ## 1st Qu.: 5155 1st Qu.: 5.000 1st Qu.: 82.00 1st Qu.: 833.9 ## Median : 8326 Median : 8.000 Median : 85.00 Median : 1133.7 ## Mean :10135 Mean : 9.131 Mean : 85.64 Mean : 1363.7 ## 3rd Qu.:13369 3rd Qu.:12.000 3rd Qu.: 88.00 3rd Qu.: 1530.0 ## Max. :46915 Max. :38.000 Max. :103.00 Max. :11246.1 ## NA&#39;s :115 NA&#39;s :115 ## ag ## Min. : 80.00 ## 1st Qu.: 80.00 ## Median : 80.00 ## Mean : 81.81 ## 3rd Qu.: 80.00 ## Max. :100.00 ## Try not to attach the data when you are manipulation data like this. Exercise 3.1 Generate a histogram of cost per day for middle-aged men aged between 40 and 60. Hint: subset your data step by step. 3.4 Reading file using read.table, read.csv, etc. As you get more experience with R programming, there are many other options to import data. Create a new project folder by File-&gt;New Project-&gt;New Directory-&gt;Empty Project and then create a new folder. Download and save the heartatk4R.txt file to that new folder. That folder becomes your default working directory. Now you can read the data by yourself. heartatk4R &lt;- read.table(&quot;datasets/heartatk4R.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) Then we can repeat the above commands to check and adjust data types for the columns, using str(), as.factor(), and as.numeric() functions. In summary, we have the following code to read in the data. # Reading the heart attack dataset. I am not using the Import Dataset in Rstudio. # We have to make sure the file is in the current working directory # To set working directory from Rstudio main menu, go to Session -&gt; Set Working Directory rm(list = ls()) # Erase all objects in memory getwd() # show working directory x &lt;- read.table(&quot;datasets/heartatk4R.txt&quot;, sep=&quot;\\t&quot;, header = TRUE) head(x) # show the first few rows # change several columns to factors x$DRG &lt;- as.factor(x$DRG) x$DIED &lt;- as.factor(x$DIED) x$DIAGNOSIS &lt;- as.factor(x$DIAGNOSIS) x$SEX &lt;- as.factor(x$SEX) str(x) # show the data types of columns summary(x) # show summary of dataset Alternatively, you can skip all of the above and do this. x &lt;- read.table(&quot;http://statland.org/R/RC/heartatk4R.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, colClasses = c(&quot;character&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;)) We are reading data directly from the internet with the URL. And we are specifying the data type for each column. Exercise 3.2 Type in Table 3.1 in Excel and save as a CSV file. Create a new Rstudio project as outlined above. Create a script file with comments and starting scripts, including the rm(ls()) and getwd() command. Copy the CSV file to the new folder. Import to Rstudio. Convert data types. Copy the generated R code similar to those shown in Figure 3.2 into the script file. Submit your R code and the results from the str(patients) function to show the data structure and head(patients) to show the data. Table 3.1: An example of a multivariate dataset. LastName Age Sex BloodPressure Weight HeartAttack Smith 19 M 100 130.2 1 Bird 55 F 86 300 0 Wilson 23 M 200+ 212.7 0 3.5 General procedure to read data into R: If data is compressed, unzip using 7-zip, WinRAR, Winzip, gzip. Any of these will do. Is it a text file (CSV, txt, …) or Binary file (XLS, XLSX, …)? Convert binary to text file using corresponding application. Comma separated values (CSV) files, use comma to separate the columns. Another common type is tab-delimitated text files, which uses the tab or \\(\\t\\) as it is invisible character. Open with a text editor (TexPad, NotePad++) to have a look. Rows and columns? Row and column names? row.names = 1 header = T Deliminaters between columns?(space, comma, tab…) sep = “\\(\\t\\)” Missing values? NA, na, NULL, blank, NaN, 0 missingstring = Open as text file in Excel, choose appropriate deliminater while importing, or use the Text to Column under Data in Excel. Beware of the annoying automatic conversion in Excel “OCT4”-&gt;“4-OCT”. Edit column names by removing spaces, or shorten them for easy of reference in R. Save as CSV for reading in R. Change working directory to where the file was saved. Main menu: File-&gt;Change dir… read.table ( ), or read.csv( ). For example, x &lt;- read.table(“somefile.txt”, sep = “\\(\\t\\)”, header = TRUE, missingstring = “NA”) Double check the data with str( x ), make sure each column is recognized correctly as “character”, “factor” and “numeric”. Pay attention to columns contain numbers but are actually IDs (i.e. student IDs), these should be treated as character. For example, x $ ids &lt;- as.character(x $ ids), here x is the data frame and ids is the column name. Also pay attention to columns contain numbers but actually codes for some discrete categories (1, 2, 3, representing treatment 1, treatment 2 and treatment 3). These need to be reformatted as factors. This could be done with something like x $ treatment &lt;- as.factor(x $ treatment). Refresher using cheat sheets that summarize many R functions is available here: https://www.rstudio.com/resources/cheatsheets/. It is important to know the different types of R objects: scalars, vectors, data frames, matrix, and lists. We will show you the detail in chapter 6. 3.6 Recommended workflow for EVERY project It is best to create a separate folder that contains all related files. You can do the same for research projects. In Rstudio, this is called a Project. 1. Create a project in a new folder. I recommend you to start by setting up a project in a new folder by going to File-&gt;New project-&gt;New Directory-&gt;Empty Project. Then choose where the directory will be created on your hard drive. I created a directory called “learnR” under “C:\\Ge working \\ RBook” . Rstudio creates a Project file named like “learnR.Rproj”, which contains information such as scripts files and working folders. Projects files can be saved and later opened from File-&gt;Open. You get everything ready to go on a particular assignment or research project. Copy required data files to the new directory. From Windows or Mac operation systems, you can now copy all required data files to the directory just created. Creating a script file. Once you have a new project created, the first step is to start a new script file by clicking the File + button or go to File-&gt;New file and choose R script file. By default, the script file is called Untitled1.R. Rstudio will ask you to change it the first time you hit “Save” button . Start your R script by adding comments on background information. Comments starting with “#” are ignored by R when running, but they are helpful for humans, including yourself, to understand the code. We re-cycle and re-use our codes over and over, so it is vital to add information about what a chunk of code does. Figure 3.3 shows a recommended workflow for beginning your script. Write your scripts while saving your project files. If you click on the Run button, Rstudio runs the current line of code where your cursor is located. You can also select multiple lines and run them at once. You can jump back and forth but remember you are operating on the data objects sequentially. So sometimes you want to get a fresh start by running the reset line, namely:rm(list=ls()). This command lists and then deletes all data objects from R’s brain. As you develop your coding skills, following these guidelines can make you more efficient. Remember to save everything once a while by hitting the Save button on the main icon bar! Even though Rstudio saves your scripts every 5 seconds, it can crash. Figure 3.3: Beginning a project in Rstudio, a recommended workflow: commenting, resetting, checking working folder. "],
["analyzing-heart-attack-data-set-i.html", "Chapter 4 Analyzing heart attack data set I 4.1 Begin your analysis by examining each column separately 4.2 Possible correlation between two numeric columns? 4.3 Associations between categorical variables? 4.4 Associations between a categorical and a numeric variables? 4.5 Associations between multiple columns?", " Chapter 4 Analyzing heart attack data set I The heart attack data set (http://statland.org/R/RC/tables4R.htm), included in the ActivStats1 CD, contains all 12,844 cases of hospital discharges of the patients admitted for heart attack but did not have surgery in New York State in 1993. This information is essential for the interpretation of our results, as this is a purely observational study. It is not a random sample or controlled experiment. The data set is formatted as a table (Table 4.1) with rows representing cases and columns representing characteristics, which is a typical format for many datasets. If you download and open the file in NotePad++ or Excel, you can see that the columns are separated by tabs, and there are missing values noted by “NA”. Four columns (DIAGNOSIS, SEX, DRG, DIED) contain nominal values, representing labels of categories. See an excellent explanation of data types here. DIAGNOSIS column contains codes defined in the International Classification of Diseases (IDC), 9th Edition. This is the code that your doctor sends to your insurance company for billing. The numbers, such as 41041, actually code for which part of the heart was affected. Although these are numbers, it does not make any sense to add or subtract or compute the mean. If I have a room of 30 students each with student ids, such as 732324, it does not make any sense to compute the average of these numbers. Such categorical data needs to be recognized as factors in R. Similarly, DRG column has three possible numbers, 121 for survivors with cardiovascular complications, 122 for survivors without complications, and 123 for patients who died. Moreover, DIED also codes for prognosis, it is 1 if the patient passed away, and 0 if survived. Table 4.1: First 15 rows of the heart attack dataset Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE 1 41041 F 122 0 4752.00 10 79 2 41041 F 122 0 3941.00 6 34 3 41091 F 122 0 3657.00 5 76 4 41081 F 122 0 1481.00 2 80 5 41091 M 122 0 1681.00 1 55 6 41091 M 121 0 6378.64 9 84 7 41091 F 121 0 10958.52 15 84 8 41091 F 121 0 16583.93 15 70 9 41041 M 121 0 4015.33 2 76 10 41041 F 123 1 1989.44 1 65 11 41041 F 121 0 7471.63 6 52 12 41091 M 121 0 3930.63 5 72 13 41091 F 122 0 NA 9 83 14 41091 F 122 0 4433.93 4 61 15 41041 M 122 0 3318.21 2 53 Take a look at this dataset in Excel, and consider these questions. What type of people are more likely to suffer from heart attacks? Which patient is more likely to survive a heart attack? Suppose you have a friend who was just admitted to hospital for heart attack. She is a 65 years old with DIAGNOSIS code of 41081. What is the odds that she survives without complication? Also, consider yourself as a CEO of an insurance company, and you want to know what types of patients incur more charges and whether a particular subgroup of people, such as men, or people over 70, should pay a higher premium. To answer these questions, we need to do is: • Import data files into R • Exploratory data analysis (EDA) • Statistical modeling (regression) x &lt;- heartatk4R # Make a copy of the data for manipulation, call it x. str(x) # structure of data object, data types for each column ## &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41001&quot;,&quot;41011&quot;,..: 5 5 9 8 9 9 9 9 5 5 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : num 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : num 79 34 76 80 55 84 84 70 76 65 ... 4.1 Begin your analysis by examining each column separately If you are single and meet someone in a bar, you typically start with small talks and learn some basic information about him/her. We should do the same thing with data. But too often, we go right to the business of building models or testing hypothesis without exploring our data. As a first step, we are going to examine each column separately. This can be very basic things such as mean, median, ranges, distributions, and normality. This is important because sometimes the data is so skewed or far from normal distribution that we need to use non-parametric tests, or transformate the raw data using log transformation, or more generally box-cox transformation, before conducting other analyses. Exercise 4.1 Perform the following analysis on your own. If you forgot the R commands, refer to our previous learning materials. You may also find these commands faster by asking Dr. Google. Graphical EDA: Plot distribution of charges using box plot, histogram, qqplot, lag plot, sequence plot. And interpret your results in PLAIN English. Note that there are missing values in this column that may cause some problems for some plots. You can remove missing values by defining a new variable by running temp = CHARGES [ ! is.na (CHARGES) ] and then run your plot on temp. Quantitative EDA: test of normality, and confidence interval. Note that if the Shapiro-Wilk normality test cannot handle the 12,000 data points, you can either try to find other tests in the nortest library or sample randomly by running temp = sample( CHARGES, 4000) You can attach your data set if you want to refer to the columns directly by name, such as LOS instead of x$LOS. attach(x) For categorical columns, we want to know how many different levels, and their frequencies. In addition to quantitative analysis, we also use various charts. For categorical values such as SEX and DIAGNOSIS, we can produce pie charts and bar plots, or percentages using the table( ) function followed by pie( ) and barplot(). barplot(table(DIAGNOSIS)) This generates a bar plot of counts. This basic plot could be further refined: counts &lt;- sort(table(DIAGNOSIS), decreasing = TRUE) # tabulate&amp;sort percentages &lt;- 100 * counts / length(DIAGNOSIS) # convert to % barplot(percentages, las = 3, ylab = &quot;Percentage&quot;, col = &quot;green&quot;) # Figure 4.1 Figure 4.1: Barplot by percentage. Note that the “las = 3”, changes the orientation of the labels to vertical. Try plot without it or set it to 2. Of course you can do all these in one line: barplot(100* sort(table(DIAGNOSIS), decreasing=T) / length(DIAGNOSIS), las = 3, ylab = “Percentage”, col = “green”). table(SEX) # tabulate the frequencies of M and F ## SEX ## F M ## 5065 7779 pie(table(SEX)) # pie chart Figure 4.2: Pie chart of patients by SEX. Exercise 4.2 Compute the counts and percentages of each levels of DRG. Use bar plot and pie charts similar to Figure 4.1 and Figure 4.2 to visualize. Briefly discuss your results. 4.2 Possible correlation between two numeric columns? This is done using various measures of correlation coefficients such as Pearson’s correlation coefficients (PPC), which is given by \\[r=∑_{i=1}^{n}(\\frac{x_i-\\overline{x}}{s_x}) (\\frac{y_i-\\overline{y}}{s_y})\\] where x i and y i are the ith values, \\(\\overline{x}\\) and \\(\\overline{y}\\) are sample means, and s x and s y are sample standard deviations. Note that Pearson’s correlation ranges from -1 to 1, with -1 indicating perfect negative correlation. Negative correlation is just as important and informative as positive ones. Figure 4.3: Interpretation of Pearson’s correlation coefficient. The numbers are Pearson’s correlation coefficient r. Figure 4.3 shows some examples of Pearson’s correlation with many scatter plots. The second row of figures shows examples with X-Y plots with different slopes but Pearson’s correlation are all 1. Pearson’s correlation only indicates degree of correlation, and is independent of slope. The figures in the 3rd row show that Pearson’s correlation coefficient’s limitation: it cannot detect nonlinear correlation. Table 4.2 below gives some guideline on how to interpret Pearson’s correlation coefficient. Table 4.2: Interpretation of correlation coefficient. Correlation Negative Positive - -0.09 to 0.0 0.0 to 0.09 Small -0.3 to -0.1 0.1 to 0.3 Medium -0.5 to -0.3 0.3 to 0.5 Large -1.0 to -0.5 0.5 to 1.0 There is a small, but statistically significant correlation between age and length of stay in the hospital after a heart attack. The plain English interpretation (read: a no-bullshit version that could be understood by your grandmother) is this: Older people tend to stay slightly longer in the hospital after a heart attack. cor.test(AGE, LOS) ## ## Pearson&#39;s product-moment correlation ## ## data: AGE and LOS ## t = 21.006, df = 12842, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1654881 0.1989282 ## sample estimates: ## cor ## 0.1822609 Note that the correlation coefficient r and the p value measure two different things. r indicates the size of effect, while p value tells us statistical significance. Based on the statistic sample, p value tells how certain we are about the difference being real, namely not due to random fluctuation. If we have a large sample, we could detect very small correlation with significance. Conversely, if we only have a few observations, a large r could have large p value, hence not significant. More generally, we need to distinguish effect size and significance in statistical analyses. Like many commonly-used parametric statistical methods which rely on means and standard deviations, the Pearson’s correlation coefficient is not robust, meaning its value are sensitive to outliers and can be misleading. It is also very sensitive to distribution. Non-parametric approaches typically rank original data and do calculations on the ranks instead of raw data. They are often more robust. The only drawback might be loss of sensitivity. There are corresponding non-parametric versions for most of the parametric tests. Spearman’s rank correlation coefficient ρ is a non-parametric measure of correlation. The Spearman correlation coefficient ρ is often thought of as being the Pearson correlation coefficient between the ranked variables. In practice, however, a simpler procedure is normally used to calculate ρ. The n raw scores Xi, Yi are converted to ranks xi, yi, and the differences di = xi − yi between the ranks of each observation on the two variables are calculated. If there are no tied ranks, then ρ is given by:\\[ρ=1-\\frac{6∑d_{i}^{2}}{n(n_{}^{2}-1)}\\] In R, we can calculate Spearman’s ρ and test its significance but customize the cor.test() function: cor.test(AGE, LOS, method = &quot;spearman&quot;) ## Warning in cor.test.default(AGE, LOS, method = &quot;spearman&quot;): Cannot compute ## exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: AGE and LOS ## S = 2.9448e+11, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.1661032 Interpretation of Spearman’s ρ is similar to Pearson’s r. The statistical significance can also be determined similarly as demonstrated above. Alternative non-parametric statistic for correlation is Kendall tau rank correlation coefficient. We already know that we could use scatter plots to visualize correlation between two numeric columns. But when there are many data points, in this case we have over 12,000, it could be hard to comprehend. This is especially the case, when the data is integers and there are a lot of data points overlap on top of each other. Yes, graphics can be misleading. plot(AGE, LOS) # standard scatter plot smoothScatter(AGE, LOS) # a smoothed color density representation of a scatterplot Figure 4.4: Smoothed Scatter plots use colors to code for the density of data points. This is useful when there are overlapping points. Exercise 4.3 Investigate the correlation between length of stay and charges. Try both parametric and non-parametric methods to quantify correlation and use graphs. Remember to include plain English interpretation of your results even your grandpa can understand. 4.3 Associations between categorical variables? There are four columns in the heart attack data set that contain categorical values (DIAGNOSIS, DRG, SEX, and DIED). These columns could be associated with each other. For example, there is a correlation between SEX and DIED. Are men and women equally likely to survive a heart attack? counts &lt;- table(SEX, DIED) # tabulates SEX and DIED and generate counts in a 2d array. counts Table 4.3: A 2x2 contingency table summarizing the distribution of DIED frequency by SEX. Sex DIED_0 DIED_1 F 4298 767 M 7136 643 We got a contingency table as shown in Table 4.3. To convert into percentages of survived, we can do: counts / rowSums(counts) ## DIED ## SEX 0 1 ## F 0.84856861 0.15143139 ## M 0.91734156 0.08265844 We can see that 15.1% of females died in the hospital, much higher than the 8.26% for male patients. This gender difference is quite a surprise to me. But could this happen just by chance? To answer this question, we need a statistical test. Chi-square test for the correlation of two categorical variables. The null hypothesis is that men and women are equally likely to die from a heart attack. chisq.test(counts) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: counts ## X-squared = 147.76, df = 1, p-value &lt; 2.2e-16 You have seen this p-value before? Probably! It is the smallest non-zero number R shows for lots of tests. However, p is definitely small! Hence we reject the hypothesis that the mortality rate is the same for men and women. Looking at the data, it is higher for women. The chi-square test for a 2x2 contingency table gives accurate p-values provided that the number of expected observation is greater than 5. If this is not true, then you should use the Fisher Exact test. The chi-square test is an approximation to the Fisher Exact test. The Fisher Exact test is computationally intensive; Karl Pearson developed the chi-square approximation before we had computers to do the work. With fast computers available today, you can use the Fisher Exact test for quite large data sets, and be more confident in the p-values. You can use the chi-square test for contingency tables that have more than two rows or two columns. For contingency tables that have more than two rows or two columns, the p-value computed by the chi square approximation is reasonably accurate provided that the expected number of observations in every cell is greater than 1, and that no more than 20 percent of the cells have an expected number of observations less than 5. Again, the Fisher Exact test can handle quite large data sets with today’s computers, and avoid the problems with chi-square test. fisher.test(counts) # Fisher’s Exact test ## ## Fisher&#39;s Exact Test for Count Data ## ## data: counts ## p-value &lt; 2.2e-16 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.4509331 0.5653197 ## sample estimates: ## odds ratio ## 0.5049435 In this case, the result of fisher’s test is the same as chi-square test. If you want to make your point to a boss who is either stupid or too busy, you need a chart. Below we show two barplots, one stacked and one side by side. counts &lt;- table(DIED, SEX) # SEX define columns now, as I want the bars to represent M or F. barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = &quot;DIED&quot;, args.legend = list(x = &quot;topleft&quot;)) # Figure 4.5A barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = &quot;DIED&quot;, beside = T) # Figure 4.5B Figure 4.5: Barplot showing the correlation of two categorical variables. A. Stacked. B. Side by side. Another way of showing the proportions is mosaic plot. mosaicplot(table(SEX, DIED), color = T) # Figure 4.6 Figure 4.6: Mosaic plot of DIED by SEX. The mosaic plot in Figure 4.6 is similar to the barplot in Figure 4.5, but the bars are stretched to the same height, the width is defined by proportion of Male vs. Female. The size of the four blocks in the figure represents the counts of the corresponding combination. Also note that the blocks are also color-coded for different combination. Horizontally, the blocks are divided by SEX, we could observe that there are more men in this dataset than women. Vertically, the blocks are divided by DIED (1 for died in hospital). We could conclude that regardless of gender, only a small proportion of patients died in hospital. Between men and women, we also see that the percentage of women that died in hospital is higher than that in men. This is a rather unusual. We could use mosaic plots for multiple factors. mosaicplot(table(SEX, DIED, DRG), color = rainbow(3)) # Figure 4.7 Figure 4.7: Mosaic plot of three factors. Here we nested the tabulate command inside the mosaic plot. As shown in Figure 4.7, we further divided each of the 4 quadrants of Figure 4.6 into three parts according to DRG codes, in red, green and blue. One thing we could tell is that a smaller proportion of surviving males developed complications, compared with females. Activity: interpret the mosaic plot of the Titanic dataset(built-in in R). ? Titanic # this leads you to information about the famous Titanic dataset. mosaicplot(~ Sex + Age + Survived, data = Titanic, color = rainbow(2)) This is a mosaic plot of the whole Titanic dataset mosaicplot(Titanic, color = rainbow(2)) Did men and women survived by equal proportion? Did girls and women survived by equal proportion? Exercise 4.4 The DIAGNOSIS column contains IDC codes that specifies the part of the heart that are affected. Are men and women equal in their frequencies of diagnoses? Use a stacked bar plot and a mosaic plot to compare the difference in frequency of DIAGNOSIS between men and women. Hint: Use table to generate the counts and then visualize. Exercise 4.5 Use these approaches to investigate whether men and women differ in their diagnosis. 4.4 Associations between a categorical and a numeric variables? Do women stay longer in the hospital? Does the charges differ for people with different diagnosis (part of the heart affected)? We should know by now how to answer these questions with T-test, and more generally ANOVA, following our examples commands used in our analysis of the Iris flower data set. For data visualization, boxplot is the most straight forward way. But beyond boxplot, we can use the ggplot2 package for more detailed examination of distribution of variables in two or more groups. library(ggplot2) ggplot(heartatk4R, aes(x = AGE, group = SEX, y = c(..count..[..group.. == 1]/sum(..count..[..group.. == 1]), ..count..[..group.. == 2]/sum(..count..[..group.. == 2])) * 100)) + geom_histogram(binwidth = 6, colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + facet_grid(SEX ~ .) + labs(y = &quot;Percent of Total&quot;) Figure 4.8: Histogram of AGE by SEX using ggplot2 package Now the two histograms are arranged, and it is very easy to see that women’s age are more skewed to the right, meaning women are considerably older than men. I am surprised at first by this huge difference, as the average age of women if bigger by 11. Further research shows that for women the symptoms of heart attacks are milder and often go unnoticed. We can further divide the population according to survival status by adding another factor: library(dplyr) heartatk4R %&gt;% mutate(GROUP = paste(DIED, SEX, sep = &quot;-&quot;)) %&gt;% ggplot(aes(x = AGE, group = GROUP, y = c(..count..[..group.. == 1]/sum(..count..[..group.. == 1]), ..count..[..group.. == 2]/sum(..count..[..group.. == 2]), ..count..[..group.. == 3]/sum(..count..[..group.. == 3]), ..count..[..group.. == 4]/sum(..count..[..group.. == 4])) * 100))+ geom_histogram(binwidth = 5, colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + facet_grid(DIED ~ SEX) + labs(y = &quot;Percent of Total&quot;) Figure 4.9: Histogram of AGE by SEX and DIED using ggplot2 package detach(&quot;package:dplyr&quot;) We can see that patients who did not survive heart attack tend to be older, for both men and women. This is perhaps better illustrated with density plot: ggplot(heartatk4R, aes(x = AGE, fill = SEX)) + geom_density(alpha = .3) Figure 4.10: Density plot of AGE by SEX using ggplot2 package The result is similar to Figure 4.8, but as a density plot. Now for each gender, we further divide the patients by their survival status. Instead of splitting into multiple panels, the curves are overlaid. ggplot(heartatk4R, aes(x = AGE, fill = DIED)) + geom_density(alpha = .3) + facet_grid(SEX ~ .) Figure 4.11: Density plot of AGE by SEX and DIED using ggplot2 package Exercise 4.6 Generate the pie chart for each of the categorical variables in heart attack dataset. Generate histogram for each of the numerical variables. Interpret each plot. Exercise 4.7 Use the ggplot2 package to compare the distribution of lengths of stay among patients who survived and those who did not. Use both histograms and density plot. Interpret your results. Exercise 4.8 Use the ggplot2 package to compare the distribution of lengths of stay among patients who survived and those who did not, but compare men and women separately (similar to Figure 4.11). Exercise 4.9 Use student’s t-test, boxplot, histogram and density plots to compare the age distribution between survived and those who didn’t. Exercise 4.10 Use ANOVA, boxplot, and histogram and density plots to compare the charges among people who have different DRG codes. 4.5 Associations between multiple columns? We can use the ggplot2 package to investigate correlations among multiple columns by figures with multiple panels. ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_boxplot(color = &quot;blue&quot;) + facet_grid(SEX ~ .) Figure 4.12: Multiple boxplots by ggplot2 package. Recall that 121 indicate survivors with complication, 122 survivors with no complication, and 123, died. As you could see this clearly indicate our previous observation people who died in hospital are older than survivors and that patients who developed complications seems to be older than those that did not. Did people with complications stayed longer in the hospital? Exercise 4.11 Are the surviving women younger than the women who died? Similar question can be asked for men. Produce a figure that compares, in a gender-specific way, age distribution between patients who died in the hospital and those who survived. Exercise 4.12 Use the ggplot2 package to produce boxplots to compare the length of stage of men vs. women for each of the DRG categories indicating complication status. You should produce a plot similar to Figure 4.13. Offer interpretation. Figure 4.13: Multiple boxplots by ggplot2 package. All of these techniques we introduced so far enable us to LEARN about your dataset without any of priori hypothesis, ideas, and judgments. Many companies claim that they want to know their customers first as individuals and then do business. Same thing applies to data mining. You need to know you dataset as it is before making predictions, classifications etc. You should also INTERACT with your data by asking questions based on domain knowledge and common sense. Generates lots and lots of plots to support or reject hypothesis you may have. I demonstrated this by using the heart attack dataset in the last few pages. You should do the same thing when you have a new dataset. Sometimes, the thing that you discovered is more important than the initial objectives. # scatterplot of LOS vs. AGE ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() # scatterplot of LOS vs. AGE, divided by SEX ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() + facet_grid(SEX ~ .) # scatterplot colored by DIED ggplot(heartatk4R, aes(x = AGE, y = LOS, color = DIED)) + geom_point() + facet_grid(SEX ~ .) Note that ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() + facet_grid(SEX ~ .) generates multiple scatterplots of LOS ~ AGE according to different values of SEX, while color = DIED will add these two color-coded scatter plots into the same figure. Figure 4.14: A scatter plot of LOS vs. AGE, using SEX and DIED as factors. Figure 4.14 seems to suggest that the positive association between AGE and LOS is noticeable in patients who did not die in hospital, regardless of sex. This is a statistician’s language. Try this instead that could be understood by both the statistician and his/her grandmother. Older patients tend to stay longer in the hospital after surviving a heart attack. This is true for both men and women. Another way to visualize complex correlation is bubble plot. Bubble plot is an extension of scatter plot. It uses an additional dimension of data to determine the size of the symbols. Interesting video using bubble plot: http://youtu.be/jbkSRLYSojo y &lt;- x[sample(1:12844, 200), ] # randomly sample 200 patients plot(y$AGE, y$LOS, cex = y$CHARGES / 6000, col = rainbow(2)[y$SEX], xlab = &quot;AGE&quot;, ylab = &quot;LOS&quot;) legend(&quot;topleft&quot;, levels(y$SEX), col = rainbow(2), pch = 1) Figure 4.15: Bubble plot example. Figure 4.15 is a busy plot. Female patients are shown in red while males in blue. Size of the plot is proportional to charges. So on this plot we are visualizing 4 columns of data! Other common methods we can use to detect complex correlations and structures include principal component analysis (PCA), Multidimensional scaling (MDS), hierarchical clustering etc. Velleman, P. F.; Data Description Inc. ActivStats, 2000-2001 release.; A.W. Longman,: Glenview, IL, 2001. "],
["analyzing-heart-attack-data-set-ii.html", "Chapter 5 Analyzing heart attack data set II 5.1 Scatter plot in ggplot2 5.2 Histograms and density plots 5.3 Box plots and Violin plots 5.4 Bar plot with error bars 5.5 Statistical models are easy; interpretations and verifications are not!", " Chapter 5 Analyzing heart attack data set II We continue to investigate the heart attack dataset. First, let’s read the data in again and change several variables to factors. Note that you must set the working directory to where the data file is stored on your computer. I encourage you to define a project associated with a folder. # set working directory, not necessary if loading a project #setwd(&quot;C:/Ge working/RBook/learnR/datasets&quot;) df &lt;- read.table(&quot;heartatk4R.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) # read data df$DRG &lt;- as.factor(df$DRG) # convert DRG variable to factor df$DIED &lt;- as.factor(df$DIED) df$DIAGNOSIS &lt;- as.factor(df$DIAGNOSIS) df$SEX &lt;- as.factor(df$SEX) str(df) # double check ## &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : int 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41001&quot;,&quot;41011&quot;,..: 5 5 9 8 9 9 9 9 5 5 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : int 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : int 79 34 76 80 55 84 84 70 76 65 ... 5.1 Scatter plot in ggplot2 Hadley Wickham wrote the ggplot2 package in 2005 following Leland Wilkinson’s grammar of graphics, which provides a formal, structured way of visualizing data. Similarly, R was originally written by Robert Gentleman and Ross Ihaka in the 1990s; Linux was developed by Linus Torvalds, a student, at the same period. A few superheroes can make computing much easier for millions of people. ggplot2 uses a different approach to graphics. #install.packages(&quot;ggplot2&quot;) # install the package library(ggplot2) # load the package ggplot(df, aes(x = LOS, y = CHARGES)) # Specify data frame and aesthetic mapping This line does not finish the plot; it just specifies the name of the data frame, which is the required input data type, and defines the so-called aesthetic mapping: LOS maps to x-axis while CHARGES maps to the y-axis. To complete the plot, we use the geom_point function to add data points, which is called geometric objects. ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() # scatter plot Thus, it is a two-step process to generate a scatter plot. This seems cumbersome at first, but it is convenient to add additional features or customize the plot step by step. Let’s add a trend line with a confidence interval. ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() + stat_smooth(method = lm) As we keep adding elements to the plot, this line of code gets longer. So we break the code into multiple lines as below. The “+” at the end of the lines signifies that the plot is not finished and tell R to keep reading the next line. This code below does the same thing as the above, but it is easier to read. I often use the tab key in the second line to remind myself that it is continued from the above line. ggplot(df, aes(x = LOS, y = CHARGES)) + # aesthetic mapping geom_point() + # add data points stat_smooth(method = lm) # add trend line As you can see from this code, it also enables us to add comments for each step, making the code easy to read. This is important, as we often recycle our codes. We can also customize the plot by adding additional lines of code (Figure 5.1): ggplot(df, aes(x = LOS, y = CHARGES)) + # aesthetic mapping geom_point() + # add data points stat_smooth(method = lm) + # add trend line xlim(0, 25) + # change plotting limits of x-axis labs(x = &quot;Length of stay&quot;, y = &quot;Charges ($)&quot;) + # change x and y labels annotate(&quot;text&quot;, x = 3, y = 45000, label = (&quot;R = 0.74&quot;)) # add text to plot coordinates Figure 5.1: Scatter plot using ggplot2. You can learn other ways to customize your plot by googling. For example, try to find a way to add title to the plot by using keyword “ggplot2 add title to plot”. It is easy to represent other characteristics of data points (columns) using additional aesthetic mappings, such as linetype, color, size, fill (“inside” color). ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() # basic scatter plot ggplot(df, aes(x = LOS, y = CHARGES, color = DRG)) + geom_point() # map DRG to color ggplot(df, aes(x = LOS, y = CHARGES, color = DRG, shape = SEX)) + # map SEX to shape geom_point() Figure 5.2: Changing color and shape to represent multivariate data in ggplot2. At each step, we add additional information about these patients (Figure 5.2). With ggplot2, we can visualize complex, multivariate data by mapping different columns into diverse types of aesthetics. Figure 5.1 shows a strong positive correlation between charges and length of hospital stay, which is expected as many itemized costs are billed daily. Note there are over 12,000 data points on these plots, many are plotted near or exactly at the same places, especially at the lower left corner. stat_density2d() can be used to color code density. These plots are big in file size when you save it in a vector format (metafile), which often offer higher resolution. If you have many such plots in a paper or thesis, your file size can be big. These problems can be avoided if you use the bitmap format. Exercise 5.1 Use ggplot2 to generate a scatter plot of age vs. charges in the heart attack dataset. Use different shapes of data points to represent DRG and color-code data points based on diagnosis. 5.2 Histograms and density plots In addition to data points(geom_points), there are many other geometric objects, such as histogram(geom_histogram), lines (geom_line), and bars (geom_bar), and so on. We can use these geometric objects to generate different types of plots. To plot a basic histogram: ggplot(df, aes(x = AGE)) + geom_histogram() Then we can refine it and add a density curve (Figure 5.3): ggplot(df, aes(x = AGE, y = ..density..)) + geom_histogram(fill = &quot;cornsilk&quot;, colour = &quot;grey60&quot;, size = .2) + geom_density() Figure 5.3: Histogram with density curve. Combined density plots are useful in comparing the distribution of subgroups of data points. ggplot(df, aes(x = AGE)) + geom_density(alpha = 0.3) # all data Use color to differentiate sex: ggplot(df, aes(x = AGE, color = SEX)) + geom_density(alpha = 0.3) # Figure 5.4A Here, we split the dataset into two portions, men and women, and plotted their density distribution on the same plot. Figure 5.4A shows that age distribution is very different between men and women. Women’s distribution is skewed to the right and they are on average over ten years older than men. This is surprising, given that this dataset contains all heart attack admissions in New York state in 1993. The fill mapping changes the “inside” color: ggplot(df, aes(x = AGE, fill = SEX)) + geom_density(alpha = 0.3) # Figure 5.4B This plot shows the same information. It looks nicer, at least to me. We can split the plot into multiple facets: ggplot(df, aes(x = AGE, fill = SEX)) + geom_density(alpha = 0.3) + facet_grid(DRG ~.) # Figure 5.4C Figure 5.4: Density plots. Recall the DRG=121 represents patients who survived but developed complications; DRG=122 denotes those without complications and DRG=123 are patients who did not survive. Figure 5.4C suggests that women in all three groups are older than men counterparts. If we examine the distribution of male patients’ age distribution across facets, we can see that the distribution is skewed to the right in deceased patients (DRG=123), indicating that perhaps older people are less likely to survive a heart attack. Survivors without complications (DRG= 122) tend to be younger than survivors with complications. Exercise 5.2 Create density plots like Figure 5.5 to compare the distribution of length of hospital stay (LOS) for patients with different DRG groups, separately for men and women. Offer interpretation in the context of the dataset. Limit the x-axis to 0 to 20. Figure 5.5: Distribution of LOS by DRG groups grouped by SEX. 5.3 Box plots and Violin plots We can follow the same rule to generate boxplots using the geom_boxplot ( ). Let’s start with a basic version. ggplot(df, aes(x = SEX, y = AGE)) + geom_boxplot() # basic boxplot ggplot(df, aes(x = SEX, y = AGE)) + geom_boxplot() + facet_grid(DRG ~ .) # Figure 5.6A ggplot(df, aes(x = SEX, y = AGE)) + geom_violin() + facet_grid(DRG ~ .) # Figure 5.6B The last version is a violin plot. It shows more details about the distribution as it is essentially density plots on both left and right sides of the violins. Exercise 5.3 Generate a violin plot like Figure 5.6C to compare the distribution of length of hospital stay among patients with different prognosis outcomes (DRG), separately for men and women. Interpret your result. Note the axes labels are customized. Figure 5.6: Boxplot and violin plots using ggplot2. 5.4 Bar plot with error bars Suppose we are interested in examining whether people with certain diagnosis codes stays longer or shorter in the hospital after a heart attack. We can, of course, use the aggregate function to generate a table with means LOS by category. aggregate(df, by = list(df$DIAGNOSIS), FUN = mean, na.rm = TRUE) ## Group.1 Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE ## 1 41001 7130.229 NA NA NA NA 10868.030 7.775161 65.56317 ## 2 41011 6413.323 NA NA NA NA 10631.803 7.728618 65.81305 ## 3 41021 6482.808 NA NA NA NA 10666.687 7.556000 63.96000 ## 4 41031 6356.028 NA NA NA NA 9908.118 6.679715 62.93950 ## 5 41041 6259.752 NA NA NA NA 9985.722 7.307692 63.73884 ## 6 41051 6438.974 NA NA NA NA 9062.423 7.298701 67.22078 ## 7 41071 6949.627 NA NA NA NA 9750.309 7.834997 69.01996 ## 8 41081 8384.725 NA NA NA NA 9633.123 7.048780 68.04181 ## 9 41091 6165.481 NA NA NA NA 9511.093 7.625743 67.10359 We can be happy with this table and call it quits. However, tables are often not as easy to interpret as a nicely formatted graph. Instead of the aggregate function, we use the powerful dplyr package to summarize the data and then to generate a bar plot showing both the means and standard errors. I stole some code and ideas from R graphics cookbook and this website: http://environmentalcomputing.net/plotting-with-ggplot-bar-plots-with-error-bars/ #install.packages(&quot;dplyr&quot;) # dplyr package for summary statistics by group library(dplyr) # load the package To summarize data by groups/factors, the dplyr package uses a similar type of grammar like ggplot2, where operations are added sequentially. Similar to pipes in Unix, commands separated by “%&gt;%” are executed sequentially where the output of one step becomes the input of the next. The follow 6 lines are part of one command, consisting of three big steps. stats &lt;- df %&gt;% # names of the new data frame and the data frame to be summarized group_by(DIAGNOSIS) %&gt;% # grouping variable summarise(mean = mean(LOS), # mean of each group sd = sd(LOS), # standard deviation of each group n = n(), # sample size per group se = sd(LOS) / sqrt(n())) # standard error of each group The resultant data frame is a detailed summary of the data by DIAGNOSIS: stats ## # A tibble: 9 x 5 ## DIAGNOSIS mean sd n se ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 41001 7.78 5.69 467 0.263 ## 2 41011 7.73 5.34 1824 0.125 ## 3 41021 7.56 5.26 250 0.333 ## 4 41031 6.68 4.15 281 0.247 ## 5 41041 7.31 4.75 2665 0.0920 ## 6 41051 7.30 4.54 154 0.366 ## 7 41071 7.83 5.28 1703 0.128 ## 8 41081 7.05 5.32 287 0.314 ## 9 41091 7.63 5.14 5213 0.0712 Now we can use ggplot2 to plot these statistics (Figure 5.7): ggplot(stats, aes(x = DIAGNOSIS, y = mean)) + # data &amp; aesthetic mapping geom_bar(stat = &quot;identity&quot;) + # bars represent average geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) # error bars Figure 5.7: Average LOS by DIAGNOSIS group with error bars representing standard error. Note that the mean-se and mean+se refer to the two columns in the stats data frame and defines the error bars. Because we have an extremely large sample size, the standard errors are small. People with a diagnosis of 41031 stays shorter in the hospital. By looking up the IDC code in http://www.nuemd.com, we notice that this code represents “Acute myocardial infarction of inferoposterior wall, initial episode of care”, which probably makes sense. As we did previously, we want to define the LOS bars for men and women separately. We first need to go back and generate different summary statistics. stats2 &lt;- df %&gt;% group_by(DIAGNOSIS, SEX) %&gt;% # two grouping variables summarise(mean = mean(LOS), sd = sd(LOS), n = n(), se = sd(LOS) / sqrt(n())) The entire dataset was divided into 18 groups according to all possible combinations of DIAGNOSIS and SEX. For each group, the LOS numbers are summarized in terms of mean, standard deviation (sd), observations (n), and standard errors(se). Below is the resultant data frame with summary statistics: stats2 ## # A tibble: 18 x 6 ## # Groups: DIAGNOSIS [?] ## DIAGNOSIS SEX mean sd n se ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 41001 F 8.74 6.03 175 0.456 ## 2 41001 M 7.20 5.41 292 0.316 ## 3 41011 F 8.41 6.23 692 0.237 ## 4 41011 M 7.31 4.66 1132 0.139 ## 5 41021 F 8.54 5.82 100 0.582 ## 6 41021 M 6.9 4.76 150 0.389 ## 7 41031 F 6.41 4.67 96 0.477 ## 8 41031 M 6.82 3.86 185 0.283 ## 9 41041 F 7.79 5.18 998 0.164 ## 10 41041 M 7.02 4.45 1667 0.109 ## 11 41051 F 7.93 5.64 69 0.679 ## 12 41051 M 6.79 3.34 85 0.362 ## 13 41071 F 8.65 5.76 727 0.214 ## 14 41071 M 7.23 4.80 976 0.154 ## 15 41081 F 7.58 5.60 130 0.492 ## 16 41081 M 6.61 5.04 157 0.402 ## 17 41091 F 8.34 5.76 2078 0.126 ## 18 41091 M 7.15 4.63 3135 0.0826 Now we are ready to generate bar plot for men and women separately using the fill aesthetic mapping: ggplot(stats2, aes(x = DIAGNOSIS, y = mean, fill = SEX)) + # mapping geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + # bars for mean of LOS labs(x = &quot;Diagnosis codes&quot;, y = &quot;Length of Stay&quot;) + # axes labels geom_errorbar(aes(ymin = mean - se, ymax = mean + se), # error bars position = position_dodge(.9), width = 0.2) Figure 5.8: The length of stay summarized by diagnosis and sex. Error bars represent standard error. Figure 5.9: Mean ages for patients with different diagnosis and treatment outcome. Exercise 5.4 Generate Figure 5.9 and offer your interpretation. Hint: Modify both the summarizing script and the plotting script. Overall, our investigation reveals that women are much older than men when admitted to hospital for heart attack. Therefore, prognosis is also poor with high mortality and complication rates. It is probably not because they develop heart attack later in life. It seems that symptoms are subtler in women and often get ignored. Heart attack can sometimes manifest as back pain, numbness in the arm, or pain in the jaw or teeth! (Warning: statistics professor is talking about medicine!) As you could see from these plots, ggplot2 generates nicely-looking, publication-ready graphics. Moreover, it is a relatively structured way of customizing plots. That is why it is becoming popular among R users. Once again, there are many example codes and answered R coding questions online. Whatever you want to do, you can google it, try the example code, and modify it to fit your needs. It is all free! Enjoy coding! 5.5 Statistical models are easy; interpretations and verifications are not! By using pair-wised correlation analysis, we found that women have a much higher mortality rate than men due to heart attack. We also found that women are much older than men. These are obviously confounding effects. We need to delineate the effects of multiple factors using multiple linear regression. In our model below, we express the charges as a function of all other factors using multiple linear regression. fit &lt;- lm(CHARGES ~ SEX + LOS + AGE + DRG + DIAGNOSIS, data = df) summary(fit) ## ## Call: ## lm(formula = CHARGES ~ SEX + LOS + AGE + DRG + DIAGNOSIS, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31486 -2453 -674 1766 32979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6863.753 314.515 21.823 &lt; 2e-16 *** ## SEXM 183.085 84.131 2.176 0.02956 * ## LOS 989.717 8.268 119.700 &lt; 2e-16 *** ## AGE -55.255 3.216 -17.180 &lt; 2e-16 *** ## DRG122 -916.531 86.851 -10.553 &lt; 2e-16 *** ## DRG123 1488.187 141.771 10.497 &lt; 2e-16 *** ## DIAGNOSIS41011 -136.097 230.312 -0.591 0.55458 ## DIAGNOSIS41021 137.118 349.948 0.392 0.69520 ## DIAGNOSIS41031 201.021 334.056 0.602 0.54735 ## DIAGNOSIS41041 -349.932 223.352 -1.567 0.11720 ## DIAGNOSIS41051 -1162.651 408.621 -2.845 0.00444 ** ## DIAGNOSIS41071 -755.549 233.214 -3.240 0.00120 ** ## DIAGNOSIS41081 -353.290 332.747 -1.062 0.28838 ## DIAGNOSIS41091 -1042.968 215.038 -4.850 1.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4308 on 12131 degrees of freedom ## (699 observations deleted due to missingness) ## Multiple R-squared: 0.569, Adjusted R-squared: 0.5685 ## F-statistic: 1232 on 13 and 12131 DF, p-value: &lt; 2.2e-16 As we could see from above, one levels for factors (such as female) is used as a base for comparison, so it does not show. SEXM indicates that being male, we have a marginally significant effect on charges. Everything else being equal, a male patient will incur $183 dollars more cost than female for the hospital stay. This is really small number compared to overall charges and also the p value is just marginally significant for this large sample. This is in contrast to the t.test(CHARGES ~ SEX), when we got the opposite conclusion. This is because we did not control for other factors. The most pronounced effect is LOS. This is not surprising, as many hospitals have charges on daily basis. Since the p values are cutoff at 2e-16, the t value is an indicator of significance. LOS has a t value of 119.7, which is way bigger than all others. If I am an insurance company CEO, I will do anything I can to push the hospitals to discharge patients as early as possible. The coefficients tell us for the average patient, one extra day of stay in the hospital the charges will go up by $989.7. Age has a negative effect, meaning older people actually will be charged less, when other factors are controlled. So there is no reason to charge older people more in terms of insurance premiums. And there is a little evidence to charge female more than males. Compared with patients who had complications (DRG = 121, the baseline), people have no complications (DRG = 122) incurred less charges on average by the amount of $916. Patients who died, on the other hand, are likely to be charged more. People with diagnosis codes 410914, 1051 and 41071 incurred less charges compared with those with 41001. To investigate mortality, which is a binary outcome, we use logistic regression. fit &lt;- glm(DIED ~ SEX + LOS + AGE + DIAGNOSIS, family = binomial( ), data = df) summary(fit) ## ## Call: ## glm(formula = DIED ~ SEX + LOS + AGE + DIAGNOSIS, family = binomial(), ## data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7745 -0.4583 -0.2813 -0.1517 4.4996 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.359769 0.263001 -20.379 &lt; 2e-16 *** ## SEXM -0.345084 0.064978 -5.311 1.09e-07 *** ## LOS -0.248553 0.009197 -27.025 &lt; 2e-16 *** ## AGE 0.081122 0.002995 27.082 &lt; 2e-16 *** ## DIAGNOSIS41011 -0.404406 0.157454 -2.568 0.01022 * ## DIAGNOSIS41021 -0.336452 0.254026 -1.324 0.18534 ## DIAGNOSIS41031 -0.784819 0.262089 -2.994 0.00275 ** ## DIAGNOSIS41041 -0.991677 0.158059 -6.274 3.52e-10 *** ## DIAGNOSIS41051 -0.235784 0.279752 -0.843 0.39932 ## DIAGNOSIS41071 -1.719416 0.178522 -9.631 &lt; 2e-16 *** ## DIAGNOSIS41081 -0.484542 0.226713 -2.137 0.03258 * ## DIAGNOSIS41091 -0.677723 0.145463 -4.659 3.18e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8889.4 on 12843 degrees of freedom ## Residual deviance: 6844.2 on 12832 degrees of freedom ## AIC: 6868.2 ## ## Number of Fisher Scoring iterations: 6 Note that DIED is 1 for people died, and 0 otherwise. So a negative coefficient indicates less likely to die, hence more likely to survive. Males are more likely to survive heart attack, compared with females of the same age, and with the same diagnosis. People who stayed longer are less likely to die. Older people are more likely to die. Compared with people with diagnosis code of 41001, those diagnosed with 41071, 41041, and 41091, are more likely to survive. Exercise 5.5 Use multiple linear regression to investigate the factors associated with length of stay. Obviously we need to exclude charges in our model. Interpret your results. This is not part of the exercise, but another problem we could look into is who are more likely to have complications. We should only focuse on the surviving patients and do logistic regression. We still need pair-wised examinations, because we cannot put two highly correlated factors in the same model. "],
["data-structures.html", "Chapter 6 Data structures 6.1 Basic concepts 6.2 Data structures 6.3 For more in-depth exercises", " Chapter 6 Data structures R data types and basic expressions1 : Common data structures in R include scalars, vectors, factors, matrices, data frames, and lists. These data structures can contain one or more individual data elements of several types, namely numeric (2.5), character (“Go Jacks”), or logical (TRUE or FALSE). 6.1 Basic concepts 6.1.1 Expressions Type anything at the prompt, and R will evaluate it and print the answer. 1 + 1 ## [1] 2 There’s your result, 2. It’s printed on the console right after your entry. Type the string “Go Jacks”. (Don’t forget the quotes!) &quot;Go Jacks&quot; ## [1] &quot;Go Jacks&quot; Exercise 6.1 Now try multiplying 45.6 by 78.9 6.1.2 Logical Values Some expressions return a “logical value”: TRUE or FALSE. (Many programming languages refer to these as “boolean” values.) Let’s try typing an expression that gives us a logical value: 3 &lt; 4 ## [1] TRUE And another logical value (note that you need a double-equals sign to check whether two values are equal - a single-equals sign won’t work): 2 + 2 == 5 ## [1] FALSE T and F are shorthand for TRUE and FALSE. Try this: T == TRUE ## [1] TRUE 6.1.3 Variables As in other programming languages, you can store a value into a variable to access it later. Type x = 42 to store a value in x. x is a scalar, with only one data element. x = 42 You can also use the following. This is a conventional, safer way to assign values. x &lt;- 42 x can now be used in expressions in place of the original result. Try dividing x by 2 (/ is the division operator), and other calculations. x / 2 ## [1] 21 log(x) ## [1] 3.73767 x^2 ## [1] 1764 sqrt(x) ## [1] 6.480741 x&gt;1 ## [1] TRUE You can re-assign any value to a variable at any time. Try assigning “Go Jacks!” to x. x &lt;- &quot;Go Jacks!&quot; You can print the value of a variable at any time just by typing its name in the console. Try printing the current value of x. x ## [1] &quot;Go Jacks!&quot; Now try assigning the TRUE logical value to x. x &lt;- TRUE You can store multiple values in a variable or object. That is called a vector, which is explained below. An object can also contain a table with rows and columns, like an Excel spreadsheet, as a matrix, or data frame. 6.1.4 Functions You call a function by typing its name, followed by one or more arguments to that function in parenthesis. Most of your R commands are functional calls. Let’s try using the sum function, to add up a few numbers. Enter: sum(1, 3, 5) ## [1] 9 Some arguments have names. For example, to repeat a value 3 times, you would call the rep function and provide its times argument: rep(&quot;Yo ho!&quot;, times = 3) ## [1] &quot;Yo ho!&quot; &quot;Yo ho!&quot; &quot;Yo ho!&quot; Exercise 6.2 Try to find and run the two functions that sets and returns the current working directory. Exercise 6.3 Try to find and run the function that lists all the files in the current working folder. 6.1.5 Looking for Help and Example Code ? sum A web page will pope up. This is the official help information for this function. At the bottom of the page is some example code. The quickest way to learn an R function is to run the example codes and see the input and output. You can easily copy, paste, and twist the example code to do your analysis. example() brings up examples of usage for the given function. Try displaying examples for the min function: example(min) ## ## min&gt; require(stats); require(graphics) ## ## min&gt; min(5:1, pi) #-&gt; one number ## [1] 1 ## ## min&gt; pmin(5:1, pi) #-&gt; 5 numbers ## [1] 3.141593 3.141593 3.000000 2.000000 1.000000 ## ## min&gt; x &lt;- sort(rnorm(100)); cH &lt;- 1.35 ## ## min&gt; pmin(cH, quantile(x)) # no names ## [1] -2.051145644 -0.818063058 -0.005700928 0.517765380 1.350000000 ## ## min&gt; pmin(quantile(x), cH) # has names ## 0% 25% 50% 75% 100% ## -2.051145644 -0.818063058 -0.005700928 0.517765380 1.350000000 ## ## min&gt; plot(x, pmin(cH, pmax(-cH, x)), type = &quot;b&quot;, main = &quot;Huber&#39;s function&quot;) ## ## min&gt; cut01 &lt;- function(x) pmax(pmin(x, 1), 0) ## ## min&gt; curve( x^2 - 1/4, -1.4, 1.5, col = 2) ## ## min&gt; curve(cut01(x^2 - 1/4), col = &quot;blue&quot;, add = TRUE, n = 500) ## ## min&gt; ## pmax(), pmin() preserve attributes of *first* argument ## min&gt; D &lt;- diag(x = (3:1)/4) ; n0 &lt;- numeric() ## ## min&gt; stopifnot(identical(D, cut01(D) ), ## min+ identical(n0, cut01(n0)), ## min+ identical(n0, cut01(NULL)), ## min+ identical(n0, pmax(3:1, n0, 2)), ## min+ identical(n0, pmax(n0, 4))) min(5:1, pi) # -&gt; one number ## [1] 1 Example commands and plots will show up automatically by typing Return in RStudio. In R, you need to click on the plots. example(boxplot) # bring example of boxplot I found a lot of help information about R through Google. Google tolerate typos, grammar errors, and different notations. Also, most (99 %) of your questions have been asked and answered on various forums. Many R gurus answered a ton of questions on web sites like** stackoverflow.com**, with example codes! I also use Google as a reference. It is important to add comments to your code. Everything after the “#” will be ignored by R when running. We often recycle and repurpose our codes. max(1, 3, 5) # return the maximum value of a vector ## [1] 5 6.2 Data structures 6.2.1 Vectors A vector is an object that holds a sequence of values of the same type. A vector’s values can be numbers, strings, logical values, or any other type, as long as they’re all the same type. They can come from a column of a data frame. if we have a vector x: x &lt;- c(5, 2, 22, 11, 5) Here c is for combine, do not use it as variable name. It is as special as you! Vectors cannot hold values with different modes (types). Try mixing modes and see what happens: c(1, TRUE, &quot;three&quot;) ## [1] &quot;1&quot; &quot;TRUE&quot; &quot;three&quot; All the values were converted to a single mode (characters) so that the vector can hold them all. To hold diverse types of values, you will need a list, which is explained later in this chapter. If you need a vector with a sequence of numbers you can create it with start:end notation. This is often used in loops and operations on the indices of vectors etc. Let’s make a vector with values from 5 through 9: 5:9 ## [1] 5 6 7 8 9 A more versatile way to make sequences is to call the seq function. Let’s do the same thing with seq: seq(5, 9) ## [1] 5 6 7 8 9 seq also allows you to use increments other than 1. Try it with steps of 0.5: seq(5, 9, .5) ## [1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 Exercise 6.4 Compute 1+2+3… +1000 with one line of R code. Hint: examine the example code for sum( ) function in the R help document. 6.2.1.1 Commands about vector Next we will try those commands about vector. First let’s find out what is the 4th element of our vector x, or the elements from 2 to 4. x[4] ## [1] 11 x[2:4] ## [1] 2 22 11 If you define the vector as y, y &lt;- x[2:4] No result is returned but you “captured” the result in a new vector, which holds 3 numbers. You can type y and hit return to see the results. Or do some computing with it. y &lt;- x[2:4]; y ## [1] 2 22 11 This does exactly the same in one line. Semicolon separates multiple commands. Now if we want to know the number of elements in the vector length(x) ## [1] 5 It’s also easy to know about the maximum, minimum, sum, mean and median individually or together. We can get standard deviation too. max(x) ## [1] 22 min(x) ## [1] 2 sum(x) ## [1] 45 mean(x) ## [1] 9 median(x) ## [1] 5 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2 5 5 9 11 22 sd(x) ## [1] 7.968689 rank() function ranks the elements. Ties are shown as the average of these ranks. While sort() will sort from the smallest to the biggest, decreasing = T will make it sort form the biggest to the smallest. rank(x) ## [1] 2.5 1.0 5.0 4.0 2.5 sort(x) ## [1] 2 5 5 11 22 sort(x, decreasing = T) ## [1] 22 11 5 5 2 diff() lag and iterate the differences of vector x. diff(x) ## [1] -3 20 -11 -6 rev() will reverse the position of the elements in the vector. rev(x) ## [1] 5 11 22 2 5 Operations are performed element by element. Same for log, sqrt, x^2, etc. They return vectors too. log(x) ## [1] 1.6094379 0.6931472 3.0910425 2.3978953 1.6094379 sqrt(x) ## [1] 2.236068 1.414214 4.690416 3.316625 2.236068 x^2 ## [1] 25 4 484 121 25 2*x + 1 ## [1] 11 5 45 23 11 If we don’t want the second element and save it as y: y &lt;- x[-2] y ## [1] 5 22 11 5 On the contrary, we can also add a new element to the end. These two commands get the same result. c(x, 7) ## [1] 5 2 22 11 5 7 append(x, 7) ## [1] 5 2 22 11 5 7 Sometimes we are interested in unique elements: unique(x) ## [1] 5 2 22 11 And the frequencies of the unique elements: table(x) ## x ## 2 5 11 22 ## 1 2 1 1 If we are interested in the index of the maximum or minimum: which.max(x) ## [1] 3 which.min(x) ## [1] 2 Or we need to look for the location of a special value: which(x == 11) ## [1] 4 We can randomly chose some elements from the vector. sample(x, 2) ## [1] 11 2 Elements in the vector can have names. Type “x” in the command window to see the difference. names(x) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;Amy&quot;, &quot;John&quot;) x ## x y z Amy John ## 5 2 22 11 5 Now we can refer to the elements by their names. x[&quot;Amy&quot;] ## Amy ## 11 If we have two vectors and try to compare them with each other: y &lt;- c(5, 11, 8) z &lt;- match(y, x) z ## [1] 1 4 NA match() returns the locations in 2nd vector. NA means missing, not found. To check if NA is in a vector, we use the function is.na( ). Note that the result is a vector holding logical values. Do we have missing value in our vector? is.na(x) ## x y z Amy John ## FALSE FALSE FALSE FALSE FALSE is.na(z) ## [1] FALSE FALSE TRUE Sometimes, when working with sample data, a given value isn’t available. But it’s not a good idea to just throw those values out. R has a value that explicitly indicates a sample was not available: NA. Many functions that work with vectors treat this value specially. For our z vector, try to get the sum of its values, and see what the result is: sum(z) ## [1] NA The sum is considered “not available” by default because one of the vector’s values was NA. This is the responsible thing to do; R won’t just blithely add up the numbers without warning you about the incomplete data. We can explicitly tell sum (and many other functions) to remove NA values before they do their calculations, however. Bring up documentation for the sum function: ? sum sum package:base R Documentation … As you see in the documentation, sum can take an optional named argument, na.rm. It’s set to FALSE by default, but if you set it to TRUE, all NA arguments will be removed from the vector before the calculation is performed. Try calling sum again, with na.rm parameter set to TRUE: sum(z, na.rm = TRUE) ## [1] 5 Exercise 6.5 Now compute the average of values in z. Ignore the missing values. 6.2.1.2 Scatter Plots of two vectors The plot function takes two vectors, one for X values and one for Y values, and draws a graph of them. Let’s draw a graph showing the relationship of numbers and their sines. x &lt;- seq(1, 20, 0.1) y &lt;- sqrt(x) Then simply call plot with your two vectors: plot(x, y) Great job! Notice on the graph that values from the first argument (x) are used for the horizontal axis, and values from the second (y) for the vertical. Exercise 6.6 Create a vector with 21 integers from -10 to 10, and store it in the x variable. Then create a scatterplot of x^2 against x. 6.2.1.3 Fish example of vector Once upon a time, Tom, Jerry, and Mickey went fishing and they caught 7, 3, and 9 fishes, respectively. This information can be stored in a vector, like this: c(7, 3, 9) ## [1] 7 3 9 The c() function (c is short for Combine) creates a new vector by combining a set of values. If we want to continue to use the vector, we hold it in an object and give it a name: fishes &lt;- c(7, 3, 9) fishes ## [1] 7 3 9 fishes is a vector with 3 data elements. There are many functions that operate on vectors. You can plot the vector: barplot(fishes) # see figure 6.1A You can compute the total: sum(fishes) ## [1] 19 We can access the individual elements by indices: fishes[3] ## [1] 9 Exercise 6.7 Does Mickey caught more fishes than Tom and Jerry combined? Write R code to verify this statement using the fishes vector and return a TRUE or FALSE value. Jerry protested that the ¼ inch long fish he caught and released per fishing rules was not counted properly. We can change the values in the 2nd element directly by: fishes[2] &lt;- fishes[2] + 1 On the left side, we take the current value of the 2nd element, which is 3, and add an 1 to it. The result (4) is assigned back to the 2nd element itself. As a result, the 2nd element is increased by 1. This is not an math equation, but a value assignment operation. More rigorously, we should write this as fishes[2] &lt;- fishes[2] + 1 We can also directly overwrite the values. fishes[2] &lt;- 4 fishes ## [1] 7 4 9 They started a camp fire, and each ate 1 fish for dinner. Now the fishes left: fishes2 &lt;- fishes - 1 fishes2 ## [1] 6 3 8 Most arithmetic operations work just as well on vectors as they do on single values. R subtracts 1 from each individual element. If you add a scalar (a single value) to a vector, the scalar will be added to each value in the vector, returning a new vector with the results. While they are sleeping in their camping site, a fox stole 3 fishes from Jerry’s bucket, and 4 fishes from Mickey’s bucket. How many left? stolen &lt;- c(0, 3, 4) # a new vector fishes2 - stolen ## [1] 6 0 4 If you add or subtract two vectors of the same length, R will take the corresponding values from each vector and add or subtract them. The 0 is necessary to keep the vector length the same. Proud of himself, Mickey wanted to make a 5ft x 5ft poster to show he is the best fisherman. Knowing that a picture worthes a thousand words, he learned R and started plotting. He absolutely needs his names on the plots. The data elements in a vector can have names or labels. names(fishes) &lt;- c(&quot;Tom&quot;, &quot;Jerry&quot;, &quot;Mickey&quot;) The right side is a vector, holding 3 character values. These values are assigned as the names of the 3 elements in the fishes vector. names is a built-in function. Our vector looks like: fishes ## Tom Jerry Mickey ## 7 4 9 barplot(fishes) # see figure 6.1B Figure 6.1: Simple Bar plot Assigning names for a vector also enables us to use labels to access each element. Try getting the value for Jerry: fishes[&quot;Jerry&quot;] ## Jerry ## 4 Exercise 6.8 Now see if you can set the value for Tom to something other than 5 using the name rather than the index. Tom proposes that their goal for next fishing trip is to double their catches. 2 * fishes ## Tom Jerry Mickey ## 14 8 18 Hopelessly optimistic, Jerry proposed that next time each should “square” their catches, so that together they may feed the entire school. sum(fishes ^ 2) ## [1] 146 Note that two operations are nested. You can obviously do it in two steps. Exercise 6.9 Create a vector representing the prices of groceries, bread $2.5, milk $3.1, jam $5.3, beer $9.1. And create a bar plot to represent this information. 6.2.2 Lists Vectors contain a set of values. But one vector can only contain one type of values, numbers, characters, or logical values. A list can store a series of objects of different types. y &lt;- list(height = 5,name = &quot;John Doe&quot;, BP = c(100, 77)) # a list with 3 components If we want one of the items in its original form, we can extract it with double square brackets: y[[3]] ## [1] 100 77 Alternatively, we can refer to the objects by using the dollar sign and the name of the object: y$BP ## [1] 100 77 Many R functions, such as t.test(), returns results as a list, which contain a series of components, such as a P value, a vector of residuals or coefficients, and even a matrix of data. A list is the natural way to represent this sort of thing as one big object that could be parsed. result &lt;- t.test(rnorm(100), rnorm(100)) # rnorm(100): 100 random number result # A list holds all results of t-test ## ## Welch Two Sample t-test ## ## data: rnorm(100) and rnorm(100) ## t = -1.4095, df = 197.68, p-value = 0.1603 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.50123891 0.08337985 ## sample estimates: ## mean of x mean of y ## 0.03719948 0.24612902 result$p.value # This retrieves just the P value ## [1] 0.1602518 result$estimate # this returns a vector containing two values ## mean of x mean of y ## 0.03719948 0.24612902 The help page of t.test contains information about what types of values are returned and their names. ? t.test Value A list with class “htest” containing the following components: statistic: … p.value: the p-value for the test. estimate: the estimated mean or difference in means depending on whether it was a one-sample test or a two-sample test. With this in mind, let’s run a simulation using a loop. What we want to do is to generate two sets of 100 random numbers from the standard normal distribution with zero mean and unit standard deviation, and perform t-test and get the P value. By repeating this process 500 times, we want to see the distribution of P values and count how many times we get significant result with P &lt; 0.05. pvalues &lt;- rep(1, 500) # define a vector containing 500 numbers, all equal to 1. for (i in 1:500) { # Loop: The values of i takes values from 1,2,3, …, 500 result = t.test(rnorm(100), rnorm(100)) pvalues[i] = result$p.value # P values are stored in the i-th element in the vector } hist(pvalues) # define summary(pvalues) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000943 0.2645922 0.5014414 0.5125963 0.7789147 0.9997134 sum(pvalues &lt; 0.05) # Count how many P values are less than 0.05. ## [1] 25 Exercise 6.10 Write R code to generate two vectors, each containing 100 random numbers from the standard normal distribution. Then compute Pearson’s correlation coefficient (PPC). Repeat 1000 times and plot the distribution of PPCs. Discuss your results briefly. Hint: You can use the above example code as a starting point. 6.2.3 Strings and string vectors We encounter text data sometimes. Plus, we also have row and column names. We can easily manipulate these string objects. Define a string x: “R is cool” x &lt;- &quot;R is cool&quot; If we want to know the number of character of the string: nchar(x) ## [1] 9 We can concatenate strings. By default a space is added. paste(x, &quot;!!&quot;) ## [1] &quot;R is cool !!&quot; If we want to extract sub-string from position of 6 and 9: substr(x, 6, 9) ## [1] &quot;cool&quot; Split string into a list which is separated by space: strsplit(x, &quot; &quot;) ## [[1]] ## [1] &quot;R&quot; &quot;is&quot; &quot;cool&quot; Find pattern “R” and replace with “Tim”: gsub(&quot;R&quot;, &quot;Tim&quot;, x) ## [1] &quot;Tim is cool&quot; Or remove space followed by anything: gsub(&quot; .*&quot;, &quot;&quot;, x) ## [1] &quot;R&quot; We can search for pattern like “is” in the string. grepl(&quot;is&quot;, x) ## [1] TRUE We can also convert the whole string into low case or upper case by “tolower” or “toupper” function. tolower(x) ## [1] &quot;r is cool&quot; toupper(x) ## [1] &quot;R IS COOL&quot; A string vector can hold many strings. This can be a column of names or IDs in a table. First let’s define a character vector x &lt;- c(“ab”, “cde”, “d”, “ab”). x &lt;- c(&quot;ab&quot;, &quot;cde&quot;, &quot;d&quot;, &quot;ab&quot;) We can use all commands about vector for our string vector, such as 2nd element in vector: x[2] ## [1] &quot;cde&quot; Number of strings in the vector: length(x) ## [1] 4 Unique elements: unique(x) ## [1] &quot;ab&quot; &quot;cde&quot; &quot;d&quot; Now we are interested in the duplicated element in the string vector. Is there any duplicated element in the vector? duplicated(x) ## [1] FALSE FALSE FALSE TRUE The last element is duplicated. F denotes FALSE, and T for TRUE. If we want to know the number of characters in each of the element: nchar(x) ## [1] 2 3 1 2 We can also unite two vectors x and y if we define another vector y first: y &lt;- c(&quot;ab&quot;, &quot;e&quot;) union(x, y) ## [1] &quot;ab&quot; &quot;cde&quot; &quot;d&quot; &quot;e&quot; Is there intercept among these two sets of strings? intersect(x, y) ## [1] &quot;ab&quot; We can add something like “Q” to each element: paste(x, &quot;Q&quot;) ## [1] &quot;ab Q&quot; &quot;cde Q&quot; &quot;d Q&quot; &quot;ab Q&quot; To get rid of the space between these element and “Q”, try paste0: paste0(x, &quot;Q&quot;) ## [1] &quot;abQ&quot; &quot;cdeQ&quot; &quot;dQ&quot; &quot;abQ&quot; If we want to collapse multiple strings into one, which is joined by space: paste(x, collapse = &quot; &quot;) ## [1] &quot;ab cde d ab&quot; Using these functions, we can achieve many things. For example if we have a piece of DNA sequence: DNA &lt;- &quot;taaCCATTGtaaGAACATGGTTGTCcaaaCAAGATGCTAGT&quot; Note that I am using the assignment operator “&lt;-”, instead of “=”, which also works most of the times but it could be ambiguous. First we need to convert everything to upper case. DNA &lt;- toupper(DNA) Next, we want to cut this DNA into smaller pieces by looking for a certain pattern “ATG”. This type of thing happens in nature, as some enzymes cut DNA according to certain pattern. segs &lt;- strsplit(DNA, &quot;ATG&quot;) The result is contained in an object segs, which is a list. We needed the unlist( ) function to convert list to a string vector. segs &lt;- unlist(segs) segs # a vector of strings ## [1] &quot;TAACCATTGTAAGAAC&quot; &quot;GTTGTCCAAACAAG&quot; &quot;CTAGT&quot; segs[1] # first segment ## [1] &quot;TAACCATTGTAAGAAC&quot; Exercise 6.11 In the iris flower dataset iris, define a new column called FullName which contains the full species name by adding “Iris “ in front of species name. In other words, “setosa” should become “Iris setosa”, “virginica” would be “Iris virginica”, and “versicolor” needs to be “Iris versicolor”. 6.2.4 Matrix operations A matrix has rows and columns, but it can only contain one type of values, i.e. numbers, characters, or logical values. Define a matrix using the Iris dataset. x &lt;- as.matrix(iris[1:10, 1:4]) First let’s show the first few rows. head(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 You can transform the matrix if you want, for the convenience of view and analysis. t(x) ## 1 2 3 4 5 6 7 8 9 10 ## Sepal.Length 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 ## Sepal.Width 3.5 3.0 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ## Petal.Length 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ## Petal.Width 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 We can produce a new matrix by each element is doubled and added 5 y &lt;- 2*x+5 y ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 15.2 12.0 7.8 5.4 ## 2 14.8 11.0 7.8 5.4 ## 3 14.4 11.4 7.6 5.4 ## 4 14.2 11.2 8.0 5.4 ## 5 15.0 12.2 7.8 5.4 ## 6 15.8 12.8 8.4 5.8 ## 7 14.2 11.8 7.8 5.6 ## 8 15.0 11.8 8.0 5.4 ## 9 13.8 10.8 7.8 5.4 ## 10 14.8 11.2 8.0 5.2 Also we can do other calculation by adding or multiplying matrices. x + y ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 20.3 15.5 9.2 5.6 ## 2 19.7 14.0 9.2 5.6 ## 3 19.1 14.6 8.9 5.6 ## 4 18.8 14.3 9.5 5.6 ## 5 20.0 15.8 9.2 5.6 ## 6 21.2 16.7 10.1 6.2 ## 7 18.8 15.2 9.2 5.9 ## 8 20.0 15.2 9.5 5.6 ## 9 18.2 13.7 9.2 5.6 ## 10 19.7 14.3 9.5 5.3 x * y ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 77.52 42.00 10.92 1.08 ## 2 72.52 33.00 10.92 1.08 ## 3 67.68 36.48 9.88 1.08 ## 4 65.32 34.72 12.00 1.08 ## 5 75.00 43.92 10.92 1.08 ## 6 85.32 49.92 14.28 2.32 ## 7 65.32 40.12 10.92 1.68 ## 8 75.00 40.12 12.00 1.08 ## 9 60.72 31.32 10.92 1.08 ## 10 72.52 34.72 12.00 0.52 We can also get a logical matrix using logical code like: x&gt;3 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 TRUE TRUE FALSE FALSE ## 2 TRUE FALSE FALSE FALSE ## 3 TRUE TRUE FALSE FALSE ## 4 TRUE TRUE FALSE FALSE ## 5 TRUE TRUE FALSE FALSE ## 6 TRUE TRUE FALSE FALSE ## 7 TRUE TRUE FALSE FALSE ## 8 TRUE TRUE FALSE FALSE ## 9 TRUE FALSE FALSE FALSE ## 10 TRUE TRUE FALSE FALSE Now if we want to know the mean and sum of these rows and columns, try rowMeans(), colMeans(), rowSums(), colSums(). rowMeans(x) ## 1 2 3 4 5 6 7 8 9 10 ## 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 colMeans(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 4.86 3.31 1.45 0.22 rowSums(x) ## 1 2 3 4 5 6 7 8 9 10 ## 10.2 9.5 9.4 9.4 10.2 11.4 9.7 10.1 8.9 9.6 colSums(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 48.6 33.1 14.5 2.2 Here we are computing the standard deviation by columns, using 2 for columns. apply(x, 2, sd) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 0.29135698 0.30713732 0.10801234 0.07888106 Or median by rows, using 1 for rows. apply(x, 1, median) ## 1 2 3 4 5 6 7 8 9 10 ## 2.45 2.20 2.25 2.30 2.50 2.80 2.40 2.45 2.15 2.30 Heatmap is my favorite type of graph for visualizing a large matrix data. heatmap(x, scale = &quot;column&quot;, margins = c(10,5)) Exercise 6.12 Take the first 4 columns of iris data set and format as a matrix. Compute the median for each column using the apply function. 6.2.5 Operations on data frames Once data is read in as data frame, these are commands you can use to analyze it. Read in data frame x: x &lt;- iris Using summary() we can get descriptive statistics of each column. summary(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## head() and tail() functions show the fist and last few rows. head(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica If we want to know both number of rows and number of columns of the data frame: dim(x) ## [1] 150 5 We can just get number of rows or number of columns separately: nrow(x) ## [1] 150 ncol(x) ## [1] 5 If we are interested in the column names or row names, which should be a vector of strings: colnames(x) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [5] &quot;Species&quot; rownames(x) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; ## [12] &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; ## [23] &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; ## [34] &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; ## [45] &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; ## [56] &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; ## [67] &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; &quot;74&quot; &quot;75&quot; &quot;76&quot; &quot;77&quot; ## [78] &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; &quot;82&quot; &quot;83&quot; &quot;84&quot; &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; ## [89] &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; &quot;96&quot; &quot;97&quot; &quot;98&quot; &quot;99&quot; ## [100] &quot;100&quot; &quot;101&quot; &quot;102&quot; &quot;103&quot; &quot;104&quot; &quot;105&quot; &quot;106&quot; &quot;107&quot; &quot;108&quot; &quot;109&quot; &quot;110&quot; ## [111] &quot;111&quot; &quot;112&quot; &quot;113&quot; &quot;114&quot; &quot;115&quot; &quot;116&quot; &quot;117&quot; &quot;118&quot; &quot;119&quot; &quot;120&quot; &quot;121&quot; ## [122] &quot;122&quot; &quot;123&quot; &quot;124&quot; &quot;125&quot; &quot;126&quot; &quot;127&quot; &quot;128&quot; &quot;129&quot; &quot;130&quot; &quot;131&quot; &quot;132&quot; ## [133] &quot;133&quot; &quot;134&quot; &quot;135&quot; &quot;136&quot; &quot;137&quot; &quot;138&quot; &quot;139&quot; &quot;140&quot; &quot;141&quot; &quot;142&quot; &quot;143&quot; ## [144] &quot;144&quot; &quot;145&quot; &quot;146&quot; &quot;147&quot; &quot;148&quot; &quot;149&quot; &quot;150&quot; str() is a very useful function, which shows data types for all columns. str(x) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Like string and vector, we can select one element in the data frame, like element in 2nd row and 3rd column. x[2, 3] ## [1] 1.4 Also we can subset a smaller data frame from x, such as columns 2 to 4 and rows 1 to 10: x[1:10, 2:4] ## Sepal.Width Petal.Length Petal.Width ## 1 3.5 1.4 0.2 ## 2 3.0 1.4 0.2 ## 3 3.2 1.3 0.2 ## 4 3.1 1.5 0.2 ## 5 3.6 1.4 0.2 ## 6 3.9 1.7 0.4 ## 7 3.4 1.4 0.3 ## 8 3.4 1.5 0.2 ## 9 2.9 1.4 0.2 ## 10 3.1 1.5 0.1 We can only retrieve all of the first column: x[, 1] ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 ## [18] 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 ## [35] 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 ## [52] 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 ## [69] 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 ## [86] 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 ## [103] 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 ## [120] 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 ## [137] 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 Using the data frame name x followed column name has the same effect. x$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 ## [18] 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 ## [35] 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 ## [52] 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 ## [69] 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 ## [86] 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 ## [103] 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 ## [120] 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 ## [137] 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 If we need to know the average Sepal Length, we use mean() function. By the way, expressions can be nested. mean(x$Sepal.Length) ## [1] 5.843333 It’s very common to select a subset of data by certain column. Note “==” is for comparison and “=” is for assign value. Also you can use logical symbols like “&gt;” ,“&lt;”. y &lt;- subset(x, Species == &quot;setosa&quot;) head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa subset(x, Sepal.Length &gt; 7) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 103 7.1 3.0 5.9 2.1 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 136 7.7 3.0 6.1 2.3 virginica We also can add new column named “id” to the data frame, which goes from 1 to 150. Function head() is used to examine. x$id &lt;- 1:150 head(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species id ## 1 5.1 3.5 1.4 0.2 setosa 1 ## 2 4.9 3.0 1.4 0.2 setosa 2 ## 3 4.7 3.2 1.3 0.2 setosa 3 ## 4 4.6 3.1 1.5 0.2 setosa 4 ## 5 5.0 3.6 1.4 0.2 setosa 5 ## 6 5.4 3.9 1.7 0.4 setosa 6 cbind() is for adding another exist column, like a column of random numbers y. y &lt;- rnorm(150) x2 &lt;- cbind(x, y) head(x2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species id y ## 1 5.1 3.5 1.4 0.2 setosa 1 1.26813483 ## 2 4.9 3.0 1.4 0.2 setosa 2 -0.06500598 ## 3 4.7 3.2 1.3 0.2 setosa 3 0.19988587 ## 4 4.6 3.1 1.5 0.2 setosa 4 0.61722072 ## 5 5.0 3.6 1.4 0.2 setosa 5 -0.64868923 ## 6 5.4 3.9 1.7 0.4 setosa 6 1.36103267 Similar as cbind(), rbind() is for adding another exist row or rows with same length of columns. Now we use tail() to examine. newRow &lt;- c(1, 1, 1, 1, &quot;setosa&quot;, 151) x3 &lt;- rbind(x, newRow) tail(x3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species id ## 146 6.7 3 5.2 2.3 virginica 146 ## 147 6.3 2.5 5 1.9 virginica 147 ## 148 6.5 3 5.2 2 virginica 148 ## 149 6.2 3.4 5.4 2.3 virginica 149 ## 150 5.9 3 5.1 1.8 virginica 150 ## 151 1 1 1 1 setosa 151 We can sort the data frame by certain column. For example we need to sort first column in ascending, or to sort second column in descending order. y &lt;- x[order(x[, 1]), ] head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species id ## 14 4.3 3.0 1.1 0.1 setosa 14 ## 9 4.4 2.9 1.4 0.2 setosa 9 ## 39 4.4 3.0 1.3 0.2 setosa 39 ## 43 4.4 3.2 1.3 0.2 setosa 43 ## 42 4.5 2.3 1.3 0.3 setosa 42 ## 4 4.6 3.1 1.5 0.2 setosa 4 y &lt;- x[rev(order(x[, 2])), ] head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species id ## 16 5.7 4.4 1.5 0.4 setosa 16 ## 34 5.5 4.2 1.4 0.2 setosa 34 ## 33 5.2 4.1 1.5 0.1 setosa 33 ## 15 5.8 4.0 1.2 0.2 setosa 15 ## 17 5.4 3.9 1.3 0.4 setosa 17 ## 6 5.4 3.9 1.7 0.4 setosa 6 We can view sepal length by species using boxplot. boxplot(Sepal.Length ~ Species, x) Or compute summary statistics by category, here average by species: aggregate(. ~ Species, x, mean) ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width id ## 1 setosa 5.006 3.428 1.462 0.246 25.5 ## 2 versicolor 5.936 2.770 4.260 1.326 75.5 ## 3 virginica 6.588 2.974 5.552 2.026 125.5 It is easier to conduct statistical analysis after organizing data as data frame. Analysis of variance, or ANOVA tests whether sepal length is the same cross species in data frame x. m &lt;- aov(Sepal.Length ~ Species, x) summary(m) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 63.21 31.606 119.3 &lt;2e-16 *** ## Residuals 147 38.96 0.265 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Regression analysis use linear model(lm) to analyze the relationship of these columns. Here we use sepal length as a function of sepal width and petal length plus error. m &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length, x) summary(m) ## ## Call: ## lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.96159 -0.23489 0.00077 0.21453 0.78557 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.24914 0.24797 9.07 7.04e-16 *** ## Sepal.Width 0.59552 0.06933 8.59 1.16e-14 *** ## Petal.Length 0.47192 0.01712 27.57 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3333 on 147 degrees of freedom ## Multiple R-squared: 0.8402, Adjusted R-squared: 0.838 ## F-statistic: 386.4 on 2 and 147 DF, p-value: &lt; 2.2e-16 We use data.frame() function to define a new data frame with 100 rows and two columns, ids and rand. y &lt;- data.frame(id = 1:100, rand = rnorm(100)) head(y) ## id rand ## 1 1 0.2230367 ## 2 2 -0.4808177 ## 3 3 -0.4212527 ## 4 4 0.4111252 ## 5 5 0.4948404 ## 6 6 -1.6344122 tail(y) ## id rand ## 95 95 0.41891571 ## 96 96 0.90210624 ## 97 97 -0.35872185 ## 98 98 -1.22419705 ## 99 99 0.04558426 ## 100 100 -0.46803430 Now we merge two data frames x and y by a common column called ids. Note the new data frame has only 100 rows. We can also merge data frame by row names too. z &lt;- merge(x, y, by = &quot;id&quot;) head(z) ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species rand ## 1 1 5.1 3.5 1.4 0.2 setosa 0.2230367 ## 2 2 4.9 3.0 1.4 0.2 setosa -0.4808177 ## 3 3 4.7 3.2 1.3 0.2 setosa -0.4212527 ## 4 4 4.6 3.1 1.5 0.2 setosa 0.4111252 ## 5 5 5.0 3.6 1.4 0.2 setosa 0.4948404 ## 6 6 5.4 3.9 1.7 0.4 setosa -1.6344122 summary(z) ## id Sepal.Length Sepal.Width Petal.Length ## Min. : 1.00 Min. :4.300 Min. :2.000 Min. :1.000 ## 1st Qu.: 25.75 1st Qu.:5.000 1st Qu.:2.800 1st Qu.:1.500 ## Median : 50.50 Median :5.400 Median :3.050 Median :2.450 ## Mean : 50.50 Mean :5.471 Mean :3.099 Mean :2.861 ## 3rd Qu.: 75.25 3rd Qu.:5.900 3rd Qu.:3.400 3rd Qu.:4.325 ## Max. :100.00 Max. :7.000 Max. :4.400 Max. :5.100 ## Petal.Width Species rand ## Min. :0.100 setosa :50 Min. :-2.92901 ## 1st Qu.:0.200 versicolor:50 1st Qu.:-0.68461 ## Median :0.800 virginica : 0 Median :-0.08454 ## Mean :0.786 Mean :-0.04543 ## 3rd Qu.:1.300 3rd Qu.: 0.70965 ## Max. :1.800 Max. : 2.03734 Exercise 6.13 A researcher studied the pathogenicity of fungi species on certain plants. Their results are in two files in the Datasets folder on D2L as a zipped file pathogenicity.zip. Once unzipped, it contains two files. The first (pathogenicity.txt) contains pathogenicity (the “Infection” column) in terms percentages plant tissue infected. Each isolate was used to infect 9 plants. The entire experiment is repeated according to expt column (1 or 2). The second file (species.txt) contains information about the genotypes identified by sequencing of the fungi DNA. Isolate ID links these two files. The goal is to compare the pathogens in terms of pathogenicity. These two files are available here and both are tab-deliminated text files. Create a folder, download the data files into it, and create an Rstudio project in the folder. Read in these two files. Note that for the species file, DO NOT treat Strings as factors. Unselect this in the import wizard. Keep a copy of the import command into your R script. Change the species name from “no matching sequences/no significant similarity” to “no match” Plot histogram of infection across all experiments. Merge these two files according to Isolate ID. Show the number of rows and columns of the merged data. Show the first few rows of data. Show the column names Show summary descriptive statistics for all columns Show the data types of all columns in the merged data Create a pie chart by Species (hint: tabulate frequencies first) Generate a boxplot to compare infection percentage across species Sort the combined data by infection and show the most infectious species with highest percentages. Compute average infection for each of the species (use aggregate) Run linear regression for infection as a function of species, expt, and plant. We want to know if the technical replicate factors have significant impact on the data. Interpret your results. Provide interpretation of the plots or quantitative analyses. Hint 1. Make sure the data is correctly imported in the desired type. Numeric, character or factor. Hint 2: Structure your R code by partition into parts with comments. 6.3 For more in-depth exercises Go to: http://tryr.codeschool.com/ 1 This material is based on http://tryr.codeschool.com/ "],
["vectors-1.html", "Chapter 7 Vectors", " Chapter 7 Vectors "],
["matrics-and-arrays.html", "Chapter 8 Matrics and arrays", " Chapter 8 Matrics and arrays "],
["lists-1.html", "Chapter 9 Lists", " Chapter 9 Lists "],
["data-frames.html", "Chapter 10 Data frames", " Chapter 10 Data frames "],
["strings.html", "Chapter 11 Strings", " Chapter 11 Strings "],
["advanced-topics.html", "Chapter 12 Advanced topics 12.1 Functions 12.2 Tidyverse 12.3 Shiny Apps and interactive plots", " Chapter 12 Advanced topics 12.1 Functions Many times, we want to re-use a chunk of code. The most efficient way is to wrap these code as a function, clearly define what the input and the output. Functions are fundamental building blocks of R. Most of the times when we run R commands, we are calling and executing functions. We can easily define our very own functions. For example, we have the following arithmetic function: \\[f(x)=1.5 x^3+ x^2-2x+1\\] Obviously, we can use the following code to do the computing: x &lt;- 5 1.57345 * x ^ 3 + x ^ 2 - 2 * x + 1 ## [1] 212.6813 This will work, but every time we have to re-write this code. So let’s try to define our own function: myf &lt;- function(x) { y = 1.57345 * x ^ 3 + x ^ 2 - 2 * x + 1 return(y) } Note that “{” and “}” signify the beginning and end of a block of code. “function” tells R that a function is going to be defined. At the end, the “return” statement returns the desired value. You can copy and paste the 4 lines of code to R and it defines a function called myf, which you can call by: myf(5) # or myf(x = 5) ## [1] 212.6813 As you can see you get the same results when x=5. But now you can use this in many ways. x &lt;- - 10 : 10 # x now is a vector with 21 numbers -10, -9, … 10 myf(x) ## [1] -1452.45000 -1047.04505 -724.60640 -475.69335 -290.86520 ## [6] -160.68125 -75.70080 -26.48315 -3.58760 2.42655 ## [11] 1.00000 1.57345 13.58760 46.48315 109.70080 ## [16] 212.68125 364.86520 575.69335 854.60640 1211.04505 ## [21] 1654.45000 plot(x, myf(x)) # see plot on the right. Obviously functions can handle many different calculations beyond arithmetic functions. It can take in one or more inputs and return a list of complex data objects too. Exercise 12.1 Write an R function to implement this arithmetic function: f(x)= √(|x|)+5x-6. Note |x| means the absolute value of x. Use this function to find f(4.534), and also try to produce a plot like the one above. Exercise 12.2 Go over the R cheat sheet, find some function that was not covered in class and show some demo. 12.2 Tidyverse Tidyverse is collection of powerful R packages. The packages include ggplot2, dplyr, readr, purr, tidyr, and tibble. They were all written by Hadley Wickham, a true hero in the open-source R world. Following the same design philosophy and grammar, these powerful packages are designed to make R code easier to read. As they are more intuitive, some people argue that beginners should start by learning them, instead of the base R. We even already start to use this package in chapter 5. Now Let’s explain the dplyr package in a little detail for manipulating iris data set. #install.packages(&quot;dplyr&quot;) library(dplyr) In dplyr, we use the pipe operator %&gt;% to send data to the next stage. This is similar to the “+” operator we used in ggplot2. To create a new data frame for setosa with sepals longer than 4.0: iris %&gt;% filter(Species == &quot;setosa&quot;, Sepal.Length &gt; 4) Add a new column that contains the ratios of sepal length to sepal width: iris %&gt;% mutate(ratio = Sepal.Length / Sepal.Width) Sort by sepal length in ascending order: iris %&gt;% arrange(Sepal.Length) The power of dplyr is that we can connect these pipe operators to define a work flow. Suppose we want to see the Iris setosa flowers with the largest ratio of sepal length to sepal width. iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% # filter rows select(Sepal.Length, Sepal.Width) %&gt;% # select two columns mutate(ratio = Sepal.Length / Sepal.Width) %&gt;% # add a new column arrange(desc(ratio)) %&gt;% # sort in descending order head() # only show top rows. No more pipes, end of sequence. ## Sepal.Length Sepal.Width ratio ## 1 4.5 2.3 1.956522 ## 2 5.0 3.0 1.666667 ## 3 4.9 3.0 1.633333 ## 4 4.8 3.0 1.600000 ## 5 4.8 3.0 1.600000 ## 6 5.4 3.4 1.588235 filter( ), mutate( ) and arrange( ) are 3 “verbs” that operate on the data frame sequentially. head( ) is the function that only shows the top rows. Notice the pipe operator %&gt;% at the end of each line. This code is much easier to read by humans, as it defines a sequence of operations. Two other useful verbs are group_by( ) and summarise( ). They can be used to generate summary statistics. Below, we use group_by to split the data frame into 3 data frames by the species information, compute the mean of sepal lengths and width, and then combine. So it is “split-apply-combine”. iris %&gt;% group_by(Species) %&gt;% # split by Species summarise(avgSL = mean(Sepal.Length), avgSW = mean(Sepal.Width)) %&gt;% arrange(avgSL) ## # A tibble: 3 x 3 ## Species avgSL avgSW ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 ## 2 versicolor 5.94 2.77 ## 3 virginica 6.59 2.97 Here we created a new data frame with the mean sepal length and sepal width for each of the 3 species. Obviously, we can change mean( ) to many other functions. This makes it very easy to summarize large data sets. Exercise 12.3 Read in the heart attack data, calculate the average cost per day for patients with different DIAGNOSIS codes. Restrict to females aged between 20 and 70 who stayed at least one day. Sort the results in descending order. Use the dplyr package. Use one command with multiple steps. Note: since we have missing values in CHARGES, remember to use the na.rm =TRUE option in the mean( ) function. Hint: Build your code step by step. Test each step to make sure they work separately as desired. You can use the head or summary function at the end to examine if the desired data is produced. 12.3 Shiny Apps and interactive plots Recent developments in R made it easy to create interactive charts and even complex websites. Without any web development experience, I created a site entirely in R ( iDEP http://ge-lab.org/idep/ ) to enable biologists to analyze genomic data on their own. My blog (http://gex.netlify.com ) is also created in Rstudio. Install the Shiny package by typing this in the console. #install.packages(&quot;shiny&quot;) Create a Shiny web app is a piece of cake Start a new Shiny app use the shortcut shown in Figure 13. Or, select File -&gt; New File -&gt; Shiny Web App from the RStudio main menu. Give your app a name like test1 and note where the app will be stored. The nice nerds at Rstudio understand the power of an example. A small, but functional app is shown, defined in a file called app.R. Click on Run App on the top right of the script window; you have a histogram that can be customized by a sliding bar. There are two functions: ui() defines the user interface, and server() specifies the logic. Let’s play! Change the color by changing the col = ‘darkgray’ in line 44 to your favorite color, such as ‘green’, ‘red’, etc. To make it colorful, set it to col = rainbow(10) so we can use ten colors on a rolling basis. Switch the data. Change line 40 to visualize the iris dataset: x &lt;- iris[,‘Sepal.Length’] The default number of bins of 30, specified at line 25, is probably too big. Change it to: value = 12 We obviously can change ‘Sepal.Length’ to other columns. Would it be cool if the user can choose? We need to add a control widget by inserting this line of code after line 20: selectInput(“cid”, “Column”, choices = colnames( iris ) ), Do not forget the comma at the very end! Now run this app. We can now select the columns, but the app does not do anything with it. We need to use the selected column in the server logic to customize the plot. We will change the line 41 to this: x &lt;- iris[ , input$cid ] Exercise 12.4 Find and solve the error in this app when choosing the last column by limiting to first 4 columns. Exercise 12.5 Change the title from “Histogram of x” to the name of the selected variable like “Sepal.Length” We can build a more complex app by adding a normal distribution line by expanding line 44 into these: h &lt;- hist(x, breaks = bins, col = rainbow(10), border = ‘white’) yfit &lt;- dnorm(bins, mean = mean(x), sd = sd(x)) yfit &lt;- yfit * diff( h$mids[1:2]) * length(x) lines(bins, yfit, col = “blue”) Exercise 12.6 Solve the error message in this app by plotting a pie chart when the Species column is selected. Exercise 12.7 Publish your app online by clicking on the Publish button at the top right of the app window and following the instructions. And show it off to your friend by sending them an URL. Solutions to these challenges can be found at GitHub https://github.com/gexijin/teach/blob/master/app.R To learn more, follow these excellent tutorials: https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/ Interactive plots made easy with Plotly #install.packages(&quot;plotly&quot;) library(plotly) g &lt;- ggplot(iris, aes(Petal.Width, Petal.Length , color = Species)) + geom_point() ggplotly( g ) We first generated the plot using ggplot2 and stored it in an object g, which is rendered interactive with Plotly. If you mouse over the plot, the values are highlighted. You can also select an area on the chart to zoom in. The R community is uniquely supportive. There are lots of free online books, tutorials, example codes, etc. Here are some helpful websites and information: "],
["data-exploring-of-state-dataset.html", "Chapter 13 Data exploring of state dataset", " Chapter 13 Data exploring of state dataset "],
["data-exploring-of-game-sale-dataset.html", "Chapter 14 Data exploring of game sale dataset", " Chapter 14 Data exploring of game sale dataset "],
["data-exploring-of-employee-salary-dataset.html", "Chapter 15 Data exploring of employee salary dataset", " Chapter 15 Data exploring of employee salary dataset "]
]
