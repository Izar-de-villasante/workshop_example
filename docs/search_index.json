[
["index.html", "Learning R through examples Index", " Learning R through examples Xijin Ge 2019-03-18 Index Chapter 1 Step into R program Chapter 2 Visualizing data set Chapter 3 Data structures Chapter 4 Data importing Chapter 5 Heart attack data set I Chapter 6 Heart attack data set II Chapter 7 Advanced topics Chapter 8 State data set Chapter 9 Game sale data set Chapter 10 Employee salary data set "],
["step-into-r-program-analyzing-iris-flower-dataset.html", "Chapter 1 Step into R program-Analyzing iris flower dataset 1.1 Data frames have rows and columns: the Iris flower dataset 1.2 Analyzing one set of numbers 1.3 Student’s t-test 1.4 Test for normal distribution 1.5 Analyzing a column of categorical values 1.6 Analyzing the relationship between two columns of numbers 1.7 Visualizing and testing the differences in groups 1.8 Testing the difference among multiple groups (Analysis of Variance: ANOVA)", " Chapter 1 Step into R program-Analyzing iris flower dataset Getting started Install R from www.R-project.org. Choose the cloud server or any U.S. mirror site. Install RStudio Desktop from www.RStudio.com. Rstudio uses the R software you just installed in the background, but provides a more user-friendly interface. We will use Rstudio. R commands can be typed directly into the “Console” window. Or you can enter them in the “R Script” window and click the “Run” button. Just try all of these commands and guess what’s going on. If it takes a few months to type these 188 characters, try www.RapidTyping.com. 1.1 Data frames have rows and columns: the Iris flower dataset In 1936, Edgar Anderson collected data to quantify the geographic variation of Iris flowers. The data set consists of 50 samples from each of three sub-species ( Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the lengths and the widths of sepals and petals, in centimeters (cm). This data is included in R software. Go to the Wikipedia page for this data set (yes, it is famous!). Have a quick look at the data there, think about what distinguishes the three species? If we have a flower with sepals of 6.5cm long and 3.0cm wide, petals of 6.2cm long, and 2.2cm wide, which species does it most likely belong to? Think (!) for a few minutes while eyeballing the data at Wikipedia. Figure 1.1: Iris flower. Photo from Wikipedia. Figure 1.2: Example of a data frame. To answer these questions, let’s visualize and analyze the data with R. Type these commands without the comments after “#”. iris #This will print the whole dataset, which is included with R dim(iris) # show the dimension of the data frame: 150 rows and 5 columns. head(iris) # show the first few rows; useful for bigger datasets. So the first 4 columns contain numeric values. The last one is species information as character values. This is an important distinction, as we cannot add or subtract character values. This object is a data frame, with both numeric and character columns. A matrix only contains one type of values, often just numbers. To have a look at the data in a spreadsheet, we can use the fix( ) function. #fix(iris) # examine data frame in a spreadsheet. Click on column names to double-check data types (numeric vs. character). Sometimes we need to overwrite data types guessed by R. For example, sometimes we use 1 for male and 0 for female. These are essentially categories; Values like 1.6 make no sense. In this case we need to enforce this column as characters. Note this window needs to be closed before proceeding to the next. View(iris) # this Rstudio function also shows data. Note R is case sensitive. Individual values in a data frame can be accessed using row and column indices. iris[3, 4] # shows the value in 3rd row, 4th column. It is 0.2. iris[3, ] # shows all of row 3 iris[, 4] # shows all of column 4 iris[3, 1:4] # shows row 3, columns 1 to 4. Exercise 1.1 Display data in rows 1 to 10 from columns 2 to 5. Start a new Word document. Copy and paste your R code and the results to the document and save it as PDF. colnames(iris) # Column names. ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [5] &quot;Species&quot; Remember these column names, as we are going to use them in our analysis now. Note that sepal length information is contained in the column named Sepal.Length. Since R is case sensitive, we have to type these column names exactly as above. attach(iris) # attach dataset to R working memory, so that columns can be accessible by name. Petal.Length # after attaching, we can just use column names to represent a column of numbers as a vector R is case-sensitive. “petal.length” will not be recognized. mean(Petal.Length) # mean( ) is a function that operates on Petal.Length, a vector of 150 umbers ## [1] 3.758 Exercise 1.2 Compute average sepal length. Hint: replace Petal.Length with Sepal.Length. The best way to learn about other R functions is Google search. Exercise 1.3 Google “R square root function” to find the R function, and compute the value of \\(\\sqrt(123.456)\\). 1.2 Analyzing one set of numbers x &lt;- Petal.Length # I am just lazy and don’t want to type “Petal.Length”, repeatedly. summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 4.350 3.758 5.100 6.900 The minimum petal length is 1.0, and the maximum is 6.9. Average petal length is 3.758. The mid-point or median is 5.35, as about half of the numbers is smaller than 5.35. Why the median is different from the mean? What happens if there is a typo and one number is entered 340cm instead of 3.40cm? The 3rd quartile, or 75th percentile is 5.1, as 75% of the flowers has petals shorter than 5.1. The 95th percentile for the weight of 2-year-old boy is 37 pounds. If a 2-year-old boy weighs 37 pounds, he is heavier than 95% of his peers. If a student’s GPA ranks 5th in a class of 25, he/she is at 80th percentile. The 1st quartile, or 25th percentile is 1.6. Only 25% of the flowers has petals shorter than 1.6. These summary statistics are graphically represented as a boxplot in the Figure 1.3A. Boxplots are more useful when multiple sets of numbers are compared. boxplot(x) # Figure 1.3A. It graphically represents the spread of the data. boxplot(iris[, 1:4]) # boxplot of several columns at the same time Figure 1.3B. Figure 1.3: Boxplot of petal length (A) and of all 4 columns (B). In Rstudio, you can copy a plot to clipboard using the Export button on top of the plot area. Or you can click zoom, right click on the popup plot and select “Copy Image”. Then you can paste the plot into Word. If you are using R software, instead of Rstudio, you can right click on the plots and copy as meta-file. Exercise 1.4 What can we tell from this boxplot (Figure 1.3B)? Summarize your observations in PLAIN English. Note the differences in median and the spread (tall boxes). What is the most variable characteristics? To quantify the variance, we can compute the standard deviation σ: \\[\\begin{align} σ=\\sqrt{\\frac{1}{N}[(x_{1}-u)^2+(x_{2}-u)^2+...+(x_{N}-u)^2]} \\end{align}\\] where \\[\\begin{align} u=\\frac{1}{N}(x_{1}+x_{2}+...x_{N}) \\end{align}\\] If all the measurements are close to the mean (µ), then standard deviation should be small. sd(x) # sd( ) is a function for standard deviation ## [1] 1.765298 sd(Sepal.Width) ## [1] 0.4358663 As we can see, these flowers have similar sepal width. They differ widely in petal length. This is consistent with the boxplot above. Perhaps changes in petal length lead to better survival in different habitats. With R it is very easy to generate graphs. barplot(x) Figure 1.4: Barplot of petal length As we can see, the first 50 flowers (Iris setosa) have much shorter petals than the other two species. The last 50 flowers (Iris verginica) have slightly longer petals than the middle (iris versicolor). plot(x) # Run sequence plot hist(x) # histogram lag.plot(x) qqnorm(x) # Q-Q plot for normal distribution qqline(x) Figure 1.5: Sequence plot, histogram, lag plot and normal Q-Q plot. Histogram shows the distribution of data. The histogram top right of Figure 1.5 shows that there are more flowers with Petal Length between 1 and 1.5. It also shows that the data does not show a bell-curved distribution. Lag plot is a scatter plot against the same set of number with an offset of 1. Any structure in lag plot indicate non-randomness in the order in which the data is presented. Q-Q plot can help check if data follows a Gaussian distribution, which is widely observed in many situations. Also referred to as normal distribution, it is the pre-requisite for many statistical methods. See Figure 1.6 for an example of normal distribution. Quantiles of the data is compared against those in a normal distribution. If the normal Q-Q plot is close to the reference line produced by qqline( ), then the data has a normal distribution. Exercise 1.5 Generate 500 random numbers from the standard normal distribution and generate sequence plots, histogram, lag plot, and Q-Q plot. You should get plots like those in Figure 1.6. Hint: Run x = rnorm(500) to generate these numbers, and then re-run the last 5 lines of code on this page. They are also shown on Figure 1.5. (ref:1-5) Plots for random generated numbers following a normal distribution. This is for reference. Figure 1.6: (ref:1-5) Exercise 1.6 Investigate Septal Length distribution using these techniques and summarize your observations in PLAIN English. Hint: assign x with the values in the sepal length column (x = Sepal.Length), and re-run all the code in this section. 1.3 Student’s t-test In hypothesis testing, we evaluate how likely the observed data can be generated if a certain hypothesis is true. If this probability (p value) is very small (&lt; 0.05, typically), we reject that hypothesis. Are the petal lengths of iris setosa significantly different from these of iris versicolor? x &lt;- Petal.Length[1:50] # the first 50 values of Sepal.Length are for iris setosa y &lt;- Petal.Length[51:100] # the next 50 values of Sepal.Length are for iris versicolor t.test(x, y) # t.test( ) is an R function for student’s t-test ## ## Welch Two Sample t-test ## ## data: x and y ## t = -39.493, df = 62.14, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.939618 -2.656382 ## sample estimates: ## mean of x mean of y ## 1.462 4.260 The null hypothesis is that the true mean is the same. Since p value is really small, we reject this hypothesis. Iris versicolor has longer sepals than iris setosa. Exercise 1.7 Are iris setosa and iris versicolor significantly different in sepal width? Hint: Replace Petal.Length with something else and re-run above code. We can also do t-test on one set of numbers. This is a one-sample t-test of mean: t.test(Sepal.Length, mu = 5.8) ## ## One Sample t-test ## ## data: Sepal.Length ## t = 0.64092, df = 149, p-value = 0.5226 ## alternative hypothesis: true mean is not equal to 5.8 ## 95 percent confidence interval: ## 5.709732 5.976934 ## sample estimates: ## mean of x ## 5.843333 In this case, our hypothesis is that the true average of sepal length for all iris flowers is 5.8. Since p value is quite big, we accept this hypothesis. This function also tells us the 95% confidence interval on the mean. Based on our sample of 150 iris flowers, we are 95% confident that the true mean is between 5.71 and 5.98. Exercise 1.8 Compute 95% confidence interval of petal length. 1.4 Test for normal distribution We can perform hypothesis testing on whether a set of numbers derived from normal distribution. The null hypothesis is that the data is from a normal distribution. shapiro.test(Petal.Length) ## ## Shapiro-Wilk normality test ## ## data: Petal.Length ## W = 0.87627, p-value = 7.412e-10 If petal length is normally distributed, there is only 7.412×10-10 chance of getting a test statistic of 0.87627, which is observed in our sample of 150 flowers. In other words, it is highly unlikely that petal length follows a normal distribution. We reject the normal distribution hypothesis. Exercise 1.9 Is sepal width normally distributed? Run Shapiro’s test and also generate histogram and normal Q-Q plot. 1.5 Analyzing a column of categorical values In the iris dataset, the last column contains the species information. These are “string” values or categorical values. counts &lt;- table(Species) # tabulate the frequencies counts ## Species ## setosa versicolor virginica ## 50 50 50 pie(counts) # See Figure 1.7A barplot(counts) # See Figure 1.7B Figure 1.7: Frequencies of categorical values visualized by Pie chart (A) and bar chart (B). Pie charts are very effective in showing proportions. We can see that the three species are each represented with 50 observations. 1.6 Analyzing the relationship between two columns of numbers Scatter plot is very effective in visualizing correlation between two columns of numbers. attach(iris) # attach the data set x &lt;- Petal.Width # just lazy y &lt;- Petal.Length plot(x, y) # scatterplot, refined version in Figure 1.9 Figure 1.8: Scatter plot of petal width and petal length. Figure 1.8 shows that there is a positive correlation between petal length and petal width. In other words, flowers with longer petals are often wider. So the petals are getting bigger substantially, when both dimensions increase. Another unusual feature is that there seems to be two clusters of points. Do the points in the small cluster represent one particular species of Iris? We need to further investigate this. The following will produce a plot with the species information color-coded. The resultant Figure 1.9 clearly shows that indeed one particular species, I. setosa constitutes the smaller cluster in the low left. The other two species also show difference in this plot, even though they are not easily separated. This is a very important insight into this dataset. plot(x, y, col = rainbow(3)[Species]) # change colors based on another column (Species). legend(&quot;topleft&quot;, levels(Species), fill = rainbow(3)) # add legends on topleft. Figure 1.9: Scatter plot shows the correlation of petal width and petal length. The rainbow( ) function generates 3 colors and Species information is used to choose colors. Note that Species column is a factor, which is a good way to encode columns with multiple levels. Internally, it is coded as 1, 2, 3. str(iris) # show the structure of data object ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Perhaps due to adaption to environment, change in petal length lead to better survival. With the smallest petals, Iris Setosa is found in Arctic regions. Iris versicolor is often found in the Eastern United States and Eastern Canada. Iris virginica “is common along the coastal plain from Florida to Georgia in the Southeastern United States [Wikipedia].” It appears the iris flowers in warmer places are much larger than those in colder ones. With R, it is very easy to generate lots of graphics. But we still have to do the thinking. It requires to put the plots in context. We can quantitatively characterize the strength of the correlation using several types of correlation coefficients, such as Pearson’s correlation coefficient, r. It ranges from -1 to 1. cor(x, y) ## [1] 0.9628654 This means the petal width and petal length are strongly and positively correlated. cor.test(x, y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 43.387, df = 148, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9490525 0.9729853 ## sample estimates: ## cor ## 0.9628654 Through hypothesis testing of the correlation, we reject the null hypothesis that the true correlation is zero. That means the correlation is statistically significant. Note that Pearson’s correlation coefficient is not robust against outliers and other methods such as Spearman’s exists. See help info: ?cor # show help info on cor ( ) We can also determine the equation that links petal length and petal width. This is so called regression analysis. We assume Petal.Length = a × Petal.Width + c + e, where a is the slope parameter, c is a constant, and e is some random error. This linear model can be determined by a method that minimizes the least squared-error: model &lt;- lm(y ~ x) # Linear Model (lm): petal length as a function of petal width summary(model) # shows the details ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.33542 -0.30347 -0.02955 0.25776 1.39453 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.08356 0.07297 14.85 &lt;2e-16 *** ## x 2.22994 0.05140 43.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4782 on 148 degrees of freedom ## Multiple R-squared: 0.9271, Adjusted R-squared: 0.9266 ## F-statistic: 1882 on 1 and 148 DF, p-value: &lt; 2.2e-16 As we can see, we estimated that a=2.22944 and c=1.08356. Both parameters are significantly different from zero as the p values are &lt;2×10-16 in both cases. In other words, we can reliably predict Petal.Length = 2.22944 × Petal.Width + 1.08356. This model can be put on the scatter plot as a line. plot(model) abline(model) # add regression line to existing scatter plot. Finishes Figure 1.8. Sometimes, we use this type of regression analysis to investigate whether variables are associated. Exercise 1.10 Investigate the relationship between sepal length and sepal width using scatter plots, correlation coefficients, test of correlation, and linear regression. Again interpret all your results in PLAIN and proper English. 1.7 Visualizing and testing the differences in groups Are boys taller than girls of the same age? Such situations are common. We have measurements of two groups of objects and want to know if the observed differences are real or due to random sampling error. attach(iris) # attach iris data boxplot(Petal.Length ~ Species) # Generate boxplot: Petal length by species, see Figure 1.11 Figure 1.10: Boxplot of petal length, grouped by species. From the boxplot, it is obvious that I. Setosa has much shorter petals. But are there significant differences between I. versicolor and I. virginica? We only had a small sample of 50 flowers for each species. But we want to draw some conclusion about the two species in general. We could measure all the iris flowers across the world; Or we could use statistics to make inference. First we need to extract these data x &lt;- Petal.Length[51:100] # extract Petal Length of iris versicolor, from No.51 to No.100 x # x contain 50 measurements y &lt;- Petal.Length[101:150] # extract Petal length of iris virginica, from No. 101 to No. 150 y # y contain 50 measurements boxplot(x, y) # a boxplot of the two groups of values t.test(x, y) ## ## Welch Two Sample t-test ## ## data: x and y ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean of x mean of y ## 4.260 5.552 In this student’s t-test, our null hypothesis is that the mean petal length is the same for I. versicolor and I. virginica. A small p value of 2.2x10-16 indicates under this hypothesis, it is extremely unlikely to observe the difference of 1.292cm through random sampling. Hence we reject that hypothesis and conclude that the true mean is different. If we measure all I. versicolor and I. virginica flowers in the world and compute their true average petal lengths, it is very likely that the two averages will differ. On the other hand, if p value is larger than a threshold, typically 0.05, we will accept the null hypothesis and conclude that real average petal length is the same. We actually do not need to separate two set of numbers into two data objects in order to do t-test or compare them side-by-side on one plot. We can do it right within the data frame. R can separate data points by another column. x2 &lt;- iris[51:150, ] # Extract rows 51 to 150 t.test(Petal.Length ~ Species, data = x2) # t-test of Petal.Length column, divided by the Species column in x2. boxplot(Petal.Length ~ Species, data = droplevels(x2)) # droplevels( ) removes empty levels in Species Exercise 1.11 Use boxplot and t-test to investigate whether sepal width is different between I. versicolor and I. virginica. Interpret your results. 1.8 Testing the difference among multiple groups (Analysis of Variance: ANOVA) As indicated by Figure 1.10, sepal width has small variation, even across 3 species. We want to know if the mean sepal width is the same across 3 species. This is done through Analysis of Variance (ANOVA). boxplot(Sepal.Width ~ Species) # Figure 1.12 Figure 1.11: Boxplot of sepal width across 3 species. summary(aov(Sepal.Width ~ Species)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since p value is much smaller than 0.05, we reject the null hypothesis. The mean sepal width is not the same for 3 species. This is the only thing we can conclude from this. The boxplot in Figure 1.11 seems to indicate that I. Setosa has wider sepals. Now we demonstrate how to use the lattice package for visualizing data using multiple panels. This package is not included in the base version of R. They need to be downloaded and installed. One of main advantages of R is that it is open, and users can contribute their code as packages. If you are using Rstudio, you can choose Tools-&gt;Install packages from the main menu, and then enter the name of the package. If you are using R software, you can install additional packages, by clicking Packages in the main menu, and select a mirror site. These mirror sites all work the same, but some may be faster. Lately I just use cloud mirror. After choosing a mirror and clicking “OK”, you can scroll down the long list to find your package. Alternatively, you can type this command to install packages. #install.packages (&quot;ggplot2&quot;) # choose the cloud mirror site when asked Packages only need to be installed once. But every time you need to use a package, you need to load it from your hard drive. library(ggplot2) # load the ggplot2 package Figure 1.12A shows the sepal width distribution for each of the 3 species. The 3 histograms are aligned in panels: layout of 1 column and 3 rows. Figure 1.12A shows an interesting feature in how sepal width is distributed. While for I. setosa it is skewed to the right. For the other two species it is skewed to the left. We can also have density plots side-by-side (Figure 1.12B): ggplot(iris, aes(x = Sepal.Width, group = Species, y = c(..count..[..group.. == 1]/sum(..count..[..group.. == 1]), ..count..[..group.. == 2]/sum(..count..[..group.. == 2]), ..count..[..group.. == 3]/sum(..count..[..group.. == 3])) * 100)) + geom_histogram(binwidth = .2, colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + facet_grid(Species ~ .) + labs(y = &quot;Percent of Total&quot;) ggplot(iris, aes(x = Petal.Length, fill = Species)) + geom_density(alpha = .3) Figure 1.12: Visualizing data using the ggplot2 package. Exercise 1.12 Use boxplot, multiple panel histograms and density plots to investigate whether petal width is the same among three subspecies. "],
["visualizing-data-set-analyzing-cars-and-iris-flower-data-sets.html", "Chapter 2 Visualizing data set-Analyzing cars and iris flower data sets 2.1 Basic concepts of R graphics 2.2 Visualizing mtcars dataset 2.3 Visualizing iris data set", " Chapter 2 Visualizing data set-Analyzing cars and iris flower data sets 2.1 Basic concepts of R graphics In addition to the graphics functions in base R, there are many other packages we can use to create graphics. The most widely used are lattice and ggplot2. Together with base R graphics, sometimes these are referred to as the three independent paradigms of R graphics. The lattice package extends base R graphics and enables the creating of graphs in multiple facets. The ggplot2 is developed based on a so-called Grammar of Graphics (hence the “gg”), a modular approach that builds complex graphics using layers. Note the recommended textbook R Graphics Cookbook includes all kinds of R plots and code. Some are online: http://www.cookbook-r.com/Graphs/. There are also websites lists all sorts of R graphics and example codes that you can use. http://www.r-graph-gallery.com/ contains more than 200 such examples. Another one is here: http://bxhorn.com/r-graphics-gallery/ We start with base R graphics. The first import distinction should be made about high- and low-level graphics functions in base R. See this table. Figure 2.1: List of graphics functions in base R. Sometimes we generate many graphics quickly for exploratory data analysis (EDA) to get some sense of how the data looks like. We can achieve this by using plotting functions with default settings to quickly generate a lot of “plain” plots. R is a very powerful EDA tool. However, you have to know what types of graphs are possible for the data. Other times, we want to generate really “cool”-looking graphics for papers, presentations. Making such plots typically requires a bit more coding, as you have to add different parameters easily understood. For me, it usually involves some google searches of example codes, and then I revise it via trial-and-error. If I cannot make it work, I read the help document. 2.2 Visualizing mtcars dataset 2.2.1 scatter plot The mtcars data set is included in base R. It contains various statistics on 32 different types of cars from the 1973-74 model year. The data was obtained from the 1974 Motor Trend US magazine. Our objective is to use this dataset to learn the difference between them, possibly for choosing a car to buy. mtcars # show the mtcars dataset ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ? mtcars # shows the information on this dataset 2.2.1.1 Customize scatterplots We start with a basic scatter plot. First, we attach the mtcars data to memory so that we can refer to the columns directly by their names. attach(mtcars) # attach dataset to memory ## The following object is masked from package:ggplot2: ## ## mpg plot(wt, mpg) # weight (wt) and miles per gallon (mpg), see Figure 2.3 This generates a basic scatter plot with default settings using wt as x and mpg as y. Each data points are represented as an open circle on the plot. As you could see, heavier vehicles are less fuel efficient. We can add a regression line on this scatter plot using a lower-level graphics function abline: abline(lm(mpg ~ wt)) # add regression line Note that lm(mpg ~ wt) generates a linear regression model of mpg as a function of wt, which is then passed on to abline. We add other information about these cars to customize this plot. plot(wt, mpg, pch = am) # am = 1 for automatic transmission “pch” is a parameter that specifies the types of data points on the plot. See Figure 2.2 for a whole list of possible values. The “am” column in mtcars dataset indicates whether the car is automatic transmission (am = 1) or not (am = 0). Figure 2.2: Data point types in base R. am ## [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 So R uses circles or squares according to this sequence for each of the data points. It draws a circle when am value is 1, and square when it is zero. See all the types in Figure 2.2. We add a legend to the top-right corner using a low-level graphics function legend: legend(&quot;topright&quot;, c(&quot;Automatic&quot;, &quot;Manual&quot;), pch = 0:1) This plot shows that heavier cars often use manual transmissions. Always slow down and interpret your plots in plain language. We can be happy about this plot, but we continue to fuss in more information on this graph. Using the same line of thinking, we can change the color of the data points according to other information, i.e. the number of cylinders. plot(wt, mpg, pch = am, col = rainbow(3)[as.factor(mtcars$cyl)]) #Generate 3 colors for the 3 types of cyl. The rainbow(3) generates a vector of 3 colors. The 3 levels of the cylinders will be assigned to three colors respectively. The red, green and blue represent the cylinder 4, 6, and 8 respectively. Now we alter the size of the data points to represent additional information such as horsepower, hp. Since hp is often a big number, we divide it by 50, a ratio determined by trial-and-error. These sometimes are called bubble plots or balloon plots. plot(wt, mpg, pch = am, col = rainbow(3)[as.factor(mtcars$cyl)], cex = hp / 50) legend(4.5, 34, levels(as.factor(mtcars$cyl)), title = &quot;Cylinders&quot;, pch = 15, col = rainbow(3)) Note that we added this legend at the (5, 30) position on the plot. To see all the options: ? plot This website lists all the parameters for R graphics: http://www.statmethods.net/advgraphs/parameters.html. Now we want to finish up this plot by adding axis labels, and title. We also changed the x-axis range to 1-6 using the xlim parameter to specify the limits. We finally put everything together. Figure 2.3: Enhanced scatter plot of mtcars data. Also called bubble plot or balloon plot. Note that this seemingly complicated chunk of code is built upon many smaller steps, and it involves trial-and-error. Exercise 2.1 Create a scatter plot similar to Figure 2.3 using the mtcars dataset to highlight the correlation between hp and disp (displacement). You should use colors to represent carb ( # of carburetors), types of data points to denote the number of gears, and size of the data points proportional to qsec, the number of seconds the cars need run the first ¼ mile. Add regression line and legends. Note that you should include comments and interpretations. Submit your code and plot in a PDF file. Hint: Go through the above example first. Then start small and add on step by step. Figure 2.3 is perhaps a bit too busy. Let’s develop an elegant version. plot(wt, mpg, pch = 16, cex = hp / 50, col = rainbow(3)[as.factor(mtcars$cyl)]) Notes: x; y; solid circle; horsepowersize of bubble; color cylinder 4, 6, or 8 Then we use a lower-level graphics function points to draw circles around each data point. Figure 2.4: Scatter plot showing the weight and MPG, colored by the number of cylinders. A line at mpg = 20 separates the 4-cylinder cars from 8 cylinder cars. This line adds a LOWESS smooth line determined by locally-weighted polynomial regression. Exercise 2.2 Generate the bubble plot Figure 2.4 based on the code:“plot(wt, mpg, pch = 16, cex = hp / 50, col = rainbow(3)[as.factor(mtcars$cyl)])”. Submit your complete code and the generated plot. Give a brief interpretation about your plot. 2.2.1.2 3D scatter plot Even though I’ve never been a fan of 3D plots, it is possible in R using additional packages such as scatterplot3d. Install this package and then try the following. library(scatterplot3d) scatterplot3d(wt, disp, mpg, color = rainbow(3)[as.factor(mtcars$cyl)], type = &quot;h&quot;, pch = 20) Figure 2.5: 3D scatter plots are rarely useful. 3D plots are hard to interpret. So try to avoid them. However, it is fun when you can interact with them. Using example code at this website, you can create interactive 3D plots: http://www.statmethods.net/graphs/scatterplot.html 1 I like to work directly from my Google Drive, which automatically backs up my files in the cloud and syncs across several of my computers. This is an insurance against disasters like my dog peeing on my computer and ruins my grant proposal just before the deadline. 2 Note that I was trying to avoid having spaces in column names. Instead of “Blood Pressure”, I used “BloodPressure”. This makes the columns easier to reference to. 2.2.2 Barplot with error bars If we are interested in the difference between the cars with different numbers of cylinders. The 32 models are divided into 3 groups as cyl takes the values of 4, 6, or 8. We can use the aggregate function to generate some statistics by group. stats &lt;- aggregate(. ~ cyl, data = mtcars, mean) stats ## cyl mpg disp hp drat wt qsec vs ## 1 4 26.66364 105.1364 82.63636 4.070909 2.285727 19.13727 0.9090909 ## 2 6 19.74286 183.3143 122.28571 3.585714 3.117143 17.97714 0.5714286 ## 3 8 15.10000 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000 ## am gear carb ## 1 0.7272727 4.090909 1.545455 ## 2 0.4285714 3.857143 3.428571 ## 3 0.1428571 3.285714 3.500000 This tells R to divide the cars into different groups by cyl and compute the average of all other columns for each group. The results above indicate many differences. As the number of cylinders increase, fuel efficiency measured by mpg decreases, while displacement and horsepower increases. We can obviously create a basic bar chart to show the difference in mpg. barplot(stats[, 2]) # basic bar plot The labels are missing from this basic plot. It is certainly possible to add the labels, but it is more convenient to use the tapply function, which generates a vector with names. We use tapply to calculate the mean, standard deviation (sd) and the numbers of samples for each group. Means &lt;- tapply(mpg, list(cyl), mean) # Average mpg per group Means ## 4 6 8 ## 26.66364 19.74286 15.10000 Note that it generates a vector and “4”, “6”, and “8” are names of each of the elements in this vector. tapply applies a function to a vector (mpg) according to grouping information defined by a factor (cyl) of the same length. Here it first groups the mpg numbers into 3 groups (cly= 4, 6, 8), and then within each group, the mean is calculated and returned. tapply is a member of a family of functions which includes apply, sapply, and lapply; all are powerful and efficient in computing than loops. Similarly, we can compute the standard deviation for each group. SDs &lt;- tapply(mpg, list(cyl), sd) # SD per group for mpg SDs ## 4 6 8 ## 4.509828 1.453567 2.560048 Now we can have a basic barplot with group names: barplot(Means) Our goal is to generate a graph like Figure 2.6 with both error bars and text annotation on the number of samples per group. We use two low-level graphics functions to add these elements to the plot, namely, text, and arrow. The text function adds any text to a plot to a position specified by x, y coordinates. Let’s try it. text(0.5, 5, &quot;n=11&quot;) # adding text to plot The (0.5, and 5) are the x and y location of the text information. Try to change it to something else within the plotting range, meaning x within 0 to 3 and y between 0 and 25. You can place any text anywhere. We added sample size information for the first group. We can choose to do this for each of bar manually, but obvious there should be a better way to do this. The trick is to find the precise location of all the bars and place the text there, hopefully doing this once for all of them. To achieve this, we use the values returned by the barplot object. xloc &lt;- barplot(Means) # get locations of the bars xloc # the center of each of bars on x ## [,1] ## [1,] 0.7 ## [2,] 1.9 ## [3,] 3.1 Yes, plotting functions not only generate graphs, they can also returns values. These values sometimes are useful in computing or refining the graph. Try this: h &lt;- hist( rnorm(100) ) and then type h to see the values returned by hist function. In our barplot case, we got an object containing the location of the center of the bars on x-axis. So the first bar is located on x=0.7. Since xloc has the location on all bars, we can add the information all at once: Nsamples &lt;- tapply(mpg, list(cyl), length) #number of samples per group Nsamples ## 4 6 8 ## 11 7 14 text(xloc, 2, Nsamples) # add sample size to each group The xloc specifies the center of the bars on the x-axis, namely 07, 1.9, and 3.1. The y coordinates are all 2. Try change the y location from 2 to 10, and see what happens. Looking great! The sample sizes are labeled on all the bars! This method works even if you have 20 bars! Now we want to make it explicit that these numbers represent sample size. For the first bar, we want “n=11”, instead of just “11”. First, we will append “n=” to each number and generate a string vector using the paste function paste(&quot;n=&quot;, Nsamples) # creating the strings to be added to the plot ## [1] &quot;n= 11&quot; &quot;n= 7&quot; &quot;n= 14&quot; text(xloc, 2, paste(&quot;n=&quot;, Nsamples)) # add sample size to each group Following a similar strategy, now we want to add error bars to represent standard deviations (SD) within each group. The plot is more informative as we visualize both the mean and variation within groups. For each bar, we need to draw an error bar from mean – SD to mean + SD. Let’s play with the arrows function, which draws arrows on plots. arrows(1, 15, # x,y of the starting point 1, 25) # x,y of the ending point arrows(2, 15, 2, 25, code = 3) # arrows on both ends arrows(3, 10, 3, 20, code = 3, angle = 90) # bend 90 degrees, flat Now it’s beginning to look a lot like Christmas (error bar)! I learned this clever hack of arrows as error bars from (Beckerman 2017)1. We are ready to add the error bars using the data stored in xloc, Means and SDs. barplot(Means) # re-create bar plot arrows(xloc, Means - SDs, # define 3 beginning points xloc, Means + SDs, # define 3 ending points code = 3, angle = 90, length = 0.1) Yes, we have a bar plot with error bars! We need to add a few refinements now such as colors, labels, and a title. As the first error bar is truncated, we need to adjust the range for y, by changing the ylim parameter. Putting everything together, we get this code chuck. attach(mtcars) # attach data, two columns: numbers and groups ## The following objects are masked from mtcars (pos = 4): ## ## am, carb, cyl, disp, drat, gear, hp, mpg, qsec, vs, wt ## The following object is masked from package:ggplot2: ## ## mpg Means &lt;- tapply(mpg, list(cyl), mean) #compute means by group defined by cyl SDs &lt;- tapply(mpg, list(cyl), sd) # calculate standard deviation by group Nsamples &lt;- tapply(mpg, list(cyl), length) # number of samples per group xloc &lt;- barplot(Means, # bar plot, returning the location of the bars xlab = &quot;Number of Cylinders&quot;, ylab = &quot;Average MPG&quot;, ylim = c(0,35), col = &quot;green&quot;) arrows (xloc, Means - SDs, # add error bars as arrows xloc, Means + SDs, code = 3, angle = 90, length = 0.1) text(xloc, 2, paste(&quot;n=&quot;, Nsamples)) # add sample size to each group Figure 2.6: Bar chart with error bar representing standard deviation. Sometimes we want error bars to represent standard error instead which is given by σ/√n, where σ is standard deviation and n is sample size. In R we can do complex statistical tests or regression analyses with just one line of code, for example, aov, glm. However, we went through all this trouble to generate just a bar chart! What is the point? We could click around in sigmaPlot, or GraphPad and get a barplot in less time. Well, once you figured how to do one, you can easily do this for 10 graphs. More importantly, this code clearly recorded the plotting process, which is essential for reproducible research. Exercise 2.3 Revise the above relative code to generate a bar chart showing the average weight of cars with either automatic or manual transmission. Include error bars, the number of samples and axis labels. Exercise 2.4 Create a bar chart with error bars to show the average illiteracy rate by region, using the illiteracy rate in the state.x77 data and regions defined by state.region in the state data set. Hints: 1, Check the type of data set using class(state.x77); 2, Convert the matrix dataset to data frame using df.state.x77 &lt;- as.data.frame(state.x77); 3, Attach df.state.x77. 2.2.3 Visualizing correlation between categorical variables If we are interested in the correlation between two categorical variables, we can tabulate the frequencies from a data frame: counts &lt;- table(cyl, am) This contingency table gives us the number of cars in each combination. Among the 8-cylinder cars, there are 12 models with manual transmission, and only 2 models have automatic transmission. We obviously can feed this into a fisher’s exact test to test for independence. We could easily visualize this information with a bar chart. barplot(counts) We generated a stacked barplot. Let’s refine it. We have a plot like the left of Figure 2.7. We can also put the bars side-by-side, by setting the beside option to TRUE. barplot(counts, col = rainbow(3), xlab =&quot;Transmission&quot;, ylab = &quot;Number of cars&quot;) legend(&quot;topright&quot;,rownames(counts), pch = 15, title = &quot;Cylinders&quot;, col = rainbow(3)) barplot(counts, col = rainbow(3), xlab = &quot;Transmission&quot;, ylab = &quot;Number of cars&quot;, beside = TRUE) legend(&quot;topright&quot;,rownames(counts), pch = 15, title = &quot;Cylinders&quot;, col = rainbow(3)) Figure 2.7: Bar plots showing the proportion of cars by cylinder and transmission. See the right side of Figure 2.7. Given a large dataset, we can easily tabulate categorical variables and plot these to show the relative frequencies. Another easy plot is the mosaic plot: mosaicplot(counts, col = c(&quot;red&quot;, &quot;green&quot;)) Figure 2.8: Mosaic plot. Vertically, we divide the square into 3 parts, the area of each is proportional to the number of cars with different cylinders. There are more 8-cylinder vehicles than those with 4 or 6. Horizontally, we divide the square according to the am variable, which represents automatic transmission (am =0) or manual transmission. Clearly, these two are not independent. As the number of cylinder increases, more cars are using manual transmission. While the height of the bars in the bar chart in Figure 5 represents absolute totals per category, in mosaic plots the height are equal. Thus we see proportions within each category. Exercise 2.5 Use bar plot and mosaic plot to investigate the correlation between cyl and gear. Interpret your results. 2.2.4 Detecting correlations among variables In ther beginning of this chapter we used scatter plots to study the correlation between two variables, mpg and wt in the mtcars dataset. There are many such pairwise correlations. One simple yet useful plot of the entire dataset is scatter plot matrix (SPM). SPMs can be created by the pairs function, or just run plot on a data frame. plot(mtcars) # scatter plot matrix; same as pairs(mtcars) Figure 2.9: Scatter plot matrix of the mtcars dataset. We can spend all day studying this large plot, as it contains information on all pairs of variables. For example, mpg is negatively correlated with disp, hp, and wt, and positively correlated with drat. There are many variations of scatter plot matrix, for instance, the spm function in the car package. Also try this cool plot using ellipses: http://www.r-graph-gallery.com/97-correlation-ellipses/ library(ellipse) # install.packages(&quot;ellipsis&quot;) library(RColorBrewer) # install.packages(&quot;RcolorBrewer&quot;) data &lt;- cor(mtcars) # correlation matrix my_colors &lt;- brewer.pal(5, &quot;Spectral&quot;) # Color Pannel my_colors &lt;- colorRampPalette(my_colors)(100) ord &lt;- order(data[1, ]) # Order the correlation matrix data_ord &lt;- data[ord, ord] plotcorr(data_ord, col = my_colors[data_ord * 50 + 50], mar = c(1, 1, 1, 1)) Figure 2.10: Scatter plot matrix of the mtcars dataset. Exercise 2.6 Generate a scatter plot matrix of the state.x77 data in the state data set included in base R. It includes various statistics on the 50 U.S. states. Type ? state for more information and type state.x77 to see the data. Also, visualize the correlation using the ellipses shown above. Interpret your results. What types of correlation do you find interesting? Hint: The class(state.x77) should be “matrix” not “data frame”, otherewise convert it to a “matrix”. If you examine the above code carefully, the ellipses are drawn just based on a matrix of Pearson’s correlation coefficients. We can easily quantify the relationship between all variables by generating a matrix of Pearson’s correlation coefficient: cor(mtcars) # correlation coefficient of all columns corMatrix &lt;- cor(mtcars[, 1:11]) round(corMatrix, 2) # Round to 2 digits ## mpg cyl disp hp drat wt qsec vs am gear carb ## mpg 1.00 -0.85 -0.85 -0.78 0.68 -0.87 0.42 0.66 0.60 0.48 -0.55 ## cyl -0.85 1.00 0.90 0.83 -0.70 0.78 -0.59 -0.81 -0.52 -0.49 0.53 ## disp -0.85 0.90 1.00 0.79 -0.71 0.89 -0.43 -0.71 -0.59 -0.56 0.39 ## hp -0.78 0.83 0.79 1.00 -0.45 0.66 -0.71 -0.72 -0.24 -0.13 0.75 ## drat 0.68 -0.70 -0.71 -0.45 1.00 -0.71 0.09 0.44 0.71 0.70 -0.09 ## wt -0.87 0.78 0.89 0.66 -0.71 1.00 -0.17 -0.55 -0.69 -0.58 0.43 ## qsec 0.42 -0.59 -0.43 -0.71 0.09 -0.17 1.00 0.74 -0.23 -0.21 -0.66 ## vs 0.66 -0.81 -0.71 -0.72 0.44 -0.55 0.74 1.00 0.17 0.21 -0.57 ## am 0.60 -0.52 -0.59 -0.24 0.71 -0.69 -0.23 0.17 1.00 0.79 0.06 ## gear 0.48 -0.49 -0.56 -0.13 0.70 -0.58 -0.21 0.21 0.79 1.00 0.27 ## carb -0.55 0.53 0.39 0.75 -0.09 0.43 -0.66 -0.57 0.06 0.27 1.00 We used the round function to keep two digits after the decimal point. We can examine the coefficients in this matrix. Note that strong negative correlations are also interesting. For example, wt and mpg have a correlation of r= -0.87, meaning that heavier vehicles tend to have smaller mpg. We can visualize this matrix in other ways besides the ellipses. The most logical thing to do with a matrix of correlation coefficients is to generate a tree using hierarchical clustering. plot(hclust(as.dist(1 - corMatrix))) # hierarchical clustering Figure 2.11: Hierarchical clustering tree. Here we first subtracted the r values from 1 to define a distance measure. So perfectly correlated variables with r = 1 have a distance of 0, while negatively correlated variables with r = -1 have a distance of 2. We did this operation on the entire matrix at once. You can try to run the 1- corMatrix from the command line to see the result. The result is then formatted as a distance matrix using as.dist, which is passed to the hclust function to create a hierarchical clustering tree. See more info on hclust by using ? hclust As we can see from the tree, cyl is most highly correlated with disp and then hp and wt. Broadly, the variables form two groups, with high correlation within each cluster. This is an important insight into the overall correlation structure of this data set. Exercise 2.7 Generate a hierarchical clustering tree for the 32 car models in the mtcars dataset and discuss your results. Hint: You can transpose the dataset using function t(), so that rows becomes columns and columns become rows. Then you should be able to produce a similar tree for the cars. Another straight forward method is to translate the numbers in the correlation matrix into colors using the image functions. image(corMatrix) # translate a matrix into an image Here we are using red and yellow colors to represent the positive and negative numbers, respectively. Since the row and column names are missing, we can use the heatmap function. heatmap(corMatrix, scale = &quot;none&quot;) # Generate heatmap Here a lot more things are going on. The orders are re-arranged and also a tree is drawn to summarize the similarity. We explain heatmap in details later. A more elegant way of show correlation matrix using ggplot2 is available here: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization. So as you can see, coding is not hard when you can steal ideas from others, thanks to Dr. Google and the thousands of contributors, who contribute code examples and answer questions online. R has a fantastic user community. 2.2.5 Hierarchical clustering In the mtcars dataset, we have 32 car models, each characterized by 11 parameters (dimensions, variables). We want to compare or group these cars using information about all of these parameters. We know that Honda Civic is similar to Toyota Corolla but different from a Cadillac. Quantitatively, we need to find a formula to measure the similarity. Given two models, we have 22 numbers. We need to boil them down to one number to measure relative similarity. This is often done by a distance function. The most popular one is Euclidean distance, (it is also the most abused metric):\\(Eucliden Distance D=√((mpg_1-mpg_2)^2+(hp_1-hp_2)^2+(wt_1-wt_2)^2+⋯)\\).If two cars have similar characteristics, they have similar numbers on all of these dimensions; their distance as calculated above is small. Therefore, this is a reasonable formula. Note that we democratically added the squared difference of all dimensions. We treated every dimension with equal weight. However, if we look at the raw data, we know that some characteristics, such as hp(horsepower), have much bigger numerical value than others. In other words, the difference in hp can overwhelm our magic formula and make other dimensions essentially meaningless. Since different columns of data are in very different scale, we need to do some normalization. We want to transform data so that they are comparable on scale. At the same time, we try to preserve as much information as we could. mt = as.matrix(mtcars) heatmap(mt) Figure 2.12: Heatmap with all default settings. This is not correct. Normalization is needed. Do not go out naked. In this basic heat map, data are scaled by row by default. Some numbers are massive (hp and disp), and they are all in bright yellow. This is not democratic or reasonable as these big numbers dominate the clustering. We clearly need to do some normalization or scaling for each column which contains different statistics. Through check the help information by: ? heatmap We can figure out that we need an additional parameter: heatmap(mt, scale = &quot;column&quot;) # Figure 2.13 Scaling is handled by the scale function, which subtracts the mean from each column and then divides by the standard division. Afterward, all the columns have the same mean of approximately 0 and standard deviation of 1. This is called standardization. ? scale We can also handle scaling ourselves. We can use the apply function to subtract the mean from each column and then divide by the standard division of each column. mt &lt;- apply(mt, 2, function(y)(y - mean(y)) / sd(y)) Note that we defined a function, and ‘applied’ the function to each of the columns of mt. For each column, we first calculate the mean and standard deviation. Then the mean is subtracted before being divided by standard deviation. The second parameter “2” refers to do something with the column. Use “1” for row. Sometimes, we have columns with very little real variation. Being divided by standard deviation will amplify noise. In such cases, we just subtract the mean. This is called centering. Centering is less aggressive in transforming our data than standardization. apply(mt, 2, mean) # compute column mean Here mean( ) is the function that applied to each column. The column means are close to zero. colMeans(mt) # same as above apply(mt, 2, sd) # compute sd for each column Here sd( ) is the function that applied to each column. heatmap(mt, scale = &quot;none&quot;) # Figure 2.13 Figure 2.13: Heatmap of mtcars dataset. Yellow- positive number / above average. This produced Figure 2.13. Another function with much better coloring is heatmap.2 in the gplot package. #install.packages(&quot;gplots&quot;) library(gplots) heatmap.2(mt) # plain version This basic heatmap is not very cool. So, we do some fine tuning. This function have a million parameters to tune: ? heatmap.2 heatmap.2(mt, col = greenred(75), density.info = &quot;none&quot;, trace = &quot;none&quot;, scale = &quot;none&quot;, margins = c(5, 10)) Figure 2.14: Fine-tuned heatmap using heatmap.2 in gplots package. Note that the last argument gives a large right margin, so that the long names can show up un-truncated. By default, the heatmap function scales the rows in the data matrix so that it has zero mean. In our case, we already did our scaling, so we use scale=“none” as a parameter. Also, the dendrogram on the left and top are generated using the hclust function. The distance function is Euclidean distance. All of these can be changed. Figure 2.15: Single, complete and average linkage methods for hierarchical clustering. We used the hclust function before. Let’s dive into the details a little bit. First, each of the objects (columns or rows in mtcars data), is treated as a cluster. The algorithm joins the two most similar clusters based on a distance function. This is performed iteratively until there is just a single cluster containing all objects. At each iteration, the distances between clusters are recalculated according to one of the methods—Single linkage, complete linkage, average linkage, and so on. In the single-linkage method, the distance between two clusters is defined by the smallest distance among the object pairs. This approach puts ‘friends of friends’ into a cluster. On the contrary, complete linkage method defines the distance as the largest distance between object pairs. It finds similar clusters. Between these two extremes, there are many options in between. The linkage method I found the most robust is the average linkage method, which uses the average of all distances. However, the default seems to be complete linkage. Thus we need to change that in our final version of the heat map. library(gplots) hclust2 &lt;- function(x, ...) # average linkage method hclust(x, method=&quot;average&quot;, ...) dist2 &lt;- function(x, ...) #distance method as.dist(1-cor(t(x), method=&quot;pearson&quot;)) # Transform data mt &lt;- apply(mt, 2, function(y)(y - mean(y)) / sd(y)) heatmap.2(mt, distfun = dist2, # use 1-Pearson as distance hclustfun = hclust2, # use average linkage col = greenred(75), #color green red density.info = &quot;none&quot;, trace = &quot;none&quot;, scale = &quot;none&quot;, RowSideColors = rainbow(3)[as.factor(mtcars$cyl)], margins = c(5, 10) # bottom and right margins ) legend(&quot;topright&quot;,levels(as.factor(cyl)), fill=rainbow(3)) # add legend Figure 2.16: Final version of heatmap for mtcars data. Here we defined and used our custom distance function dist2 and used average linkage method for hclust. We also added a color bar to code for the number of cylinders. We can also add color bars for the columns, as long as we have some information for each of the column. Hierarchical clustering coupled with a heatmap is a very effective method to visualize and explore multidimensional data. It not only visualizes all data points but also highlights the correlation structure in both rows and columns. It is my favorite plot, and you can find such plots in many of my scientific publications! Let’s discuss how to interpret Figure 2.16. First, the colors red and green represent positive and negative numbers, respectively. Bright red represent large positive numbers and bright green means negative numbers with large absolute values. Since we standardized our data, red indicates above average and green below average. The 8 cylinder cars which form a cluster in the bottom have bigger than average horsepower (hp), weight (wt). These cars have smaller than average fuel efficiency (mpg), acceleration(qsec), the number of gears. These cars share similar characteristics and form a tight cluster. The four-cylinder cars, on the other hand, have the opposite. The distance in the above dendrogram between two objects is proportional to some measure of dissimilarity (such as Euclidean distance) between them defined by the original data. This is true for both trees, the one on the top and the one on the left. There are many ways to quantify the similarity between objects. The first step in hierarchical clustering is to define distance or dissimilarity between objects that are characterized by vectors. We have discussed that we can use Pearson’s correlation coefficient (PCC) to measure the correlation between two numerical vectors. We could thus easily generate a measure of dis-similarity/distance by a formula like: \\(Distance(x,y) = 1-PCC(x,y)\\). This score have a maximum of 2 and minimum of 0. Similar distance measure could be defined based on any non-parametric versions of correlation coefficients. In addition to these, there are many ways to quantify dis-similarity: (See: http://www.statsoft.com/textbook/cluster-analysis/) Euclidean distance. This is probably the most commonly chosen type of distance. It simply is the geometric distance in the multidimensional space. It is computed as: \\(Distance(x,y) = √(∑_{i=1}^{m}(x_i-y_i )^2 )\\), where m is the dimension of the vectors. Note that Euclidean (and squared Euclidean) distances are usually computed from raw data, and not from standardized data. This method has certain advantages (e.g., the distance between any two objects is not affected by the addition of new objects to the analysis, which may be outliers). However, the distances can be greatly affected by differences in scale among the dimensions from which the distances are computed. For example, if one of the dimensions denotes a measured length in centimeters, and you then convert it to millimeters, the resulting Euclidean or squared Euclidean distances can be greatly affected, and consequently, the results of cluster analyses may be very different. It is good practice to transform the data, so they have similar scales. Squared Euclidean distance. You may want to square the standard Euclidean distance to place progressively greater weight on objects that are further apart. City-block (Manhattan) distance. This distance is simply the average difference across dimensions. In most cases, this distance measure yields results similar to the simple Euclidean distance. However, note that in this measure, the effect of single large differences (outliers) is dampened (since they are not squared). The city-block distance is computed as: \\(Distance(x,y) = \\frac{1}{m}∑_{i=1}^{m}|x_i-y_i|\\) Chebyshev distance. This distance measure may be appropriate in cases when we want to define two objects as “different” if they are different on any one of the dimensions. The Cheever distance is calculated by: \\(Distance(x,y) = Maximum|x_i-y_i|\\) Percent disagreement. This measure is particularly useful if the data for the dimensions included in the analysis are categorical in nature. This distance is computed as: \\(Distance(x,y) = (Number of xi ≠ yi)/ m\\) Exercise 2.8 Generate a heatmap for the statistics of 50 states in the state.x77 dataset (for information ? state) using heatmap.2 in the gplots package. Normalize your data properly before creating heatmap. Use the default Euclidean distance and complete linkage. Use state.region to color code the states and include an appropriate legend. Interpret your results. Discuss both trees. Exercise 2.9 Change distance function to 1-Pearson’s correlation coefficient. Change linkage method to average linkage. Turn off the clustering of the columns by reading the help information on heatmap.2. Observe what is different in the clustering trees. Exercise 2.10 Generate a heat map for the iris flower dataset. For data normalization, do not use standardization, just use centering (subtract the means). Use the species information in a color bar and interpret your results. 2.2.6 Representing data using faces. Serious scientific research only! Humans are sensitive to facial images. We can use this to visualize data. #install.packages(&quot;TeachingDemos&quot;) library(TeachingDemos) faces(mtcars) Figure 2.17: Using faces to represent data. This is called Chernoff’s faces. Each column of data is used to define a facial feature. The features parameters of this implementation are: 1-height of face (“mpg”), 2-width of face (“cyl”) 3-shape of face (“disp”), 4-height of mouth (“hp”), 5-width of mouth (“drat”), 6-curve of smile (“wt”), 7-height of eyes (“qsec”), 8-width of eyes(“vs”), 9-height of hair(“am”), 10-width of hair (“gear”), 11-styling of hair (“carb”). It turns out that the longer, serious faces represent smaller cars that are environmentally-friendly, while big polluters are shown as cute, baby faces. What an irony! 2.3 Visualizing iris data set 2.3.1 A Matrix only contains numbers While data frames can have a mix of numbers and characters in different columns, a matrix is often only contain numbers. Let’s extract first 4 columns from the data frame iris and convert to a matrix: attach(iris) ## The following objects are masked from iris (pos = 11): ## ## Petal.Length, Petal.Width, Sepal.Length, Sepal.Width, Species ## The following objects are masked from iris (pos = 12): ## ## Petal.Length, Petal.Width, Sepal.Length, Sepal.Width, Species ## The following objects are masked from iris (pos = 13): ## ## Petal.Length, Petal.Width, Sepal.Length, Sepal.Width, Species x &lt;- as.matrix(iris[, 1:4]) # convert to matrix colMeans(x) # column means for matrix ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.843333 3.057333 3.758000 1.199333 colSums(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 876.5 458.6 563.7 179.9 The same thing can be done with rows via rowMeans(x) and rowSums(x). Here is some matrix algebra. y &lt;- iris[1:10, 1:4] # extract the first 10 rows of iris data in columns 1 to 4. y t(y) # transpose z &lt;- y + 5 # add a number to all numbers in a matrix z &lt;- y * 1.5 # multiply a factor z + y # adding corresponding elements y * z # multiplying corresponding elements y &lt;- as.matrix(y) # convert the data.frame y to a matrix z &lt;- as.matrix(z) # convert the data.frame z to a matrix y %*% t(z) # Matrix multiplication 2.3.2 Scatter plot matrix We can generate a matrix of scatter plots simply by: pairs(iris[, 1:4]) pairs(iris[, 1:4], col = rainbow(3)[as.factor(iris$Species)]) # Figure 2.18 Figure 2.18: Scatter plot matrix. Exercise 2.11 Look at this large plot for a moment. What do you see? Provide interpretation of these scatter plots. 2.3.3 Heatmap Heatmaps with hierarchical clustering are my favorite way to visualize data matrices. The rows and columns are kept in place, and the values are coded by colors. Heatmaps can directly visualize millions of numbers in one plot. The hierarchical trees also show the similarity among rows and columns: closely connected rows or columns are similar. library(gplots) hclust2 &lt;- function(x, ...) hclust(x, method=&quot;average&quot;, ...) x &lt;- as.matrix( iris[, 1:4]) x &lt;- apply(x, 2, function(y) (y - mean(y))) heatmap.2(x, hclustfun = hclust2, # use average linkage col = greenred(75), #color green red density.info = &quot;none&quot;, trace = &quot;none&quot;, scale = &quot;none&quot;, labRow = FALSE, # no row names RowSideColors = rainbow(3)[as.factor(iris$Species)], srtCol = 45, # column labels at 45 degree margins = c(10, 10)) # bottom and right margins legend(&quot;topright&quot;, levels(iris$Species), fill = rainbow(3)) Figure 2.19: Heatmap for iris flower dataset. 2.3.4 Star plot Star plot uses stars to visualize multidimensional data. Radar chart is a useful way to display multivariate observations with an arbitrary number of variables. Each observation is represented as a star-shaped figure with one ray for each variable. For a given observation, the length of each ray is made proportional to the size of that variable. The star plot is first used by Georg von Mayr in 1877! x = iris [, 1:4] stars(x) # do I see any diamonds in Figure 2.20A? I want the bigger one! stars(x, key.loc = c(17,0)) # What does this tell you? Exercise 2.12 Based on heatmap and the star plot, what is your overall impression regarding the differences among these 3 species of flowers? 2.3.5 Segment diagrams The stars() function can also be used to generate segment diagrams, where each variable is used to generate colorful segments. The sizes of the segments are proportional to the measurements. stars(x, key.loc = c(20,0.5), draw.segments = T ) Figure 2.20: Star plots and segments diagrams. Exercise 2.13 Produce the segments diagram of the state data (state.x77) and offer some interpretation regarding South Dakota compared with other states. Hints: Convert the matrix to data frame using df.state.x77 &lt;- as.data.frame(state.x77),then attach df.state.x77. 2.3.6 Parallel coordinate plot Parallel coordinate plot is a straightforward way of visualizing multivariate data using lines. x = iris[, 1:4] matplot(t(x), type = &#39;l&#39;, #“l” is lower case L for “line”. col = rainbow(3)[iris$Species]) # Species information is color coded legend(&quot;topright&quot;, levels(iris$Species), fill = rainbow(3)) # add legend to figure. text(c(1.2, 2, 3, 3.8), 0, colnames(x)) # manually add names Figure 2.21: Parallel coordinate plots directly visualize high-dimensional data by drawing lines. The result is shown in Figure 2.21. Note that each line represents a flower. The four measurements are used to define the line. We can clearly see that I. setosa have smaller petals. In addition to this, the “lattice” package has something nicer called “parallelplot”. That function can handle columns with different scales. 2.3.7 Box plot boxplot(x) # plain version. Column names may not shown properly par(mar = c(8, 2, 2, 2)) # set figure margins (bottom, left, top, right) boxplot(x, las = 2) # Figure 2.22 Figure 2.22: Box plot of all 4 columns Notice that las = 2 option puts the data labels vertically. The par function sets the bottom, left, top and right margins respectively of the plot region in number of lines of text. Here we set the bottom margins to 8 lines so that the labels can show completely. 2.3.8 Bar plot with error bar Figure 2.23: Bar plot of average petal lengths for 3 species Exercise 2.14 Write R code to generate Figure 2.23, which show the means of petal length for each of the species with error bars corresponding to standard deviations. Bar plot of average petal lengths for 3 species Bar plot of average petal lengths for 3 species. 2.3.9 Combining plots It is possible to combine multiple plots at the same graphics window. op &lt;- par(no.readonly = TRUE) # get old parameters par(mfrow= c(2, 2)) # nrows = 2; ncols= 2 attach(iris) hist(Sepal.Length) hist(Sepal.Width) hist(Petal.Length) hist(Petal.Width) par(op) # restore old parameters; otherwise affect all subsequent plots Figure 2.24: Combine multiple histograms. The result is shown in Figure 2.24. This plot gives a good overview of the distribution of multiple variables. We can see that the overall distributions of petal length and petal width are quite unusual. Exercise 2.15 Create a combined plot for Q-Q plot of the 4 numeric variables in the iris flower data set. Arrange your plots in 1 row and 4 columns. Include straight lines and interpretations. 2.3.10 Plot using principal component analysis (PCA) PCA is a linear projection method. As illustrated in Figure 2.25, it tries to define a new set of orthogonal coordinates to represent the dataset such that the new coordinates can be ranked by the amount of variation or information it captures in the dataset. After running PCA, you get many pieces of information: • How the new coordinates are defined, • The percentage of variances captured by each of the new coordinates, • A representation of all the data points onto the new coordinates. Figure 2.25: Concept of PCA. Here the first component x’ gives a relatively accurate representation of the data. Here’s an example of running PCA in R. Note that “scale=T” in the following command means that the data is normalized before conduction PCA so that each variable has unite variance. ? prcomp pca = prcomp(iris[, 1:4], scale = T) pca # Have a look at the results. ## Standard deviations (1, .., p=4): ## [1] 1.7083611 0.9560494 0.3830886 0.1439265 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.5210659 -0.37741762 0.7195664 0.2612863 ## Sepal.Width -0.2693474 -0.92329566 -0.2443818 -0.1235096 ## Petal.Length 0.5804131 -0.02449161 -0.1421264 -0.8014492 ## Petal.Width 0.5648565 -0.06694199 -0.6342727 0.5235971 Note that the first principal component is positively correlated with Sepal length, petal length, and petal width. Recall that these three variables are highly correlated. Sepal width is the variable that is almost the same across three species with small standard deviation. PC2 is mostly determined by sepal width, less so by sepal length. plot(pca) # plot the amount of variance each principal components captures. str(pca) # this shows the structure of the object, listing all parts. ## List of 5 ## $ sdev : num [1:4] 1.708 0.956 0.383 0.144 ## $ rotation: num [1:4, 1:4] 0.521 -0.269 0.58 0.565 -0.377 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## .. ..$ : chr [1:4] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## $ center : Named num [1:4] 5.84 3.06 3.76 1.2 ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## $ scale : Named num [1:4] 0.828 0.436 1.765 0.762 ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## $ x : num [1:150, 1:4] -2.26 -2.07 -2.36 -2.29 -2.38 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:4] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## - attr(*, &quot;class&quot;)= chr &quot;prcomp&quot; head(pca$x) # the new coordinate values for each of the 150 samples ## PC1 PC2 PC3 PC4 ## [1,] -2.257141 -0.4784238 0.12727962 0.024087508 ## [2,] -2.074013 0.6718827 0.23382552 0.102662845 ## [3,] -2.356335 0.3407664 -0.04405390 0.028282305 ## [4,] -2.291707 0.5953999 -0.09098530 -0.065735340 ## [5,] -2.381863 -0.6446757 -0.01568565 -0.035802870 ## [6,] -2.068701 -1.4842053 -0.02687825 0.006586116 These numbers can be used to plot the distribution of the 150 data points. plot(pca$x[, 1:2], pch = 1, col = rainbow(3)[iris$Species], xlab = &quot;1st principal component&quot;, ylab = &quot;2nd Principal Component&quot;) legend(&quot;topright&quot;, levels(iris$Species), fill = rainbow(3)) The result (left side of Figure 2.26) is a projection of the 4-dimensional iris flowering data on 2-dimensional space using the first two principal components. From this I observed that the first principal component alone can be used to distinguish the three species. We could use simple rules like this: If PC1 &lt; -1, then Iris setosa. If PC1 &gt; 1.5 then Iris virginica. If -1 &lt; PC1 &lt; 1, then Iris versicolor. Figure 2.26: PCA plot of the iris flower dataset using R base graphics (left) and ggplot2 (right). 2.3.11 Attempt at ggplot2 There are 3 big plotting systems in R: base graphics, lattice, and ggplot2. Now let’s try ggplot2. First, let’s construct a data frame as demanded by ggplot2. pcaData &lt;- as.data.frame(pca$x[, 1:2]) pcaData &lt;- cbind(pcaData, iris$Species) colnames(pcaData) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;Species&quot;) #install.packages(&quot;ggplot2&quot;) library(ggplot2) ggplot(pcaData, aes(PC1, PC2, color = Species, shape = Species)) + # define plot area geom_point(size = 2) # adding data points Now we have a basic plot. As you could see this plot is very different from those from R base graphics. We are adding elements one by one using the “+” sign at the end of the first line. We will add details to this plot. percentVar &lt;- round(100 * summary(pca)$importance[2, 1:2], 0) # compute % variances ggplot(pcaData, aes(PC1, PC2, color = Species, shape = Species)) + # starting ggplot2 geom_point(size = 2) + # add data points xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + # x label ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) + # y label ggtitle(&quot;Principal component analysis (PCA)&quot;) + # title theme(aspect.ratio = 1) # width and height ratio The result is shown in right side of Figure 2.26. You can experiment with each of the additional element by commenting out the corresponding line of code. You can also keep adding code to further customize it. The function autoplot() in package ggfortify can generate the similar plot as Figure 2.26. The differences are caused by algorithms used in different packages. library(ggfortify) autoplot(prcomp(pca$x[, 1:2]), data = iris, colour = &#39;Species&#39;, shape = &#39;Species&#39;) More details can be found in this webpage: https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html. Exercise 2.16 Create PCA plot of the state.x77 data set (convert matrix to data frame). Use the state.region information to color code the states. Interpret your results. Hint: do not forget normalization using the scale option. 2.3.12 Classification: Predicting the odds of binary outcomes It is easy to distinguish I. setosa from the other two species, just based on petal length alone. Here we focus on building a predictive model that can predict between I. versicolor and I. virginica. For this we use the logistic regression to model the odd ratio of being I. virginica as a function of all of the 4 measurements: \\[ln(odds)=ln(\\frac{p}{1-p}) =a×Sepal.Length + b×Sepal.Width + c×Petal.Length + d×Petal.Width+c+e.\\] iris2 &lt;- iris[51:150, ] # removes the first 50 samples, which represent I. setosa iris2 &lt;- droplevels(iris2) # removes setosa, an empty levels of species. model &lt;- glm(Species ~ . , family = binomial(link = &#39;logit&#39;), data = iris2) # Species ~ . species as a function of everything else in the dataset summary(model) ## ## Call: ## glm(formula = Species ~ ., family = binomial(link = &quot;logit&quot;), ## data = iris2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.01105 -0.00541 -0.00001 0.00677 1.78065 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -42.638 25.707 -1.659 0.0972 . ## Sepal.Length -2.465 2.394 -1.030 0.3032 ## Sepal.Width -6.681 4.480 -1.491 0.1359 ## Petal.Length 9.429 4.737 1.991 0.0465 * ## Petal.Width 18.286 9.743 1.877 0.0605 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.629 on 99 degrees of freedom ## Residual deviance: 11.899 on 95 degrees of freedom ## AIC: 21.899 ## ## Number of Fisher Scoring iterations: 10 Sepal length and width are not useful in distinguishing versicolor from virginica. The most significant (P=0.0465) factor is Petal.Length. One unit increase in petal length will increase the log-odd of being virginica by 9.429. Marginally significant effect is found for Petal.Width. If you do not fully understand the mathematics behind linear regression or logistic regression, do not worry about it too much. Me either. In this class, I just want to show you how to do these analysis in R and interpret the results. I do not understand how computers work. Yet I use it every day. Exercise 2.17 So far, we used a variety of techniques to investigate the iris flower dataset. Recall that in the very beginning, I asked you to eyeball the data and answer two questions: • What distinguishes these three species? • If we have a flower with sepals of 6.5cm long and 3.0cm wide, petals of 6.2cm long, and 2.2cm wide, which species does it most likely belong to? Review all the analysis we did, examine the raw data, and answer the above questions. Write a paragraph and provide evidence of your thinking. Do more analysis if needed. References: 1 Beckerman, A. (2017). Getting started with r second edition. New York, NY, Oxford University Press. "],
["data-structures.html", "Chapter 3 Data structures 3.1 Basic concepts 3.2 Data structures 3.3 For more in-depth exercises", " Chapter 3 Data structures R data types and basic expressions1 : Common data structures in R include scalars, vectors, matrices, factors, data frames, and lists. These data structures can contain one or more individual data elements of several types, namely numeric (2.5), character (“Go Jacks”), or logical (TRUE or FALSE). 3.1 Basic concepts 3.1.1 Expressions Type anything at the prompt, and R will evaluate it and print the answer. 1 + 1 ## [1] 2 There’s your result, 2. It’s printed on the console right after your entry. Type the string “Go Jacks”. (Don’t forget the quotes!) &quot;Go Jacks&quot; ## [1] &quot;Go Jacks&quot; Exercise 3.1 Now try multiplying 45.6 by 78.9. 3.1.2 Logical Values Some expressions return a “logical value”: TRUE or FALSE. (Many programming languages refer to these as “boolean” values.) Let’s try typing an expression that gives us a logical value: 3 &lt; 4 ## [1] TRUE And another logical value (note that you need a double-equals sign to check whether two values are equal - a single-equals sign won’t work): 2 + 2 == 5 ## [1] FALSE T and F are shorthand for TRUE and FALSE. Try this: T == TRUE ## [1] TRUE 3.1.3 Variables As in other programming languages, you can store a value into a variable to access it later. Type x = 42 to store a value in x. x is a scalar, with only one data element. x = 42 You can also use the following. This is a conventional, safer way to assign values. x &lt;- 42 x can now be used in expressions in place of the original result. Try dividing x by 2 (/ is the division operator), and other calculations. x / 2 ## [1] 21 log(x) ## [1] 3.73767 x^2 ## [1] 1764 sqrt(x) ## [1] 6.480741 x &gt; 1 ## [1] TRUE You can re-assign any value to a variable at any time. Try assigning “Go Jacks!” to x. x &lt;- &quot;Go Jacks!&quot; You can print the value of a variable at any time just by typing its name in the console. Try printing the current value of x. x ## [1] &quot;Go Jacks!&quot; Now try assigning the TRUE logical value to x. x &lt;- TRUE You can store multiple values in a variable or object. That is called a vector, which is explained below. An object can also contain a table with rows and columns, like an Excel spreadsheet, as a matrix, or data frame. 3.1.4 Functions You call a function by typing its name, followed by one or more arguments to that function in parenthesis. Most of your R commands are functional calls. Let’s try using the sum function, to add up a few numbers. Enter: sum(1, 3, 5) ## [1] 9 Some arguments have names. For example, to repeat a value 3 times, you would call the rep function and provide its times argument: rep(&quot;Yo ho!&quot;, times = 3) ## [1] &quot;Yo ho!&quot; &quot;Yo ho!&quot; &quot;Yo ho!&quot; Exercise 3.2 Suppose a vector is definded as x &lt;- c(12, 56, 31, -5, 7). Calculate the mean of all elements in x, assign the mean to y. Squared each element in x and assign the result in a new vector z. Exercise 3.3 Try to find and run the two functions that sets and returns the current working directory. Exercise 3.4 Try to find and run the function that lists all the files in the current working folder. Many times, we want to re-use a chunk of code. The most efficient way is to wrap these code as a function, clearly define what the input and the output. Functions are fundamental building blocks of R. Most of the times when we run R commands, we are calling and executing functions. We can easily define our very own functions. For example, we have the following arithmetic function: \\[f(x)=1.5 x^3+ x^2-2x+1\\] Obviously, we can use the following code to do the computing: x &lt;- 5 1.57345 * x ^ 3 + x ^ 2 - 2 * x + 1 ## [1] 212.6813 This will work, but every time we have to re-write this code. So let’s try to define our own function: myf &lt;- function(x) { y = 1.57345 * x ^ 3 + x ^ 2 - 2 * x + 1 return(y) } Note that “{” and “}” signify the beginning and end of a block of code. “function” tells R that a function is going to be defined. At the end, the “return” statement returns the desired value. You can copy and paste the 4 lines of code to R and it defines a function called myf, which you can call by: myf(5) # or myf(x = 5) ## [1] 212.6813 As you can see you get the same results when x=5. But now you can use this in many ways. x &lt;- - 10 : 10 # x now is a vector with 21 numbers -10, -9, … 10 myf(x) ## [1] -1452.45000 -1047.04505 -724.60640 -475.69335 -290.86520 ## [6] -160.68125 -75.70080 -26.48315 -3.58760 2.42655 ## [11] 1.00000 1.57345 13.58760 46.48315 109.70080 ## [16] 212.68125 364.86520 575.69335 854.60640 1211.04505 ## [21] 1654.45000 plot(x, myf(x)) # see plot on the right. Obviously functions can handle many different calculations beyond arithmetic functions. It can take in one or more inputs and return a list of complex data objects too. Exercise 3.5 Write an R function to implement this arithmetic function: f(x)= |x|+5x-6. Note |x| means the absolute value of x. Use this function to find f(4.534), and also produce a plot like the example. Let’s define a function to count even numbers in a vector. #counts the number of even intergers in vec evencount &lt;- function(vec) { k &lt;- 0 # assign o to i #assign 0 to a count varialbe k for (i in vec) { #set i to vec[1],vec[2],... if (i %% 2 == 0) k &lt;- k + 1 # test if i is an even or odd number. %% is the modulo operator } return(k) #print the computed value of k } x=c(2, 5, 7, 8, 14, 12, 8, 3) #a vector evencount(x) #Call the function evencount(). ## [1] 5 A vaiable is called local variable if it is only visible within a function. Such as k and vec are local variables to the function evencount(). They disappear after the function returns. k Error: object ‘vec’ not found vec Error: object ‘vec’ not found A variable is called global variable if it is defined outside of functions.A global variable is also available within functions. Here is an example: myfun.globle &lt;- function (x){ y &lt;- 1 return(x - 2 * y) } myfun.globle(8) # set x=8, and do the calculation: 8-2*y = 8-2*(1)=6 ## [1] 6 Here y is a global variable. The function myfun.globle2() defined below returns the same values as above. But both x and y within the the paratheses (x, y=1) following function are local variables. myfun.globle2 &lt;- function (x, y = 1){ #y is set as 1 within the function by default. return(x - 2 * y) } myfun.globle2(8) #8-2*y = 8-2*(1)=6 ## [1] 6 Exercise 3.6 Define a function counting the values that are less than 0 for two vectors x=rnorm(50) and y=rnorm(5000) respectively. Define another function to calcute the proportion that values are less than 0 for x and y respectively. Compare calculated proportions with theoretical proportion 0.5, what conclusions can you make? 3.1.5 Looking for Help and Example Code ? sum A web page will pope up. This is the official help information for this function. At the bottom of the page is some example code. The quickest way to learn an R function is to run the example codes and see the input and output. You can easily copy, paste, and twist the example code to do your analysis. example() brings up examples of usage for the given function. Try displaying examples for the min function: example(min) ## ## min&gt; require(stats); require(graphics) ## ## min&gt; min(5:1, pi) #-&gt; one number ## [1] 1 ## ## min&gt; pmin(5:1, pi) #-&gt; 5 numbers ## [1] 3.141593 3.141593 3.000000 2.000000 1.000000 ## ## min&gt; x &lt;- sort(rnorm(100)); cH &lt;- 1.35 ## ## min&gt; pmin(cH, quantile(x)) # no names ## [1] -2.7605956 -0.9073021 -0.2438619 0.4374379 1.3500000 ## ## min&gt; pmin(quantile(x), cH) # has names ## 0% 25% 50% 75% 100% ## -2.7605956 -0.9073021 -0.2438619 0.4374379 1.3500000 ## ## min&gt; plot(x, pmin(cH, pmax(-cH, x)), type = &quot;b&quot;, main = &quot;Huber&#39;s function&quot;) ## ## min&gt; cut01 &lt;- function(x) pmax(pmin(x, 1), 0) ## ## min&gt; curve( x^2 - 1/4, -1.4, 1.5, col = 2) ## ## min&gt; curve(cut01(x^2 - 1/4), col = &quot;blue&quot;, add = TRUE, n = 500) ## ## min&gt; ## pmax(), pmin() preserve attributes of *first* argument ## min&gt; D &lt;- diag(x = (3:1)/4) ; n0 &lt;- numeric() ## ## min&gt; stopifnot(identical(D, cut01(D) ), ## min+ identical(n0, cut01(n0)), ## min+ identical(n0, cut01(NULL)), ## min+ identical(n0, pmax(3:1, n0, 2)), ## min+ identical(n0, pmax(n0, 4))) min(5:1, pi) # -&gt; one number ## [1] 1 Example commands and plots will show up automatically by typing Return in RStudio. In R, you need to click on the plots. example(boxplot) # bring example of boxplot I found a lot of help information about R through Google. Google tolerate typos, grammar errors, and different notations. Also, most (99 %) of your questions have been asked and answered on various forums. Many R gurus answered a ton of questions on web sites like** stackoverflow.com**, with example codes! I also use Google as a reference. It is important to add comments to your code. Everything after the “#” will be ignored by R when running. We often recycle and repurpose our codes. max(1, 3, 5) # return the maximum value of a vector ## [1] 5 3.2 Data structures 3.2.1 Vectors A vector is an object that holds a sequence of values of the same type. A vector’s values can be numbers, strings, logical values, or any other type, as long as they’re all the same type. They can come from a column of a data frame. if we have a vector x: x &lt;- c(5, 2, 22, 11, 5) x ## [1] 5 2 22 11 5 Here c stands for concatenate, do not use it as variable name. It is as special as you! Vectors can not hold values with different modes (types). Try mixing modes and see what happens: c(1, TRUE, &quot;three&quot;) ## [1] &quot;1&quot; &quot;TRUE&quot; &quot;three&quot; All the values were converted to a single mode (characters) so that the vector can hold them all. To hold diverse types of values, you will need a list, which is explained later in this chapter. If you need a vector with a sequence of numbers you can create it with start:end notation. This is often used in loops and operations on the indices of vectors etc. Let’s make a vector with values from 5 through 9: 5:9 ## [1] 5 6 7 8 9 A more versatile way to make sequences is to call the seq function. Let’s do the same thing with seq: seq(from = 5, to = 9) ## [1] 5 6 7 8 9 seq also allows you to use increments other than 1. Try it with steps of 0.5: seq(from = 5, to = 9, by = .5) ## [1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 Create a sequence from 5 to 9 with length 15: seq(from = 5, to = 9, length = 15) ## [1] 5.000000 5.285714 5.571429 5.857143 6.142857 6.428571 6.714286 ## [8] 7.000000 7.285714 7.571429 7.857143 8.142857 8.428571 8.714286 ## [15] 9.000000 Exercise 3.7 Compute 1+2+3… +1000 with one line of R code. Hint: examine the example code for sum( ) function in the R help document. 3.2.1.1 Commands about vector Next we will try those commands about vector. First let’s find out what is the 4th element of our vector x &lt;- c(5, 2, 22, 11, 5), or the elements from 2 to 4. x[4] ## [1] 11 x[2:4] ## [1] 2 22 11 If you define the vector as y, y &lt;- x[2:4] No result is returned but you “captured” the result in a new vector, which holds 3 numbers. You can type y and hit enter to see the results. Or do some computing with it. y &lt;- x[2:4]; y ## [1] 2 22 11 This does exactly the same in one line. Semicolon separates multiple commands. Now if we want to know the number of elements in the vector length(x) ## [1] 5 It’s also easy to know about the maximum, minimum, sum, mean and median individually or together. We can get standard deviation too. max(x) ## [1] 22 min(x) ## [1] 2 sum(x) ## [1] 45 mean(x) ## [1] 9 median(x) ## [1] 5 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2 5 5 9 11 22 sd(x) ## [1] 7.968689 rank() function ranks the elements. Ties are shown as the average of these ranks. While sort() will sort from the smallest to the biggest, decreasing = T will make it sort form the biggest to the smallest. rank(x) ## [1] 2.5 1.0 5.0 4.0 2.5 sort(x) ## [1] 2 5 5 11 22 sort(x, decreasing = T) ## [1] 22 11 5 5 2 diff() lag and iterate the differences of vector x. diff(x) ## [1] -3 20 -11 -6 rev() will reverse the position of the elements in the vector. rev(x) ## [1] 5 11 22 2 5 Operations are performed element by element. Same for log, sqrt, x^2, etc. They return vectors too. log(x) ## [1] 1.6094379 0.6931472 3.0910425 2.3978953 1.6094379 sqrt(x) ## [1] 2.236068 1.414214 4.690416 3.316625 2.236068 x^2 ## [1] 25 4 484 121 25 2*x + 1 ## [1] 11 5 45 23 11 If we don’t want the second element and save it as y: y &lt;- x[-2] y ## [1] 5 22 11 5 Add an element 100 to the vector x between the second and the third element: x &lt;- c(5, 2, 22, 11, 5) x &lt;- c(x[1:2], 100, x[3:5] ) #add an element 100 to x between elements 8 and 9 x #The value 100 is added to the previous vector x ## [1] 5 2 100 22 11 5 Length of the new created x is: length(x) ## [1] 6 To add a new element to the end, we can use the two commands below, they generate same result. x &lt;- c(5, 2, 22, 11, 5) c(x, 7) ## [1] 5 2 22 11 5 7 append(x, 7) ## [1] 5 2 22 11 5 7 Creat an empty vector y: y &lt;- c() y ## NULL length(y) ## [1] 0 Sometimes we are interested in unique elements: x &lt;- c(5, 2, 22, 11, 5) unique(x) ## [1] 5 2 22 11 And the frequencies of the unique elements: x &lt;- c(5, 2, 22, 11, 5) table(x) ## x ## 2 5 11 22 ## 1 2 1 1 If we are interested in the index of the maximum or minimum: x &lt;- c(5, 2, 22, 11, 5) which.max(x) ## [1] 3 which.min(x) ## [1] 2 Or we need to look for the location of a special value: x &lt;- c(5, 2, 22, 11, 5) which(x == 11) ## [1] 4 Or more complicated, we want to find the locations where \\(x^2&gt;100\\): x &lt;- c(5, 2, 22, 11, 5) x^2 ## [1] 25 4 484 121 25 which (x^2 &gt; 100) ## [1] 3 4 We can randomly select some elements from the vector. Run the following code more than once, do you always get the same results? The answer is “No”. Because the 3 elements are randomly selected. x &lt;- c(5, 2, 22, 11, 5) sample(x, 3) ## [1] 2 5 22 Elements in the vector can have names. Type “x” in the command window to see the difference. x &lt;- c(5, 2, 22, 11, 5) names(x) &lt;- c(&quot;David&quot;, &quot;Breck&quot;, &quot;Zach&quot;, &quot;Amy&quot;, &quot;John&quot;) x ## David Breck Zach Amy John ## 5 2 22 11 5 Now we can refer to the elements by their names. x[&quot;Amy&quot;] ## Amy ## 11 The any() and all() functions produce logical values. They return if any of all of their arguments are TRUE. x &lt;- c(5, 2, 22, 11, 5) any(x &lt; 10) ## [1] TRUE x &lt;- c(5, 2, 22, 11, 5) any(x &lt; 0) ## [1] FALSE x &lt;- c(5, 2, 22, 11, 5) all(x &lt; 10) ## [1] FALSE x &lt;- c(5, 2, 22, 11, 5) any(x &gt; 0) ## [1] TRUE x&gt;10 ## [1] FALSE FALSE TRUE TRUE FALSE If we want to get a subset from a vector, there are multiple methods can be used.Here are some examples: x &lt;- c(NA, 2, -4, NA, 9, -1, 5) x ## [1] NA 2 -4 NA 9 -1 5 y &lt;- x[x &lt; 0] y ## [1] NA -4 NA -1 There are annoying NAs in the subset y. We can remove the NAs by applying na.rm() function to y. Or we can use the subset() function to get a “clean” data without NAs. subset(x, x &lt; 0) ## [1] -4 -1 The ifelse() function allows us do conditional element selection. The usage is ifelse(test, yes, no). The yes and no depends on the test is true of false. Here are two examples. x &lt;- c(2, -3, 4, -1, -5, 6) y &lt;- ifelse(x &gt; 0, &#39;Positive&#39;, &#39;Negative&#39;) y ## [1] &quot;Positive&quot; &quot;Negative&quot; &quot;Positive&quot; &quot;Negative&quot; &quot;Negative&quot; &quot;Positive&quot; In this example, the element in y is either ‘positive’ or ‘negative’. It depends on x greater than 0 or less than 0. x &lt;- c(3, 4, -6, 1, -2) y &lt;- ifelse (x &lt; 0, abs(x), 2 * x + 1) y ## [1] 7 9 6 3 2 In this example, if an element in x is less than 0, then take the absolute value of the element. Otherwise multiply the element by 2 then add 1. Exercise 3.8 Using sample selction function randomly select 10 integers from 1 to 100. Create a vector y which satisfies the following conditions: if an selcted integer is an even number, then y returns ‘even’, otherwise y returns ‘odd’. If we have two vectors and try to compare them with each other: x &lt;- c(5, 2, 22, 11, 5) y &lt;- c(5, 11, 8) z &lt;- match(y, x) z ## [1] 1 4 NA match() returns the locations in 2nd vector. NA means missing, not found. To check if NA is in a vector, we use the function is.na( ). Note that the result is a vector holding logical values. Do we have missing value in our vector? is.na(x) ## [1] FALSE FALSE FALSE FALSE FALSE is.na(z) ## [1] FALSE FALSE TRUE Sometimes, when working with sample data, a given value isn’t available. But it’s not a good idea to just throw those values out. R has a value that explicitly indicates a sample was not available: NA. Many functions that work with vectors treat this value specially. For our z vector, try to get the sum of its values, and see what the result is: sum(z) ## [1] NA The sum is considered “not available” by default because one of the vector’s values was NA. This is the responsible thing to do; R won’t just blithely add up the numbers without warning you about the incomplete data. We can explicitly tell sum (and many other functions) to remove NA values before they do their calculations, however. Bring up documentation for the sum function: ? sum sum package:base R Documentation … As you see in the documentation, sum can take an optional named argument, na.rm. It’s set to FALSE by default, but if you set it to TRUE, all NA arguments will be removed from the vector before the calculation is performed. Try calling sum again, with na.rm parameter set to TRUE: sum(z, na.rm = TRUE) ## [1] 5 Exercise 3.9 Now compute the average of values in z. Ignore the missing values. Let’s using examples to show the differences between NULL and NA. # build up a vector of numbers greater than 10 in vector vec.x vec.x &lt;- c(40, 3, 11, 0, 9) z1 &lt;- NULL for (i in vec.x) { if (i &gt; 10) z1 &lt;- c(z1, i) } z1 ## [1] 40 11 length(z1) ## [1] 2 # build up a vector of numbers greater than 10 in vector vec.x vec.x &lt;- c(40, 3, 11, 0, 9) z2 &lt;- NA for (i in vec.x) { if (i &gt; 10) z2 &lt;- c(z2, i) } z2 ## [1] NA 40 11 length(z2) ## [1] 3 Comparing the length of z1 and z2, we know the NULL is counted as nonexistent, but the NA is counted as a missing value. Let’s do some opertations related to vectors. Firsly, we start from the operation between a vector and a scalar. # Operation between x and a scalar x &lt;- c(1, 4, 8, 9, 10) y &lt;- 1 x+y ## [1] 2 5 9 10 11 As you can see, 1 is added to each element in x. The operation is equivalent to: x &lt;- c(1, 4, 8, 9, 10) y &lt;- c(1, 1, 1, 1, 1) x+y ## [1] 2 5 9 10 11 The operation between vectors with same length is element-wise. For example: # Operation between two vectors with same length x &lt;- c(1, 4, 8, 9, 10) y &lt;- c(1, 2, 0, 3, 15) x+y ## [1] 2 6 8 12 25 x*y ## [1] 1 8 0 27 150 If vectors have different length, then R will automatically recycles the shorter one, untill it has the same length as the longer one. For example: x &lt;- c(1, 4, 8, 9, 10) y &lt;- c(1, 2) x+y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 2 6 9 11 11 The y was recyled, in fact the real operation is showed below: x &lt;- c(1, 4, 8, 9, 10) y &lt;- c(1, 2, 1, 2, 1) x+y ## [1] 2 6 9 11 11 3.2.1.2 Scatter Plots of two vectors The plot function takes two vectors, one for X values and one for Y values, and draws a graph of them. Let’s draw a graph showing the relationship of numbers and their sines. x &lt;- seq(1, 20, 0.1) y &lt;- sqrt(x) Then simply call plot with your two vectors: plot(x, y) Great job! Notice on the graph that values from the first argument (x) are used for the horizontal axis, and values from the second (y) for the vertical. Exercise 3.10 Create a vector with 21 integers from -10 to 10, and store it in the x variable. Then create a scatterplot of x^2 against x. 3.2.1.3 Fish example of vector Once upon a time, Tom, Jerry, and Mickey went fishing and they caught 7, 3, and 9 fishes, respectively. This information can be stored in a vector, like this: c(7, 3, 9) ## [1] 7 3 9 The c() function creates a new vector by combining a set of values. If we want to continue to use the vector, we hold it in an object and give it a name: fishes &lt;- c(7, 3, 9) fishes ## [1] 7 3 9 fishes is a vector with 3 data elements. There are many functions that operate on vectors. You can plot the vector: barplot(fishes) # see figure 6.1A You can compute the total: sum(fishes) ## [1] 19 We can access the individual elements by indices: fishes[3] ## [1] 9 Exercise 3.11 Does Mickey caught more fishes than Tom and Jerry combined? Write R code to verify this statement using the fishes vector and return a TRUE or FALSE value. Jerry protested that the ¼ inch long fish he caught and released per fishing rules was not counted properly. We can change the values in the 2nd element directly by: fishes[2] &lt;- fishes[2] + 1 On the left side, we take the current value of the 2nd element, which is 3, and add an 1 to it. The result (4) is assigned back to the 2nd element itself. As a result, the 2nd element is increased by 1. This is not an math equation, but a value assignment operation. More rigorously, we should write this as fishes[2] &lt;- fishes[2] + 1 We can also directly overwrite the values. fishes[2] &lt;- 4 fishes ## [1] 7 4 9 They started a camp fire, and each ate 1 fish for dinner. Now the fishes left: fishes2 &lt;- fishes - 1 fishes2 ## [1] 6 3 8 Most arithmetic operations work just as well on vectors as they do on single values. R subtracts 1 from each individual element. If you add a scalar (a single value) to a vector, the scalar will be added to each value in the vector, returning a new vector with the results. While they are sleeping in their camping site, a fox stole 3 fishes from Jerry’s bucket, and 4 fishes from Mickey’s bucket. How many left? stolen &lt;- c(0, 3, 4) # a new vector fishes2 - stolen ## [1] 6 0 4 If you add or subtract two vectors of the same length, R will take the corresponding values from each vector and add or subtract them. The 0 is necessary to keep the vector length the same. Proud of himself, Mickey wanted to make a 5ft x 5ft poster to show he is the best fisherman. Knowing that a picture worthes a thousand words, he learned R and started plotting. He absolutely needs his names on the plots. The data elements in a vector can have names or labels. names(fishes) &lt;- c(&quot;Tom&quot;, &quot;Jerry&quot;, &quot;Mickey&quot;) The right side is a vector, holding 3 character values. These values are assigned as the names of the 3 elements in the fishes vector. names is a built-in function. Our vector looks like: fishes ## Tom Jerry Mickey ## 7 4 9 barplot(fishes) # see figure 6.1B Figure 3.1: Simple Bar plot Assigning names for a vector also enables us to use labels to access each element. Try getting the value for Jerry: fishes[&quot;Jerry&quot;] ## Jerry ## 4 Exercise 3.12 Now see if you can set the value for Tom to something other than 5 using the name rather than the index. Tom proposes that their goal for next fishing trip is to double their catches. 2 * fishes ## Tom Jerry Mickey ## 14 8 18 Hopelessly optimistic, Jerry proposed that next time each should “square” their catches, so that together they may feed the entire school. sum(fishes ^ 2) ## [1] 146 Note that two operations are nested. You can obviously do it in two steps. Exercise 3.13 Create a vector representing the prices of groceries, bread $2.5, milk $3.1, jam $5.3, beer $9.1. And create a bar plot to represent this information. 3.2.2 Matrix operations Matrix is a two dimensional data structure in R programming. Technically, a matrix is also a vector, but with two additional attributes: the number of rows and the number of columns. A matrix has rows and columns, but it can only contain one type of values, i.e. numbers, characters, or logical values. We can creat a matrix by using rbind or cbind function. rbind combine all row Here are two examples: m &lt;- rbind(c(3, 4, 5), c(10, 13, 15)) # combine vectors by row m ## [,1] [,2] [,3] ## [1,] 3 4 5 ## [2,] 10 13 15 n &lt;- cbind(c(3, 4, 5), c(10, 13, 15), c(3, 2, 1)) # combine vectors by column n ## [,1] [,2] [,3] ## [1,] 3 10 3 ## [2,] 4 13 2 ## [3,] 5 15 1 s &lt;- rbind(m,n) #combine two matrices m and n by row s ## [,1] [,2] [,3] ## [1,] 3 4 5 ## [2,] 10 13 15 ## [3,] 3 10 3 ## [4,] 4 13 2 ## [5,] 5 15 1 To use rbind() combining matrices by row, the matrices must have same number of columns. Similar to cbind(), the matrices must have same number of rows. We can also create a matrix by using the matrix() function: x &lt;- matrix(seq(1:12), nrow = 4, ncol = 3) x ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 The argument seq() create a sequence from 1 to 12, nrom() define the number of rows in the matrix, ncol() define the number of columns in the matrix. We don’t have to give both nrom() and ncol() since if one is provided, the other is inferred from length of the data. y &lt;- matrix(seq(1:12), nrow = 4) y ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 As we can see that, the matrix is filled in column-wise by default. If you want to fill a matrix by row-wise, add the byrow = TRUE to the argument: z &lt;- matrix(seq(1:12), nrow = 4, byrow = TRUE) # fill matrix row-wise z ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 The following code will create an empty matrix y: w &lt;- matrix(nrow = 4, ncol = 3) w ## [,1] [,2] [,3] ## [1,] NA NA NA ## [2,] NA NA NA ## [3,] NA NA NA ## [4,] NA NA NA We can assign values to the matrix. For example, let’s assign the value 3 to the position at first row and first column and value 100 to position of the second row and third column: w[1,1] &lt;- 3 w[2,3] &lt;- 100 w ## [,1] [,2] [,3] ## [1,] 3 NA NA ## [2,] NA NA 100 ## [3,] NA NA NA ## [4,] NA NA NA We can also create a matrix from a vector by setting its dimension using function dim(). x &lt;- c(1, 5, 6, 9, 8, 10, 21, 15, 76) x ## [1] 1 5 6 9 8 10 21 15 76 class(x) ## [1] &quot;numeric&quot; dim(x) &lt;- c(3, 3) x ## [,1] [,2] [,3] ## [1,] 1 9 21 ## [2,] 5 8 15 ## [3,] 6 10 76 class(x) ## [1] &quot;matrix&quot; We can convert a non-matrix data set to a matrix using as.matrix() function. Take the data iris as an example. subset.iris &lt;- iris[1:10, 1:4] class(subset.iris) ## [1] &quot;data.frame&quot; The data structure of subset.iris is a data frame. The function as.matrix wii transfer a data frame to a matix. x &lt;- as.matrix(subset.iris) class(x) ## [1] &quot;matrix&quot; Various matrix operation can be applied in R. For example: x &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2) x ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 x^2 #Squared each element in x ## [,1] [,2] [,3] ## [1,] 1 9 25 ## [2,] 4 16 36 You can transform the matrix if you want, for the convenience of view and analysis. y &lt;- t(x) # transpose of x y ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 x %*% y # Matrix Multiplication ## [,1] [,2] ## [1,] 35 44 ## [2,] 44 56 #x - y # Matrix subtraction Error in x - y : non-conformable arrays The error reminders us that the matrices for subtraction must have same dimensions. y &lt;- matrix(rep(1, 6), nrow = 2) y ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 x - y ## [,1] [,2] [,3] ## [1,] 0 2 4 ## [2,] 1 3 5 We can produce a new matrix by each element is doubled and added 5 z &lt;- 2 * x + 5 z ## [,1] [,2] [,3] ## [1,] 7 11 15 ## [2,] 9 13 17 We can also get a logical matrix using logical code like: x &lt;- matrix(c(12, 34, 51, 27, 26, 10), ncol = 2) x &gt; 20 ## [,1] [,2] ## [1,] FALSE TRUE ## [2,] TRUE TRUE ## [3,] TRUE FALSE We can extract all TRUE results from x by using x[x&gt;20]. x[x &gt; 20] ## [1] 34 51 27 26 Similar we can define a vector with logical values, then apply it to x to get all TRUE values. log.vau &lt;- c(FALSE, TRUE, TRUE, TRUE, TRUE, FALSE) x[log.vau] ## [1] 34 51 27 26 Remember matrix is a vector, and filled by column-wise. Therefore the vector with logical values applies to x by column-wise order. Since matrix is a vector with two dimension, all operations for vectors also apply to matrix. For example: x[1, ] # Get the first row of x ## [1] 12 27 a &lt;- as.matrix(iris[, 1:4]) #Take out the first 4 columns of iris and convert it to matrix. c &lt;- a[5:10, 2:4] # Extract a subset c ## Sepal.Width Petal.Length Petal.Width ## [1,] 3.6 1.4 0.2 ## [2,] 3.9 1.7 0.4 ## [3,] 3.4 1.4 0.3 ## [4,] 3.4 1.5 0.2 ## [5,] 2.9 1.4 0.2 ## [6,] 3.1 1.5 0.1 x &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2) c[1:2, ] &lt;- x # Replace the first two rows of c with x c ## Sepal.Width Petal.Length Petal.Width ## [1,] 1.0 3.0 5.0 ## [2,] 2.0 4.0 6.0 ## [3,] 3.4 1.4 0.3 ## [4,] 3.4 1.5 0.2 ## [5,] 2.9 1.4 0.2 ## [6,] 3.1 1.5 0.1 Now if we want to know the mean and sum of these rows and columns, try rowMeans(), colMeans(), rowSums(), colSums(). x &lt;- as.matrix(iris[1:10, 1:4]) rowMeans(x) ## 1 2 3 4 5 6 7 8 9 10 ## 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 colMeans(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 4.86 3.31 1.45 0.22 rowSums(x) ## 1 2 3 4 5 6 7 8 9 10 ## 10.2 9.5 9.4 9.4 10.2 11.4 9.7 10.1 8.9 9.6 colSums(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 48.6 33.1 14.5 2.2 Here we are computing the standard deviation by columns, using apply() function.The second argument “1” or “2” in apply() reprents the function applies to rows or columns. apply(x, 2, sd) # Calculate the standard deviation of each column in x ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 0.29135698 0.30713732 0.10801234 0.07888106 Or median by rows, using 1 for rows. apply(x, 1, median) # Calculate the median of each row in x ## 1 2 3 4 5 6 7 8 9 10 ## 2.45 2.20 2.25 2.30 2.50 2.80 2.40 2.45 2.15 2.30 Heatmap is my favorite type of graph for visualizing a large matrix data. heatmap(x, scale = &quot;column&quot;, margins = c(10,5)) Exercise 3.14 Let subset.iris &lt;- as.matrix(iris[1:10, 1:4]), Using apply function to calculate the mean of subset.iris by column. Example: Define a function to find the positions of minimal value in each column of subset.iris &lt;- as.matrix(iris[1:10, 1:4]). find.min.posi &lt;- function(x){ y &lt;- function(xcol){ return(which.min(xcol)) } return(apply(x, 2, y)) } subset.iris &lt;- as.matrix(iris[1:10, 1:4]) subset.iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5.0 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 find.min.posi(subset.iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 9 9 3 10 As you can see that the minimal value for Sepal.Length is 4.4 which locates at the 9th row. Similar to other variable. The minimal value locates at the 9th, 3rd and 10th row for Sepal.Width, Petal.Lenth and Petal.Width respectively. Exercise 3.15 Let subset.iris.2 &lt;- as.matrix(iris[, 1:4]), fill blanks in the find.max function defined below to find the maximal value in each column of subset.iris.2. find.max &lt;- function(x){ y &lt;- function(xcol){ return(_________) } return(_______(x, ____, y)) } ________(subset.iris.2) To make a matrix more easily readable, we can use functions colnames() and rownames to assign names to the columns and rows of the matrix. For example: y &lt;- rbind(c(1, 3, 5), c(2, 4, 6)) y ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 colnames(y) &lt;- c(&#39;First.Col&#39;, &#39;Second.Col&#39;, &#39;Third.Col&#39;) row.names(y) &lt;- c(&#39;odd.number&#39;, &#39;even.number&#39;) y ## First.Col Second.Col Third.Col ## odd.number 1 3 5 ## even.number 2 4 6 Another interesting application of matrix is the manipulation of images.You may have heard pixel matrix which represents “picure elements”. They are small little dots making up images. Each image is a matrix with thousands or even millions of pixels. Each pixel can only be one color at a time. If we have a grayscale imge, the brightness of color for each pixel is determined by the value assigned to it. In other words, the pixel value is a single number that represents the brightness of the pixel. For example, if the color value 0.2941 is assigned to a pixel which locates 3rd row and 4th column, then the image at 3rd row and 4th column is pretty dark. The range of the colors from black to white correspond to the scale varies from 0% to 100%. Most time, we need to blur images or add mosaic to a picture for various purposes. Let’s use one example to demonstrate how to add mosaic to a grayscale image. Example: Add mosaic to Einstein image Firstly, let’s read an grayscale image of Einstein into R and view the image. library(pixmap) EINSTEIN &lt;- read.pnm(&quot;EINSTEIN.pgm&quot;, cellres = 1) plot(EINSTEIN) Then let us look at structure of this image: str(EINSTEIN) ## Formal class &#39;pixmapGrey&#39; [package &quot;pixmap&quot;] with 6 slots ## ..@ grey : num [1:512, 1:512] 0.596 0.541 0.522 0.529 0.561 ... ## ..@ channels: chr &quot;grey&quot; ## ..@ size : int [1:2] 512 512 ## ..@ cellres : num [1:2] 1 1 ## ..@ bbox : num [1:4] 0 0 512 512 ## ..@ bbcent : logi FALSE Here we get a new class which is S4 type. We don’t go into depth for this time. One fact we need to pay attendtion is that we must use “@” instead of “$” sign to designate the components. class(EINSTEIN@ grey) ## [1] &quot;matrix&quot; The class of EINSTEIN@ grey is matrix. The output ..@ grey : num [1:512, 1:512] shows that the dimension of matrix is 512*512. The values “0.596 0.541 0.522 0.529 0.561 …” reprent the brightness values in the matrix. For example, the value of the pixel at 3rd row and 4th column is 0.2941176 as showed below: EINSTEIN@ grey[3,4] ## [1] 0.2941176 If we change the value 0.2941176 to 0 by EINSTEIN@ grey[3,4] &lt;- 0, then that pixel will become pure black. If we assign a random number between 0 to 1 to the value, then the color in the pixel will be randomly assigned based on the random number. Using this idea, we define a masoic function which will be used to blur the image. mosaic.plot &lt;- function(image, yrange, xrange){ length.y &lt;- length(yrange) length.x &lt;- length(xrange) image2 &lt;- image whitenoise &lt;- matrix(nrow = length.y, ncol = length.x, runif(length.y * length.x)) image2@grey[yrange, xrange] &lt;- whitenoise return(image2) } The argument image is the original image, yrange is the range of rows that you want to blur, xrange is the range of columns that you want to blur. The xrange and yrange construct the mosaic region. Since we don’t want to change the origial image,therefore we copy the original image to image2 by assigning image to image2. The whitenoise creates a matrix filled with random numbers following uniform distribution. The dimensions of whitenoise is determined by the mosaic region. Replace the values in the original image that you want to blur image2@grey[yrange,xrange] by the whitenoise matrix. EINSTEIN.mosaic &lt;- mosaic.plot(EINSTEIN, 175:249, 118:295) plot(EINSTEIN.mosaic) Here, we take yrange=175:249 and xrange=118:295 to select a sub-matrix from 175th row to 249th row, and 118th column to 295th column. This sub-matrix store the color values of Einstein’ eyes region. The sub-matix is replaced by whitenoise matrix. Therefore the image near eyes region are replaced by random colors. The function locator() allows us to find the relevant rows and columns. Type locator() in the Console window, then R will wait for you to click a point within an image, then click esc on your keyboard to exit the function. Then the function will return the coordinates of that point in the Console window. If you click more points once, then the function will return all coordinates of these points sorted by your clicking order. You must be careful about the y-coordinate. The row numbers in pixmap objects increase from the top of the image to the bottom, therefore you need to opposite the y-coordinate by subtracting them from the number of rows in the original image. For example, the y-coordinates that I obtained from locator() function are 337 and 263. After subtracting them from 512, I got 175 and 249. They are the yrange used in the mosaic function. Exercise 3.16 Using the mosaic.plot() function to blur the image of mona_lisa.pgm by adding mosaic to her eyes region. Your output should look like the graph below. Exercise 3.17 Fill in blanks of the following function so that the eyes region of Mona Lisa is covered by pure color, not mosaic. mosaic.plot.2 &lt;- function(picture, yrange, xrange, degree){ length.y &lt;- _______(yrange) length.x &lt;- length(______) pic2 &lt;- picture pic2@____[_______,_______] &lt;- degree return(pic2) } Setting the degree=0, 0.5 and 1 respectively, plot the images of Mona Lisa that eyes region are covered by pure colors. 3.2.3 Lists All elements in vectors must be same type. The list can combine objectes of different types. Let’s start from creating a list: y &lt;- list(5, &quot;John Doe&quot;, c(100, 7), mean) # a list with 4 components y ## [[1]] ## [1] 5 ## ## [[2]] ## [1] &quot;John Doe&quot; ## ## [[3]] ## [1] 100 7 ## ## [[4]] ## function (x, ...) ## UseMethod(&quot;mean&quot;) ## &lt;bytecode: 0x00000000182bc130&gt; ## &lt;environment: namespace:base&gt; There are 4 components in the list y. We can associate each components with a tag. We add tags “height”, “name”, “BP” and “fun” to the y. y &lt;- list(height = 5, name = &quot;John Doe&quot;, BP = c(100, 77), fun = mean) y ## $height ## [1] 5 ## ## $name ## [1] &quot;John Doe&quot; ## ## $BP ## [1] 100 77 ## ## $fun ## function (x, ...) ## UseMethod(&quot;mean&quot;) ## &lt;bytecode: 0x00000000182bc130&gt; ## &lt;environment: namespace:base&gt; Here is another example for creating a list: x1 &lt;- matrix(c(4, 2, 5, 6, 10, 9), ncol = 3) # create a matrix x2 &lt;- 8 : 1 x3 &lt;- c(2.4, 5.1, 9.0, 4.4) x4 &lt;- c(&#39;u&#39;, &#39;v&#39;, &#39;w&#39;) z &lt;- list(x1, x2, x3, x4) z ## [[1]] ## [,1] [,2] [,3] ## [1,] 4 5 10 ## [2,] 2 6 9 ## ## [[2]] ## [1] 8 7 6 5 4 3 2 1 ## ## [[3]] ## [1] 2.4 5.1 9.0 4.4 ## ## [[4]] ## [1] &quot;u&quot; &quot;v&quot; &quot;w&quot; We can build a list by creating an empty list and assigning values to it. v &lt;- list() #Create an empty list v[[1]] &lt;- matrix(c(4, 2, 5, 6, 10, 9), ncol = 3) # Assign a matrix to v[[1]] v[[2]] &lt;- 8 : 1 #Assign a vector to v[[2]] v[[3]] &lt;- x3 #Assign x3 to v[[3]] where x3 is defined as above v[[4]] &lt;- x4 #Assign x4 to v[[4]] where x4 is defined as above v ## [[1]] ## [,1] [,2] [,3] ## [1,] 4 5 10 ## [2,] 2 6 9 ## ## [[2]] ## [1] 8 7 6 5 4 3 2 1 ## ## [[3]] ## [1] 2.4 5.1 9.0 4.4 ## ## [[4]] ## [1] &quot;u&quot; &quot;v&quot; &quot;w&quot; We used different methods creating list z and v. The list v and z are identical because that we assigned same components for v as that of in z. There are several different ways to access the elements in a list. For example, to access the third component in y: y &lt;- list(height = 5, name = &quot;John Doe&quot;, BP = c(100, 77), fun = mean) y[[3]] #Specifing the number in a double square braket ## [1] 100 77 y$BP #Using $ sign ## [1] 100 77 y[[&quot;BP&quot;]] #Using the tag ## [1] 100 77 Note: we used double square braket ‘[[]]’ to access the element in a list, which is different from using single square braket ‘[]’ in a vector. If we use sigle square braket in a list, we will get a sublist. For example, y[3] returns a list, y[[3]] returns a numerical vector. Functions class() and is.list can be used to check the types. y[3] ## $BP ## [1] 100 77 class(y[3]) ## [1] &quot;list&quot; is.list(y[3]) ## [1] TRUE y[[3]] ## [1] 100 77 class(y[[3]]) #Get the class of y[[3]] ## [1] &quot;numeric&quot; is.list(y[[3]]) #Check if y[[3]] is a list or not ## [1] FALSE The forth component of y is fun = mean. There are no quotation marks around mean. The class(y[[4]]) returns a function. class(y[[4]]) ## [1] &quot;function&quot; This implies that y[[4]] is same as the mean() function. For example: y[[4]](c(1, 2, 3, 4, 5)) #Get the mean of 1, 2, 3, 4, 5 ## [1] 3 Exercise 3.18 Access the second component in list1 by three ways as we introduced above. What is the type of list1[“Hobby”] and list1[[“Hobby”]] respectively? list1 &lt;- list(Name = &quot;Tom&quot;, Hobby = &quot;Fishing&quot;, Num.fish = c(16, 27, 5)) Here are some examples for list operation: y &lt;- list(height = 5, name = &quot;John Doe&quot;, BP = c(100, 77), fun = mean) y[[2]] &lt;- &quot;Mike&quot; #Change the 2nd component from &quot;John Doe&quot; to &quot;Mike&quot; y[[4]] &lt;- NULL #Delete the 4th component by setting it to NULL y ## $height ## [1] 5 ## ## $name ## [1] &quot;Mike&quot; ## ## $BP ## [1] 100 77 The output shows that the name has been changed to Mike and the mean function is deleted. So far, the length of y is 3: length(y) ## [1] 3 We can add the forth components to y using the following code: y$class &lt;- c(&quot;math&quot;, &quot;art&quot;) #Add a class component y ## $height ## [1] 5 ## ## $name ## [1] &quot;Mike&quot; ## ## $BP ## [1] 100 77 ## ## $class ## [1] &quot;math&quot; &quot;art&quot; If we have a vector of tags and a vector of corresponding values, we can associate the names and values into a list by creating an empty list and then fill it via assignment statements: sales &lt;- c(100, 105, 98, 112) seasons &lt;- c(&quot;Spring&quot;, &quot;Summer&quot;, &quot;Fall&quot;, &quot;Winter&quot;) sale.tag &lt;- list() #Create an empty list sale.tag[seasons] &lt;- sales sale.tag ## $Spring ## [1] 100 ## ## $Summer ## [1] 105 ## ## $Fall ## [1] 98 ## ## $Winter ## [1] 112 Equivalently, we can associate the names and values by creating an empty list and then assign each component corresponding value or vector. For example. sale.tag &lt;- list() #Create an empty list sale.tag[[&quot;Spring&quot;]] &lt;- 100 sale.tag[[&quot;Summer&quot;]] &lt;- 105 sale.tag[[&quot;Fall&quot;]] &lt;- 98 sale.tag[[&quot;Winter&quot;]] &lt;- 112 sale.tag ## $Spring ## [1] 100 ## ## $Summer ## [1] 105 ## ## $Fall ## [1] 98 ## ## $Winter ## [1] 112 Exercise 3.19 Based on the data set of sale.tag, choose all correct answers from the following options A - F. A: sale.tag[[1]] B: sale.tag[[Spring]] C: sale.tag[[“Spring”]] D: sale.tag[“Spring”] E: sale.tag[1] F: sale.tag[Spring] 1, Which options return a vector. Answer:___________ 2, Which options return a list. Answer:___________ 3, Which options returns an error. Answer:___________ Suppose we want to calculate the mean of sales in the dataset sale.tag. Because basic statistical functions work on vectors but not on lists. The mean() does not work: &gt; mean(sale.tag) [1] NA Warning message: In mean.default(sale.tag) : argument is not numeric or logical: returning NA To calculate the mean, we need to use function unlist() to flatten the list into a vector. mean(unlist(sale.tag)) ## [1] 103.75 Similar to apply() funciton works on vectors, lapply() (for list apply) works on each component of lists. Here is an example: x &lt;- list(c(10, 20, 30), c(4, 5, 6, 7, 8)) mean.x &lt;- lapply(x, mean) mean.x ## [[1]] ## [1] 20 ## ## [[2]] ## [1] 6 The mean.x returns a list consisting of 20 and 6 which are means of components of x respectively. When the list returned by lapply() could be simplified to a vector or matrix, we can use function sapply() (for simplified [l]apply). Let’s redo the previous example by sapply() function. mean.x.2 &lt;- sapply(x, mean) mean.x.2 ## [1] 20 6 We can double check the outputs by class() or is.vector() function: class(mean.x) ## [1] &quot;list&quot; class(mean.x.2) ## [1] &quot;numeric&quot; is.vector(mean.x.2) ## [1] TRUE Here is another example for sapply() function. Suppose we have NULL elements in a list.null. To remove the NULL element, apply is.null function to every element of the list.null. If the logical value is TRUE, i.e. the element is a NULL, then delete the element by setting NULL to it. list.null &lt;- list(&#39;art&#39;, NULL, c(2, 4, 6, 8)) list.null ## [[1]] ## [1] &quot;art&quot; ## ## [[2]] ## NULL ## ## [[3]] ## [1] 2 4 6 8 list.null[sapply(list.null, is.null)] &lt;- NULL list.null ## [[1]] ## [1] &quot;art&quot; ## ## [[2]] ## [1] 2 4 6 8 For a small size data set, if we can locate the position of NULL element, we can easily remove the NULL element by setting NULL to that element. For example, we already know the second element of list.null is NULL, to remove this NULL element, we set a NULL to the second element: list.null &lt;- list(&#39;art&#39;, NULL, c(2, 4, 6, 8)) list.null[[2]] &lt;- NULL list.null ## [[1]] ## [1] &quot;art&quot; ## ## [[2]] ## [1] 2 4 6 8 But for large size of data set, we turn to the sapply() function to remove NULL elements. Example: Suppose we have a list of customers who visited our store in the last 10 days. Some customers visited the store more than once. We want to find out on which days and the total number of days that each customer visited. The customer names are recorded by the order of the day they came: customer &lt;- list(&quot;Alex&quot;, &quot;Brandon&quot;, &quot;Alex&quot;, &quot;Daniel&quot;, &quot;Grace&quot;, &quot;Mary&quot;, &quot;Mary&quot;, &quot;Alex&quot;, &quot;Tom&quot;, &quot;Grace&quot;) Firsly, we define a function which returns all locations of same elements in a list. loc.names &lt;- function(f){ y &lt;- unlist(f) # Flatten a list x into a vector y x &lt;- list() # Create an empty list for (i in 1:length(y)){ c.name &lt;- y[i] # Assign the ith element to c.name. x[[c.name]] &lt;- c(x[[c.name]],i) # assign values to x[[c.name]]. c.name is the name of x. } return(x) } the f will be replaced by your own list. In this example, f will be replaced by the list of customer when we apply the funtion. x &lt;- list () # create an empty list. In the for loop, i starts from 1 to the length of y. In our example, the length of y is the length of list of customers: 10. c.name &lt;- y[i] # Assign the ith element in y to a new variable c.name. For example, if i=1, y[1] is the “Alex”, then c.name = “Alex”. x[[c.name]] &lt;- c(x[[c.name]], i) For i=1, we have c.name &lt;- “Alex”, then x[[c.name]] = x[[“Alex”]] = NULL since x starting from an empty list. Therefore c(x[[“Alex”]], i) = c(NULL, i) = i = 1. The value 1 will be assigned to x[[“Alex”]] by the code: x[[“Alex”]] &lt;- c(x[[“Alex”]], i) = 1. Now x is not an empty list, it is a list with 1 component: x &lt;- list() x[[&quot;Alex&quot;]] &lt;- 1 x ## $Alex ## [1] 1 For i=2, y[2] &lt;- “Brandon”, then similar to the case of i=1, we have x[[“Brandon”]] &lt;- 1: x[[&quot;Brandon&quot;]] &lt;- 1 x ## $Alex ## [1] 1 ## ## $Brandon ## [1] 1 For i=3, y[3] &lt;- “Alex”. We have met “Alex” once, which returned x[[“Alex”]] &lt;- 1. Now we meet “Alex” again which means we are going to update x[[“Alex”]] by x[[“Alex”]] &lt;- c(x[[“Alex”]], i) = c(1, i) = c(1, 3). The updated x is: x[[&quot;Alex&quot;]] &lt;- c(1, 3) x ## $Alex ## [1] 1 3 ## ## $Brandon ## [1] 1 The process will stop till i = length (y) = 10. The loc.names() funciton will return all locations of all names which are already stored in x. Let’s find all days of cumsters who visited to our store. customer &lt;- list(&quot;Alex&quot;, &quot;Brandon&quot;, &quot;Alex&quot;, &quot;Daniel&quot;, &quot;Grace&quot;, &quot;Mary&quot;, &quot;Mary&quot;, &quot;Alex&quot;, &quot;Tom&quot;, &quot;Grace&quot;) v1 &lt;- loc.names(customer) v1 ## $Alex ## [1] 1 3 8 ## ## $Brandon ## [1] 2 ## ## $Daniel ## [1] 4 ## ## $Grace ## [1] 5 10 ## ## $Mary ## [1] 6 7 ## ## $Tom ## [1] 9 The output tells us that Alex visited our store at the first, third and eighth days, Brandon came at the second day, etc. Then we use sapply() function to calculate the repeat times of each element. v2 &lt;- sapply(v1,length) # Calculate the length of each component in v1 v2 ## Alex Brandon Daniel Grace Mary Tom ## 3 1 1 2 2 1 You can sort the v2 by the number of days that they presented by function sort(): sort(v2, decreasing = T) ## Alex Grace Mary Brandon Daniel Tom ## 3 2 2 1 1 1 Exercise 3.20 Fill blanks in the function my.fun() so that for a given list, the function returns all locations of each element and sort the frequency of the elements in an increasing order. Then apply my.fun() to a set of gender: F F M M M M I I F I F M I F M I I M I I F Where F, M and I represent Female, Male and Infant respectively. my.fun &lt;- function(f){ y &lt;- unlist(f) x &lt;- ___________ for (i in 1:____){ g1 &lt;- y[i] x[[______]] &lt;- c(x[[____]],i) } freq &lt;- _________(x, length) z &lt;- sort(________, decreasing = ____) lst &lt;- list(x, z) ___________(lst) } gender &lt;- list( “F”, ________________________________, “F”) __________(gender) # Many R functions, such as t.test(), returns results as a list, which contain a series of components, such as a p value, a vector of residuals or coefficients, and even a matrix of data. A list is the natural way to represent this sort of thing as one big object that could be parsed. r1 &lt;- rnorm(100) # Create 100 random numbers following Normal Distribution r2 &lt;- rnorm(100) t.result &lt;- t.test(r1, r2) # Run the t test. Test if the mean of r1 and r2 are same. is.list(t.result) # Check if the t.result is a list ## [1] TRUE t.result # A list holds all components of t-test ## ## Welch Two Sample t-test ## ## data: r1 and r2 ## t = -0.23312, df = 196.75, p-value = 0.8159 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.3183481 0.2510406 ## sample estimates: ## mean of x mean of y ## -0.06306022 -0.02940646 names(t.result) # Returns all tags of the t test result ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; ## [6] &quot;null.value&quot; &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; t.result$p.value # Retrieves p-value for the test ## [1] 0.8159094 t.result$estimate # Returns mean of r1 and r2 ## mean of x mean of y ## -0.06306022 -0.02940646 The help page of t.test contains information about what types of values are returned and their names. Run the ?t.test to access the help page. Let’s run a simulation using a loop. What we want to do is to generate two sets of 100 random numbers from the standard normal distribution with zero mean and unit standard deviation, and perform t-test and get the p value. By repeating this process 500 times, we want to see the distribution of p values and count how many times we get significant result with p &lt; 0.05. pvalues &lt;- rep(1, 500) # define a vector containing 500 numbers, all equal to 1. for (i in 1:500) { # Loop: The values of i takes values from 1,2,3, …, 500 result = t.test(rnorm(100), rnorm(100)) pvalues[i] = result$p.value # p values are stored in the i-th element in the vector } hist(pvalues) # Histogram of p-values summary(pvalues) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.002938 0.229169 0.435681 0.468743 0.707609 0.999934 sum(pvalues &lt; 0.05) # Number of p-value&#39;s which are less than 0.05. ## [1] 31 3.2.4 Data Frames Similar to a matrix, a data frame is a two-dimensional array-like structure. But it differs from the matrix in that the mode of each column could be different. In fact, a data frame is a special case of a two-dimensional list. The data set iris that we analyzied before is a data frame. class(iris) ## [1] &quot;data.frame&quot; head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa There are some properties of a data frame.Firstly, the mode for each column in a data frame can be of numeric, factor or character. Such as the class of Sepal.Length is numeric, and the class of Species is factor. class(iris$Sepal.Length) ## [1] &quot;numeric&quot; class(iris$Species) ## [1] &quot;factor&quot; Secondly, the column names must be non-empty. Usually, the columns represent various variables in a statistical problem. There are 5 column names or variables in the iris data set, they are Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species respectively. colnames(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [5] &quot;Species&quot; Thirdly, each column should contain same number of data items. In other words, each variable contains same observations. Fourthly, the row names are unique. If they are not pre-named, the indices “1”, “2”, “3”, \\(\\cdots\\), “n” will be assigned to each row, where “n” is the length of a row or the number of observations. For example, there are 150 observations for each variable in the iris data, the rows names are shown below: rownames(iris) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; ## [12] &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; ## [23] &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; ## [34] &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; ## [45] &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; ## [56] &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; ## [67] &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; &quot;74&quot; &quot;75&quot; &quot;76&quot; &quot;77&quot; ## [78] &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; &quot;82&quot; &quot;83&quot; &quot;84&quot; &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; ## [89] &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; &quot;96&quot; &quot;97&quot; &quot;98&quot; &quot;99&quot; ## [100] &quot;100&quot; &quot;101&quot; &quot;102&quot; &quot;103&quot; &quot;104&quot; &quot;105&quot; &quot;106&quot; &quot;107&quot; &quot;108&quot; &quot;109&quot; &quot;110&quot; ## [111] &quot;111&quot; &quot;112&quot; &quot;113&quot; &quot;114&quot; &quot;115&quot; &quot;116&quot; &quot;117&quot; &quot;118&quot; &quot;119&quot; &quot;120&quot; &quot;121&quot; ## [122] &quot;122&quot; &quot;123&quot; &quot;124&quot; &quot;125&quot; &quot;126&quot; &quot;127&quot; &quot;128&quot; &quot;129&quot; &quot;130&quot; &quot;131&quot; &quot;132&quot; ## [133] &quot;133&quot; &quot;134&quot; &quot;135&quot; &quot;136&quot; &quot;137&quot; &quot;138&quot; &quot;139&quot; &quot;140&quot; &quot;141&quot; &quot;142&quot; &quot;143&quot; ## [144] &quot;144&quot; &quot;145&quot; &quot;146&quot; &quot;147&quot; &quot;148&quot; &quot;149&quot; &quot;150&quot; We can creat a data frame by using function data.frame() to combine vectors. For example: x &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) y &lt;- c(41, 32, 13, 89) z &lt;- c(&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;FALSE&quot;, &quot;TRUE&quot;) df1 &lt;- data.frame(x,y,z) df1 ## x y z ## 1 A 41 TRUE ## 2 B 32 FALSE ## 3 C 13 FALSE ## 4 D 89 TRUE As you can see in the output, the underneath the x means that the class of x has been changed from a character to a factor. So does the z. We can check the change by the class() function: class(x) ## [1] &quot;character&quot; class(df1$x) ## [1] &quot;factor&quot; To keep the class of x unchanged after becoming a component of a data frame, we need an argument stringsAsFactors = FALSE in the data.frame() function. class(df1$x) ## [1] &quot;factor&quot; df2 &lt;- data.frame(x,y,z,stringsAsFactors = FALSE) df2 ## x y z ## 1 A 41 TRUE ## 2 B 32 FALSE ## 3 C 13 FALSE ## 4 D 89 TRUE class(df2$x) ## [1] &quot;character&quot; ``` Exercise 3.21 What is the class of the R built-in data set mtcars? Show the function that you used to check the class. Once data is read in as data frame, these are commands you can use to analyze it. Read in data frame x: x &lt;- iris Using summary() we can get descriptive statistics of each column. summary(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## head() and tail() functions show the fist and last few rows. head(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica If we want to know both number of rows and number of columns of the data frame: dim(x) ## [1] 150 5 We can just get number of rows or number of columns separately: nrow(x) ## [1] 150 ncol(x) ## [1] 5 str() is a very useful function, which shows data types for all columns. str(x) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Like matrix, we can select one element in a data frame. Let’s select the element in the second row and third column: x[2, 3] ## [1] 1.4 Also we can subtract a subset data frame from x. For example, let’s select the columns 2 to 4 and rows 1 to 10: x[1:10, 2:4] ## Sepal.Width Petal.Length Petal.Width ## 1 3.5 1.4 0.2 ## 2 3.0 1.4 0.2 ## 3 3.2 1.3 0.2 ## 4 3.1 1.5 0.2 ## 5 3.6 1.4 0.2 ## 6 3.9 1.7 0.4 ## 7 3.4 1.4 0.3 ## 8 3.4 1.5 0.2 ## 9 2.9 1.4 0.2 ## 10 3.1 1.5 0.1 We can view the first column: x[, 1] ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 ## [18] 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 ## [35] 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 ## [52] 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 ## [69] 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 ## [86] 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 ## [103] 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 ## [120] 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 ## [137] 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 Using the data frame name x followed column name has the same effect. x$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 ## [18] 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 ## [35] 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 ## [52] 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 ## [69] 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 ## [86] 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 ## [103] 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 ## [120] 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 ## [137] 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 The argument drop=FALSE will keep a one-column vector to a data frame. For example: class(x[, 1]) ## [1] &quot;numeric&quot; class(x[,1, drop = FALSE]) ## [1] &quot;data.frame&quot; If we need to know the average Sepal Length, we use mean() function. By the way, expressions can be nested. mean(x$Sepal.Length) ## [1] 5.843333 It’s very common to select a subset of data by certain column. The “==”, “&gt;”, and “&lt;” are logical operations and the “=” is an equality sign for assigning value. Let’s select a subset which contains only the species of setosa: y &lt;- subset(x, Species == &quot;setosa&quot;) head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Similar we can obtain a subset that the lengh of sepal are greater than 7. subset(x, Sepal.Length &gt; 7) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 103 7.1 3.0 5.9 2.1 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 136 7.7 3.0 6.1 2.3 virginica If there are more than one conditions are applied to the data, we use &amp; to connect the conditions. For example, we want to select a subset which contains only the species of virginica, and the length of sepal is greater than 7, meanwhile remove the length of petal from the data: x.3conds &lt;- x[x$Sepal.Length&gt;7 &amp; x$Species == &quot;virginica&quot;, -3] # -3 means removing the 3rd column from x. Exercise 3.22 Select a subset which satisfies that the cyl is 6 and mpg greater than 21.2, meanwhile the subset doesn’t include the variable carb. We can add new column named “id” to the data frame, which goes from 1 to 150. Function head() is used to view the first 6 rows of the new data set. x.id &lt;- cbind(id = c(1:150), x) head(x.id) ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 1 5.1 3.5 1.4 0.2 setosa ## 2 2 4.9 3.0 1.4 0.2 setosa ## 3 3 4.7 3.2 1.3 0.2 setosa ## 4 4 4.6 3.1 1.5 0.2 setosa ## 5 5 5.0 3.6 1.4 0.2 setosa ## 6 6 5.4 3.9 1.7 0.4 setosa Adding another column of random numbers y to x.id. y &lt;- rnorm(150) x2 &lt;- cbind(y, x.id) head(x2) ## y id Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 -0.5088504 1 5.1 3.5 1.4 0.2 setosa ## 2 -0.5166610 2 4.9 3.0 1.4 0.2 setosa ## 3 0.8596682 3 4.7 3.2 1.3 0.2 setosa ## 4 -1.1868927 4 4.6 3.1 1.5 0.2 setosa ## 5 0.5840677 5 5.0 3.6 1.4 0.2 setosa ## 6 -0.4281178 6 5.4 3.9 1.7 0.4 setosa Similar as cbind(), rbind() is for adding another exist row or rows with same length of columns. Now we use tail() to examine the result. newRow &lt;- c(1, 1, 1, 1, &quot;setosa&quot;, 151) x3 &lt;- rbind(x, newRow) tail(x3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 146 6.7 3 5.2 2.3 virginica ## 147 6.3 2.5 5 1.9 virginica ## 148 6.5 3 5.2 2 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3 5.1 1.8 virginica ## 151 1 1 1 1 setosa We can sort the data frame by certain column. For example we need to sort first column in ascending, or to sort second column in descending order. y &lt;- x[order(x[, 1]), ] head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 14 4.3 3.0 1.1 0.1 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 4 4.6 3.1 1.5 0.2 setosa y &lt;- x[rev(order(x[, 2])), ] head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 16 5.7 4.4 1.5 0.4 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 6 5.4 3.9 1.7 0.4 setosa We can view sepal length by species using boxplot. boxplot(Sepal.Length ~ Species, x) We can obtain summary statistics by category. Let’s calculate the mean of the variables by species: aggregate(. ~ Species, x, mean) ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 setosa 5.006 3.428 1.462 0.246 ## 2 versicolor 5.936 2.770 4.260 1.326 ## 3 virginica 6.588 2.974 5.552 2.026 Exercise 3.23 Use aggregate() function to calculate the median of the variables: mpg, disp, hp, and wt across the number of cylinders cyl in the mtcars data set. It is easier to conduct statistical analysis after organizing data as data frame. Analysis of variance, or ANOVA tests whether sepal length has significant difference cross species in data frame iris. m &lt;- aov(Sepal.Length ~ Species, iris) summary(m) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 63.21 31.606 119.3 &lt;2e-16 *** ## Residuals 147 38.96 0.265 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The p-value is close to 0, therefore we reject the null hypothesis and conclude that the length of sepal among three species are not same. We can use the sapply() function to do the ANOVA test for all of the 4 numeric variables acorss the species. aov.fun &lt;- function(temx){ m1 &lt;- aov(temx ~ iris$Species) summary(m1) } nvar &lt;- iris[-5] # Remove the variable Species which lies at the 5th column in the data set. da &lt;- sapply(nvar, aov.fun) # Apply the aov.fun() function to each of the variable in the data set nvar. da ## $Sepal.Length ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iris$Species 2 63.212 31.606 119.26 &lt; 2.2e-16 *** ## Residuals 147 38.956 0.265 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## $Sepal.Width ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iris$Species 2 11.345 5.6725 49.16 &lt; 2.2e-16 *** ## Residuals 147 16.962 0.1154 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## $Petal.Length ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iris$Species 2 437.10 218.551 1180.2 &lt; 2.2e-16 *** ## Residuals 147 27.22 0.185 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## $Petal.Width ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iris$Species 2 80.413 40.207 960.01 &lt; 2.2e-16 *** ## Residuals 147 6.157 0.042 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Regression analysis use linear model(lm) to analyze the relationship of these columns. Here we use sepal length as a function of sepal width and petal length plus error. m &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length, x) summary(m) ## ## Call: ## lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.96159 -0.23489 0.00077 0.21453 0.78557 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.24914 0.24797 9.07 7.04e-16 *** ## Sepal.Width 0.59552 0.06933 8.59 1.16e-14 *** ## Petal.Length 0.47192 0.01712 27.57 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3333 on 147 degrees of freedom ## Multiple R-squared: 0.8402, Adjusted R-squared: 0.838 ## F-statistic: 386.4 on 2 and 147 DF, p-value: &lt; 2.2e-16 Exercise 3.24 Test if the variables mpg, disp, hp, wt and qsec have significant difference across the three cyl in the R build data set mtcars. Fill the blanks and interpret your conclusion based on the p-values. aov.fun.car &lt;- function(temx){ m2 &lt;- ________(________ ~ _________$______) summary(________) } sub.car &lt;- mtcars[, ___________] aov.car &lt;- ___________(_________, __________) ____________ We can merge two data frames by the function merge(). Here are some examples: dfmg1 &lt;- data.frame(V1 = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), V2 = c(90, 80, 70, 60), stringsAsFactors = FALSE) dfmg1 ## V1 V2 ## 1 A 90 ## 2 B 80 ## 3 C 70 ## 4 D 60 dfmg2 &lt;- data.frame(V1 = c(&quot;A&quot;, &quot;C&quot;), W2 = c(95, 85), stringsAsFactors = FALSE) dfmg2 ## V1 W2 ## 1 A 95 ## 2 C 85 merge(dfmg1, dfmg2) ## V1 V2 W2 ## 1 A 90 95 ## 2 C 70 85 Here the two data frames dfmg1 and dfmg2 have the same variable V1. After merging the two data frames, the rows with common elements “A” and “C” in both V1’s are selected. If two columns contains similar information, but they have different names in different data frames, then we can use by.x and by.y arguments to merge them. Let by.x= and by.y= specify the variable names in the first and second data frame respectively. See the example below. dfmg3 &lt;- data.frame(W1 = c(&quot;A&quot;, &quot;F&quot;, &quot;C&quot;), W2 = c(95, 85, 65), stringsAsFactors = FALSE) dfmg3 ## W1 W2 ## 1 A 95 ## 2 F 85 ## 3 C 65 merge(dfmg1, dfmg3, by.x=&quot;V1&quot;, by.y=&quot;W1&quot;) ## V1 V2 W2 ## 1 A 90 95 ## 2 C 70 65 We use data.frame() function to define a new data frame with 100 rows and two columns, ids and rand. y &lt;- data.frame(id = 1:100, rand = rnorm(100)) head(y) ## id rand ## 1 1 -0.9690806 ## 2 2 -0.8627249 ## 3 3 0.5116143 ## 4 4 1.8832466 ## 5 5 -0.4020629 ## 6 6 1.1601572 tail(y) ## id rand ## 95 95 0.1027033 ## 96 96 0.9739388 ## 97 97 0.4851516 ## 98 98 0.6876934 ## 99 99 -2.1557477 ## 100 100 0.1931663 Now we merge two data frames x and y by a common column named id. Note the new data frame z has only 100 rows. We can also merge data frame by row names too. x &lt;- iris x$id = c(1:150) z &lt;- merge(x, y, by = &quot;id&quot;) head(z) ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species rand ## 1 1 5.1 3.5 1.4 0.2 setosa -0.9690806 ## 2 2 4.9 3.0 1.4 0.2 setosa -0.8627249 ## 3 3 4.7 3.2 1.3 0.2 setosa 0.5116143 ## 4 4 4.6 3.1 1.5 0.2 setosa 1.8832466 ## 5 5 5.0 3.6 1.4 0.2 setosa -0.4020629 ## 6 6 5.4 3.9 1.7 0.4 setosa 1.1601572 tail(z) ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 95 95 5.6 2.7 4.2 1.3 versicolor ## 96 96 5.7 3.0 4.2 1.2 versicolor ## 97 97 5.7 2.9 4.2 1.3 versicolor ## 98 98 6.2 2.9 4.3 1.3 versicolor ## 99 99 5.1 2.5 3.0 1.1 versicolor ## 100 100 5.7 2.8 4.1 1.3 versicolor ## rand ## 95 0.1027033 ## 96 0.9739388 ## 97 0.4851516 ## 98 0.6876934 ## 99 -2.1557477 ## 100 0.1931663 summary(z) ## id Sepal.Length Sepal.Width Petal.Length ## Min. : 1.00 Min. :4.300 Min. :2.000 Min. :1.000 ## 1st Qu.: 25.75 1st Qu.:5.000 1st Qu.:2.800 1st Qu.:1.500 ## Median : 50.50 Median :5.400 Median :3.050 Median :2.450 ## Mean : 50.50 Mean :5.471 Mean :3.099 Mean :2.861 ## 3rd Qu.: 75.25 3rd Qu.:5.900 3rd Qu.:3.400 3rd Qu.:4.325 ## Max. :100.00 Max. :7.000 Max. :4.400 Max. :5.100 ## Petal.Width Species rand ## Min. :0.100 setosa :50 Min. :-2.567629 ## 1st Qu.:0.200 versicolor:50 1st Qu.:-0.597593 ## Median :0.800 virginica : 0 Median : 0.040094 ## Mean :0.786 Mean : 0.008087 ## 3rd Qu.:1.300 3rd Qu.: 0.528937 ## Max. :1.800 Max. : 2.518522 3.2.5 Strings and string vectors We encounter text data sometimes. Plus, we also have row and column names. We can easily manipulate these string objects. Define a string x: “R is cool” x &lt;- &quot;R is cool&quot; If we want to know the number of character of the string: nchar(x) ## [1] 9 We can concatenate strings. By default a space is added. paste(x, &quot;!!&quot;) ## [1] &quot;R is cool !!&quot; If we want to extract sub-string from position of 6 and 9: substr(x, 6, 9) ## [1] &quot;cool&quot; Split string into a list which is separated by space: strsplit(x, &quot; &quot;) ## [[1]] ## [1] &quot;R&quot; &quot;is&quot; &quot;cool&quot; Find pattern “R” and replace with “Tim”: gsub(&quot;R&quot;, &quot;Tim&quot;, x) ## [1] &quot;Tim is cool&quot; Or remove space followed by anything: gsub(&quot; .*&quot;, &quot;&quot;, x) ## [1] &quot;R&quot; We can search for pattern like “is” in the string. grepl(&quot;is&quot;, x) ## [1] TRUE We can also convert the whole string into low case or upper case by “tolower” or “toupper” function. tolower(x) ## [1] &quot;r is cool&quot; toupper(x) ## [1] &quot;R IS COOL&quot; A string vector can hold many strings. This can be a column of names or IDs in a table. First let’s define a character vector x &lt;- c(“ab”, “cde”, “d”, “ab”). x &lt;- c(&quot;ab&quot;, &quot;cde&quot;, &quot;d&quot;, &quot;ab&quot;) We can use all commands about vector for our string vector, such as 2nd element in vector: x[2] ## [1] &quot;cde&quot; Number of strings in the vector: length(x) ## [1] 4 Unique elements: unique(x) ## [1] &quot;ab&quot; &quot;cde&quot; &quot;d&quot; Now we are interested in the duplicated element in the string vector. Is there any duplicated element in the vector? duplicated(x) ## [1] FALSE FALSE FALSE TRUE The last element is duplicated. F denotes FALSE, and T for TRUE. If we want to know the number of characters in each of the element: nchar(x) ## [1] 2 3 1 2 We can also unite two vectors x and y if we define another vector y first: y &lt;- c(&quot;ab&quot;, &quot;e&quot;) union(x, y) ## [1] &quot;ab&quot; &quot;cde&quot; &quot;d&quot; &quot;e&quot; Is there intercept among these two sets of strings? intersect(x, y) ## [1] &quot;ab&quot; We can add something like “Q” to each element: paste(x, &quot;Q&quot;) ## [1] &quot;ab Q&quot; &quot;cde Q&quot; &quot;d Q&quot; &quot;ab Q&quot; To get rid of the space between these element and “Q”, try paste0: paste0(x, &quot;Q&quot;) ## [1] &quot;abQ&quot; &quot;cdeQ&quot; &quot;dQ&quot; &quot;abQ&quot; If we want to collapse multiple strings into one, which is joined by space: paste(x, collapse = &quot; &quot;) ## [1] &quot;ab cde d ab&quot; Using these functions, we can achieve many things. For example if we have a piece of DNA sequence: DNA &lt;- &quot;taaCCATTGtaaGAACATGGTTGTCcaaaCAAGATGCTAGT&quot; Note that I am using the assignment operator “&lt;-”, instead of “=”, which also works most of the times but it could be ambiguous. First we need to convert everything to upper case. DNA &lt;- toupper(DNA) Next, we want to cut this DNA into smaller pieces by looking for a certain pattern “ATG”. This type of thing happens in nature, as some enzymes cut DNA according to certain pattern. segs &lt;- strsplit(DNA, &quot;ATG&quot;) The result is contained in an object segs, which is a list. We needed the unlist( ) function to convert list to a string vector. segs &lt;- unlist(segs) segs # a vector of strings ## [1] &quot;TAACCATTGTAAGAAC&quot; &quot;GTTGTCCAAACAAG&quot; &quot;CTAGT&quot; segs[1] # first segment ## [1] &quot;TAACCATTGTAAGAAC&quot; Exercise 3.25 In the iris flower dataset iris, define a new column called FullName which contains the full species name by adding “Iris“ in front of species name. In other words, “setosa” should become “Iris setosa”, “virginica” would be “Iris virginica”, and “versicolor” needs to be “Iris versicolor”. 3.3 For more in-depth exercises Go to: http://tryr.codeschool.com/ "],
["data-importing.html", "Chapter 4 Data importing 4.1 Enter data manually 4.2 Reading data from file using Import dataset in Rstudio 4.3 Data manipulation in a data frame 4.4 Reading file using read.table, read.csv, etc. 4.5 General procedure to read data into R: 4.6 Recommended workflow for EVERY project", " Chapter 4 Data importing There are many different ways to get data into R. You can enter data manually (see below), or semi-manually (see below). You can read data into R from a local file or a file on the internet. You can also use R to retrieve data from databases, local or remote. The most import thing is to read data set into R correctly. A dataset not read in correctly will never be analyzed or visualized correctly. 4.1 Enter data manually x &lt;- c(2.1, 3.1, 3.2, 5.4) sum(x) ## [1] 13.8 A &lt;- matrix( c(2, 4, 3, 1, 5, 7), # the data elements nrow = 2, # number of rows ncol = 3) # number of columns A # show the matrix ## [,1] [,2] [,3] ## [1,] 2 3 5 ## [2,] 4 1 7 x &lt;- scan() # Enter values from keyboard, separated by Return key. End by empty line. 2.1 ## [1] 2.1 3.1 ## [1] 3.1 4.1 ## [1] 4.1 Note that you can paste a column of numbers from Excel. 4.2 Reading data from file using Import dataset in Rstudio Before reading files into R, we often need to open the files to take a look. Notepad or WordPad that come with Windows is very limited (and sooo amateur)! Do not even think about using Microsoft Word! I strongly recommend that you install a powerful text editor such as NotePad++ (https://notepad-plus-plus.org/), or TextPad (https://www.textpad.com/). If you are a Mac user, try TextMate, TextWrangler etc. I use NotePad++ almost every day to look into data, and also write R programs, as it can highlight R commands based on R syntax. I even use a tool called NppToR (https://sourceforge.net/projects/npptor/) to send R commands from NotePad++ directly to R, and I love it! Regardless of their extensions in file names, all plain text files can be opened by these text editors. Plain text files only contain text without any formatting, links and images. The file names can be “poems.txt”, “poems.tex”, “students.csv”, or just “data” without extension. I often save my R scripts as text file with names like “code_1-22-2017.R”. You can import text files, regardless of file names, to Microsoft Excel, which can properly parse your file into columns if the correct delimiter is specified. Comma separated values (CSV) files, use comma to separate the columns. CSV files can also be conveniently opened by Excel. And Rstudio likes it too. So let’s try to use CSV files. Another common type is tab-delimitated text files, which uses the tab or \\(\\t\\) as it is invisible character. Other types of files such as Excel .xls or .xlsx files often needed to be saved as CSV files. Probably the most intuitive way to read data into Rstudio is to use the Import dataset function available at File-&gt;Import Dataset from the Rstudio menu. You can also click Import dataset button on the top-right of the Rstudio interface. The preferred format is CSV. But it is not required. You can open a text file with Excel, and save it as a CSV file. Download data and read background information Download the heartatk4R.txt file from this page http://statland.org/R/RC/tables4R.htm. It is a tab-deliminated text file, meaning the different columns are separated by tab, hence the “\\(\\t\\)” above. 2. In Rstudio, click File-&gt;Import Dataset-&gt;From text(readr)…, find the file on your hard drive. You should change the Delimiter to “tab”, and the preview shows that the data is correctly parsed into multiple columns. You can also change the name of your data object by changing the default “heartatk4R” to “x” on the lower left of the import interface. See Figure 4.1. The awesome nerds at Rstudio actually helped you generating these 3 lines of code: library(readr) x &lt;- read_delim(&quot;datasets/heartatk4R.txt&quot;,&quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) View(x) # shows the data, will change if you modify it. Before you click Import, I highly recommend that you select all the codes and copy it to your clipboard. After clicking Import, you can paste the code into a script window. If you do not have a script window open, you can create one by clicking the File + icon on the top left. Copy and paste these code to your script file. You will need it when you want to re-run the analysis without going through the above steps. You can see the data appears as a spreadsheet, which can be sorted by clicking on the column names. This spreadsheet can be closed. To reopen, click on x object, which is a data frame named after the input file. You data is now available as x. Figure 4.1: Importing data into R. Check data type. Most of the times, R can guess the type of data in each column. But we always need to double check using the str command. If not satisfied, we can enforce data type conversion in R using as.numeric, as.factor, or as.character functions. str(x) # structure of data object, data types for each column ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : num 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: num 41041 41041 41091 41081 41091 ... ## $ SEX : chr &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... ## $ DRG : num 122 122 122 122 122 121 121 121 121 123 ... ## $ DIED : num 0 0 0 0 0 0 0 0 0 1 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : chr &quot;0010&quot; &quot;0006&quot; &quot;0005&quot; &quot;0002&quot; ... ## $ AGE : chr &quot;079&quot; &quot;034&quot; &quot;076&quot; &quot;080&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Patient = col_double(), ## .. DIAGNOSIS = col_double(), ## .. SEX = col_character(), ## .. DRG = col_double(), ## .. DIED = col_double(), ## .. CHARGES = col_double(), ## .. LOS = col_character(), ## .. AGE = col_character() ## .. ) Note that the first column is just patient id number runs from 1 to 12844. It will not be useful in our analysis. The numbers in DIAGNOSIS, DRG, and DIED are integers but they actually code for certain categories. They are not measurements. It does not make sense, for example, to add them or average them. Most of the times, there is no particular order. The same is true for SEX. We need to reformat these columns as factors. We are going to use x$SEX to refer to the SEX column of the data frame x: x$DIAGNOSIS &lt;- as.factor(x$DIAGNOSIS) # convert this column to factor x$SEX &lt;- as.factor(x$SEX) x$DRG &lt;- as.factor(x$DRG) x$DIED &lt;- as.factor(x$DIED) Now the last three columns are actually numeric measurements. But the LOS and AGE were actually read as characters, due the fact that 10 is recorded as 0010. x$LOS &lt;- as.numeric(x$LOS) # convert to numeric x$AGE &lt;- as.numeric(x$AGE) str(x) # double check structure of data ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : num 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41001&quot;,&quot;41011&quot;,..: 5 5 9 8 9 9 9 9 5 5 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : num 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : num 79 34 76 80 55 84 84 70 76 65 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Patient = col_double(), ## .. DIAGNOSIS = col_double(), ## .. SEX = col_character(), ## .. DRG = col_double(), ## .. DIED = col_double(), ## .. CHARGES = col_double(), ## .. LOS = col_character(), ## .. AGE = col_character() ## .. ) summary(x) # a summary often gives us a lot of useful information ## Patient DIAGNOSIS SEX DRG DIED ## Min. : 1 41091 :5213 F:5065 121:5387 0:11434 ## 1st Qu.: 3212 41041 :2665 M:7779 122:6047 1: 1410 ## Median : 6422 41011 :1824 123:1410 ## Mean : 6422 41071 :1703 ## 3rd Qu.: 9633 41001 : 467 ## Max. :12844 41081 : 287 ## (Other): 685 ## CHARGES LOS AGE ## Min. : 3 Min. : 0.000 Min. : 20.00 ## 1st Qu.: 5422 1st Qu.: 4.000 1st Qu.: 57.00 ## Median : 8445 Median : 7.000 Median : 67.00 ## Mean : 9879 Mean : 7.569 Mean : 66.29 ## 3rd Qu.:12569 3rd Qu.:10.000 3rd Qu.: 77.00 ## Max. :47910 Max. :38.000 Max. :103.00 ## NA&#39;s :699 The summary( ) function is very useful to get basic information about data frames. Note that for numeric columns we are shown mean, median, etc, while for factors the frequencies are shown. This reassured us that the data types are correctly recognized. It also shows missing values for CHARGES. Some people got free treatment for heart attack? Maybe not. Missing does not mean zero. Maybe the data was not entered for some patients. Except enforcing data type conversion by as.factor, as.numeric and so on, We can also reformat the columns before clicking Import: After locating the file, you can click on the automatically detected data type under each of the column names as shown in Figure 4.2. By selecting “Factor” from the drop down and enter all possible levels separated by commas, you can successfully format this column as a factor. Figure 4.2: Changing data types while importing data into Rstudio. But if the column has too many levels, it is trouble to type in manually. We can read the column in as character first and then do as.facter conversion. 4.3 Data manipulation in a data frame We can sort the data by age. Again, type these commands in the script window, instead of directly into the Console window. And save the scripts once a while. x &lt;- x[order(x$AGE), ] # sort by ascending order by AGE Global Environment window contains the names and sizes of all the variables or objects in the computer memory. R programming is all about creating and modifying these objects in the memory with clear, step-by-step instructions. We also can sort the data by clicking on the column names in spreadsheet from Global Environment. Just like in Excel, you can add a new column with computed results: x$pdc &lt;- x$CHARGES / x$LOS Here we created a new column pdc to represent per day cost. We can also create a column to represent age groups using the floor function just returns the integer part. x$ag &lt;- floor(x$AGE/10) * 10 You can now do things like this: boxplot(x$CHARGES ~ x$ag) Each box represents an age group. Older patients tends to stay longer in the hospital after being admitted for heart attack. You can extract a subset of cases: x2 &lt;- subset(x, SEX == &quot;F&quot;) # Only females. “==” is for comparison and “=” is for assign value. x3 &lt;- subset(x, AGE &gt; 80) # only people older than 80 summary(x3) ## Patient DIAGNOSIS SEX DRG DIED ## Min. : 6 41091 :928 F:1263 121:1056 0:1518 ## 1st Qu.: 3527 41071 :347 M: 785 122: 462 1: 530 ## Median : 6834 41011 :280 123: 530 ## Mean : 6633 41041 :275 ## 3rd Qu.: 9734 41001 : 78 ## Max. :12844 41081 : 58 ## (Other): 82 ## CHARGES LOS AGE pdc ## Min. : 92 Min. : 1.000 Min. : 81.00 Min. : 18.4 ## 1st Qu.: 5155 1st Qu.: 5.000 1st Qu.: 82.00 1st Qu.: 833.9 ## Median : 8326 Median : 8.000 Median : 85.00 Median : 1133.7 ## Mean :10135 Mean : 9.131 Mean : 85.64 Mean : 1363.7 ## 3rd Qu.:13369 3rd Qu.:12.000 3rd Qu.: 88.00 3rd Qu.: 1530.0 ## Max. :46915 Max. :38.000 Max. :103.00 Max. :11246.1 ## NA&#39;s :115 NA&#39;s :115 ## ag ## Min. : 80.00 ## 1st Qu.: 80.00 ## Median : 80.00 ## Mean : 81.81 ## 3rd Qu.: 80.00 ## Max. :100.00 ## Try not to attach the data when you are manipulation data like this. Exercise 4.1 Answer the following questions. a). Get a subset which contains the middle-aged men whose ages are greater than 40 and less than or equal to 60. b). Calculate cost per day for middle-aged men. c). Generate a histogram of the cost per day. 4.4 Reading file using read.table, read.csv, etc. As you get more experience with R programming, there are many other options to import data. Create a new project folder by File-&gt;New Project-&gt;New Directory-&gt;Empty Project and then create a new folder. Download and save the heartatk4R.txt file to that new folder. That folder becomes your default working directory. Now you can read the data by yourself. heartatk4R &lt;- read.table(&quot;datasets/heartatk4R.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) Then we can repeat the above commands to check and adjust data types for the columns, using str(), as.factor(), and as.numeric() functions. In summary, we have the following code to read in the data. Reading the heart attack dataset. I am not using the Import Dataset in Rstudio. We have to make sure the file is in the current working directory. To set working directory from Rstudio main menu, go to Session -&gt; Set Working Directory. rm(list = ls()) # Erase all objects in memory getwd() # show working directory x &lt;- read.table(&quot;datasets/heartatk4R.txt&quot;, sep=&quot;\\t&quot;, header = TRUE) head(x) # show the first few rows # change several columns to factors x$DRG &lt;- as.factor(x$DRG) x$DIED &lt;- as.factor(x$DIED) x$DIAGNOSIS &lt;- as.factor(x$DIAGNOSIS) x$SEX &lt;- as.factor(x$SEX) str(x) # show the data types of columns summary(x) # show summary of dataset Alternatively, you can skip all of the above and do this. x &lt;- read.table(&quot;http://statland.org/R/RC/heartatk4R.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, colClasses = c(&quot;character&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;)) We are reading data directly from the internet with the URL. And we are specifying the data type for each column. 4.5 General procedure to read data into R: If data is compressed, unzip using 7-zip, WinRAR, Winzip, gzip. Any of these will do. Is it a text file (CSV, txt, …) or Binary file (XLS, XLSX, …)? Convert binary to text file using corresponding application. Comma separated values (CSV) files, use comma to separate the columns. Another common type is tab-delimitated text files, which uses the tab or \\(\\t\\) as it is invisible character. Open with a text editor (TexPad, NotePad++) to have a look. Rows and columns? Row and column names? row.names = 1, header = T Deliminaters between columns?(space, comma, tab…) sep = “\\(\\t\\)” Missing values? NA, na, NULL, blank, NaN, 0 missingstring = Open as text file in Excel, choose appropriate deliminater while importing, or use the Text to Column under Data in Excel. Beware of the annoying automatic conversion in Excel “OCT4”-&gt;“4-OCT”. Edit column names by removing spaces, or shorten them for easy of reference in R. Save as CSV for reading in R. Change working directory to where the file was saved. Main menu: File-&gt;Change dir… read.table ( ), or read.csv( ). For example, x &lt;- read.table(“somefile.txt”, sep = “\\(\\t\\)”, header = TRUE, missingstring = “NA”) Double check the data with str( x ), make sure each column is recognized correctly as “character”, “factor” and “numeric”. Pay attention to columns contain numbers but are actually IDs (i.e. student IDs), these should be treated as character. For example, x $ ids &lt;- as.character(x $ ids), here x is the data frame and ids is the column name. Also pay attention to columns contain numbers but actually codes for some discrete categories (1, 2, 3, representing treatment 1, treatment 2 and treatment 3). These need to be reformatted as factors. This could be done with something like x $ treatment &lt;- as.factor(x $ treatment). Refresher using cheat sheets that summarize many R functions is available here: https://www.rstudio.com/resources/cheatsheets/. It is important to know the different types of R objects: scalars, vectors, data frames, matrix, and lists. 4.6 Recommended workflow for EVERY project It is best to create a separate folder that contains all related files. You can do the same for research projects. In Rstudio, this is called a Project. 1. Create a project in a new folder. I recommend you to start by setting up a project in a new folder by going to File-&gt;New project-&gt;New Directory-&gt;Empty Project. Then choose where the directory will be created on your hard drive. I created a directory called “learnR” under “C:\\Ge working \\ RBook” . Rstudio creates a Project file named like “learnR.Rproj”, which contains information such as scripts files and working folders. Projects files can be saved and later opened from File-&gt;Open. You get everything ready to go on a particular assignment or research project. Copy required data files to the new directory. From Windows or Mac operation systems, you can now copy all required data files to the directory just created. Creating a script file. Once you have a new project created, the first step is to start a new script file by clicking the File + button or go to File-&gt;New file and choose R script file. By default, the script file is called Untitled1.R. Rstudio will ask you to change it the first time you hit “Save” button . Start your R script by adding comments on background information. Comments starting with “#” are ignored by R when running, but they are helpful for humans, including yourself, to understand the code. We re-cycle and re-use our codes over and over, so it is vital to add information about what a chunk of code does. Figure 4.3 shows a recommended workflow for beginning your script. Write your scripts while saving your project files. If you click on the Run button, Rstudio runs the current line of code where your cursor is located. You can also select multiple lines and run them at once. You can jump back and forth but remember you are operating on the data objects sequentially. So sometimes you want to get a fresh start by running the reset line, namely:rm(list=ls()). This command lists and then deletes all data objects from R’s brain. As you develop your coding skills, following these guidelines can make you more efficient. Remember to save everything once a while by hitting the Save button on the main icon bar! Even though Rstudio saves your scripts every 5 seconds, it can crash. Figure 4.3: Beginning a project in Rstudio, a recommended workflow: commenting, resetting, checking working folder. Exercise 4.2 Type in Table 4.1 in Excel and save as a CSV file. Create a new Rstudio project as outlined above. Copy the CSV file to the new folder. Import the csv file to Rstudio. Create a script file which includes the rm(list = ls()) and getwd() command, the generated R code when importing the csv file, (similar to those shown in Figure 4.2), and the code that convert data types (Age, BloodPressure and Weight should be numeric, LastName should be character and HeartAttack should be factor). Name the data set as patients. Submit the R script your created, data structure of the data set patient, and use head(patients) to show the data. Table 4.1: An example of a multivariate dataset. LastName Age Sex BloodPressure Weight HeartAttack Smith 19 M 100 130.2 1 Bird 55 F 86 300 0 Wilson 23 M 200 212.7 0 "],
["analyzing-heart-attack-data-set-i.html", "Chapter 5 Analyzing heart attack data set I 5.1 Begin your analysis by examining each column separately 5.2 Possible correlation between two numeric columns? 5.3 Associations between categorical variables? 5.4 Associations between a categorical and a numeric variables? 5.5 Associations between multiple columns?", " Chapter 5 Analyzing heart attack data set I The heart attack data set (http://statland.org/R/RC/tables4R.htm), included in the ActivStats1 CD, contains all 12,844 cases of hospital discharges of the patients admitted for heart attack but did not have surgery in New York State in 1993. This information is essential for the interpretation of our results, as this is a purely observational study. It is not a random sample or controlled experiment. The data set is formatted as a table (Table 5.1) with rows representing cases and columns representing characteristics, which is a typical format for many datasets. If you download and open the file in NotePad++ or Excel, you can see that the columns are separated by tabs, and there are missing values noted by “NA”. Four columns (DIAGNOSIS, SEX, DRG, DIED) contain nominal values, representing labels of categories. See an excellent explanation of data types here. DIAGNOSIS column contains codes defined in the International Classification of Diseases (IDC), 9th Edition. This is the code that your doctor sends to your insurance company for billing. The numbers, such as 41041, actually code for which part of the heart was affected. Although these are numbers, it does not make any sense to add or subtract or compute the mean. If I have a room of 30 students each with student ids, such as 732324, it does not make any sense to compute the average of these numbers. Such categorical data needs to be recognized as factors in R. Similarly, DRG column has three possible numbers, 121 for survivors with cardiovascular complications, 122 for survivors without complications, and 123 for patients who died. Moreover, DIED also codes for prognosis, it is 1 if the patient passed away, and 0 if survived. heartatk4R &lt;- read.table(&quot;http://statland.org/R/RC/heartatk4R.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, colClasses = c(&quot;character&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;)) Table 5.1: First 15 rows of the heart attack dataset Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE 1 41041 F 122 0 4752.00 10 79 2 41041 F 122 0 3941.00 6 34 3 41091 F 122 0 3657.00 5 76 4 41081 F 122 0 1481.00 2 80 5 41091 M 122 0 1681.00 1 55 6 41091 M 121 0 6378.64 9 84 7 41091 F 121 0 10958.52 15 84 8 41091 F 121 0 16583.93 15 70 9 41041 M 121 0 4015.33 2 76 10 41041 F 123 1 1989.44 1 65 11 41041 F 121 0 7471.63 6 52 12 41091 M 121 0 3930.63 5 72 13 41091 F 122 0 NA 9 83 14 41091 F 122 0 4433.93 4 61 15 41041 M 122 0 3318.21 2 53 Take a look at this dataset in Excel, and consider these questions. What type of people are more likely to suffer from heart attacks? Which patient is more likely to survive a heart attack? Suppose you have a friend who was just admitted to hospital for heart attack. She is a 65 years old with DIAGNOSIS code of 41081. What is the odds that she survives without complication? Also, consider yourself as a CEO of an insurance company, and you want to know what types of patients incur more charges and whether a particular subgroup of people, such as men, or people over 70, should pay a higher premium. To answer these questions, we need to do is: • Import data files into R • Exploratory data analysis (EDA) • Statistical modeling (regression) x &lt;- heartatk4R # Make a copy of the data for manipulation, call it x. str(x) # structure of data object, data types for each column ## &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41001&quot;,&quot;41011&quot;,..: 5 5 9 8 9 9 9 9 5 5 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : num 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : num 79 34 76 80 55 84 84 70 76 65 ... 5.1 Begin your analysis by examining each column separately If you are single and meet someone in a bar, you typically start with small talks and learn some basic information about him/her. We should do the same thing with data. But too often, we go right to the business of building models or testing hypothesis without exploring our data. As a first step, we are going to examine each column separately. This can be very basic things such as mean, median, ranges, distributions, and normality. This is important because sometimes the data is so skewed or far from normal distribution that we need to use non-parametric tests, or transformate the raw data using log transformation, or more generally box-cox transformation, before conducting other analyses. Exercise 5.1 Perform the following analysis for the heartatk4R dataset. If you forgot the R commands, refer to our previous learning materials. You may also find these commands faster by asking Dr. Google. Graphical EDA: Plot distribution of charges using box plot, histogram, qqplot, lag plot, sequence plot. And interpret your results in PLAIN English. Note that there are missing values in this column that may cause some problems for some plots. You can remove missing values by defining a new variable by running temp = CHARGES [ ! is.na (CHARGES) ] and then run your plot on temp. Quantitative EDA: test of normality, and confidence interval. Note that if the Shapiro-Wilk normality test cannot handle the 12,000 data points, you can either try to find other tests in the nortest library or sample randomly by running temp = sample( CHARGES, 4000) You can attach your data set if you want to refer to the columns directly by name, such as LOS instead of x$LOS. attach(x) For categorical columns, we want to know how many different levels, and their frequencies. In addition to quantitative analysis, we also use various charts. For categorical values such as SEX and DIAGNOSIS, we can produce pie charts and bar plots, or percentages using the table( ) function followed by pie( ) and barplot(). barplot(table(DIAGNOSIS)) This generates a bar plot of counts. This basic plot could be further refined: counts &lt;- sort(table(DIAGNOSIS), decreasing = TRUE) # tabulate&amp;sort percentages &lt;- 100 * counts / length(DIAGNOSIS) # convert to % barplot(percentages, las = 3, ylab = &quot;Percentage&quot;, col = &quot;green&quot;) # Figure 4.1 Figure 5.1: Barplot by percentage. Note that the “las = 3”, changes the orientation of the labels to vertical. Try plot without it or set it to 2. Of course you can do all these in one line: barplot(100* sort(table(DIAGNOSIS), decreasing=T) / length(DIAGNOSIS), las = 3, ylab = “Percentage”, col = “green”). table(SEX) # tabulate the frequencies of M and F ## SEX ## F M ## 5065 7779 pie(table(SEX)) # pie chart Figure 5.2: Pie chart of patients by SEX. Exercise 5.2 Compute the counts and percentages of each levels of DRG. Use bar plot and pie charts similar to Figure 5.1 and Figure 5.2 to visualize. Briefly discuss your results. 5.2 Possible correlation between two numeric columns? This is done using various measures of correlation coefficients such as Pearson’s correlation coefficients (PPC), which is given by \\[r=∑_{i=1}^{n}(\\frac{x_i-\\overline{x}}{s_x}) (\\frac{y_i-\\overline{y}}{s_y})\\] where x i and y i are the ith values, \\(\\overline{x}\\) and \\(\\overline{y}\\) are sample means, and s x and s y are sample standard deviations. Note that Pearson’s correlation ranges from -1 to 1, with -1 indicating perfect negative correlation. Negative correlation is just as important and informative as positive ones. Figure 5.3: Interpretation of Pearson’s correlation coefficient. The numbers are Pearson’s correlation coefficient r. Figure 5.3 shows some examples of Pearson’s correlation with many scatter plots. The second row of figures shows examples with X-Y plots with different slopes but Pearson’s correlation are all 1. Pearson’s correlation only indicates degree of correlation, and is independent of slope. The figures in the 3rd row show that Pearson’s correlation coefficient’s limitation: it cannot detect nonlinear correlation. Table 5.2 below gives some guideline on how to interpret Pearson’s correlation coefficient. Table 5.2: Interpretation of correlation coefficient. Correlation Negative Positive - -0.09 to 0.0 0.0 to 0.09 Small -0.3 to -0.1 0.1 to 0.3 Medium -0.5 to -0.3 0.3 to 0.5 Large -1.0 to -0.5 0.5 to 1.0 There is a small, but statistically significant correlation between age and length of stay in the hospital after a heart attack. The plain English interpretation (read: a no-bullshit version that could be understood by your grandmother) is this: Older people tend to stay slightly longer in the hospital after a heart attack. cor.test(AGE, LOS) ## ## Pearson&#39;s product-moment correlation ## ## data: AGE and LOS ## t = 21.006, df = 12842, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1654881 0.1989282 ## sample estimates: ## cor ## 0.1822609 Note that the correlation coefficient r and the p value measure two different things. r indicates the size of effect, while p value tells us statistical significance. Based on the statistic sample, p value tells how certain we are about the difference being real, namely not due to random fluctuation. If we have a large sample, we could detect very small correlation with significance. Conversely, if we only have a few observations, a large r could have large p value, hence not significant. More generally, we need to distinguish effect size and significance in statistical analyses. Like many commonly-used parametric statistical methods which rely on means and standard deviations, the Pearson’s correlation coefficient is not robust, meaning its value are sensitive to outliers and can be misleading. It is also very sensitive to distribution. Non-parametric approaches typically rank original data and do calculations on the ranks instead of raw data. They are often more robust. The only drawback might be loss of sensitivity. There are corresponding non-parametric versions for most of the parametric tests. Spearman’s rank correlation coefficient ρ is a non-parametric measure of correlation. The Spearman correlation coefficient ρ is often thought of as being the Pearson correlation coefficient between the ranked variables. In practice, however, a simpler procedure is normally used to calculate ρ. The n raw scores Xi, Yi are converted to ranks xi, yi, and the differences di = xi − yi between the ranks of each observation on the two variables are calculated. If there are no tied ranks, then ρ is given by:\\[ρ=1-\\frac{6∑d_{i}^{2}}{n(n_{}^{2}-1)}\\] In R, we can calculate Spearman’s ρ and test its significance but customize the cor.test() function: cor.test(AGE, LOS, method = &quot;spearman&quot;) ## Warning in cor.test.default(AGE, LOS, method = &quot;spearman&quot;): Cannot compute ## exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: AGE and LOS ## S = 2.9448e+11, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.1661032 Interpretation of Spearman’s ρ is similar to Pearson’s r. The statistical significance can also be determined similarly as demonstrated above. Alternative non-parametric statistic for correlation is Kendall tau rank correlation coefficient. We already know that we could use scatter plots to visualize correlation between two numeric columns. But when there are many data points, in this case we have over 12,000, it could be hard to comprehend. This is especially the case, when the data is integers and there are a lot of data points overlap on top of each other. Yes, graphics can be misleading. plot(AGE, LOS) # standard scatter plot smoothScatter(AGE, LOS) # a smoothed color density representation of a scatterplot Figure 5.4: Smoothed Scatter plots use colors to code for the density of data points. This is useful when there are overlapping points. Exercise 5.3 Investigate the correlation between length of stay and charges. Try both parametric and non-parametric methods to quantify correlation and use graphs. Remember to include plain English interpretation of your results even your grandpa can understand. 5.3 Associations between categorical variables? There are four columns in the heart attack data set that contain categorical values (DIAGNOSIS, DRG, SEX, and DIED). These columns could be associated with each other. For example, there is a correlation between SEX and DIED. Are men and women equally likely to survive a heart attack? counts &lt;- table(SEX, DIED) # tabulates SEX and DIED and generate counts in a 2d array. counts Table 5.3: A 2x2 contingency table summarizing the distribution of DIED frequency by SEX. Sex DIED_0 DIED_1 F 4298 767 M 7136 643 We got a contingency table as shown in Table 5.3. To convert into percentages of survived, we can do: counts / rowSums(counts) ## DIED ## SEX 0 1 ## F 0.84856861 0.15143139 ## M 0.91734156 0.08265844 We can see that 15.1% of females died in the hospital, much higher than the 8.26% for male patients. This gender difference is quite a surprise to me. But could this happen just by chance? To answer this question, we need a statistical test. Chi-square test for the correlation of two categorical variables. The null hypothesis is that men and women are equally likely to die from a heart attack. chisq.test(counts) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: counts ## X-squared = 147.76, df = 1, p-value &lt; 2.2e-16 You have seen this p-value before? Probably! It is the smallest non-zero number R shows for lots of tests. However, p is definitely small! Hence we reject the hypothesis that the mortality rate is the same for men and women. Looking at the data, it is higher for women. The chi-square test for a 2x2 contingency table gives accurate p-values provided that the number of expected observation is greater than 5. If this is not true, then you should use the Fisher Exact test. The chi-square test is an approximation to the Fisher Exact test. The Fisher Exact test is computationally intensive; Karl Pearson developed the chi-square approximation before we had computers to do the work. With fast computers available today, you can use the Fisher Exact test for quite large data sets, and be more confident in the p-values. You can use the chi-square test for contingency tables that have more than two rows or two columns. For contingency tables that have more than two rows or two columns, the p-value computed by the chi square approximation is reasonably accurate provided that the expected number of observations in every cell is greater than 1, and that no more than 20 percent of the cells have an expected number of observations less than 5. Again, the Fisher Exact test can handle quite large data sets with today’s computers, and avoid the problems with chi-square test. fisher.test(counts) # Fisher’s Exact test ## ## Fisher&#39;s Exact Test for Count Data ## ## data: counts ## p-value &lt; 2.2e-16 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.4509331 0.5653197 ## sample estimates: ## odds ratio ## 0.5049435 In this case, the result of fisher’s test is the same as chi-square test. If you want to make your point to a boss who is either stupid or too busy, you need a chart. Below we show two barplots, one stacked and one side by side. counts &lt;- table(DIED, SEX) # SEX define columns now, as I want the bars to represent M or F. barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = &quot;DIED&quot;, args.legend = list(x = &quot;topleft&quot;)) # Figure 4.5A barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = &quot;DIED&quot;, beside = T) # Figure 4.5B Figure 5.5: Barplot showing the correlation of two categorical variables. A. Stacked. B. Side by side. Another way of showing the proportions is mosaic plot. mosaicplot(table(SEX, DIED), color = T) # Figure 4.6 Figure 5.6: Mosaic plot of DIED by SEX. The mosaic plot in Figure 5.6 is similar to the barplot in Figure 5.5, but the bars are stretched to the same height, the width is defined by proportion of Male vs. Female. The size of the four blocks in the figure represents the counts of the corresponding combination. Also note that the blocks are also color-coded for different combination. Horizontally, the blocks are divided by SEX, we could observe that there are more men in this dataset than women. Vertically, the blocks are divided by DIED (1 for died in hospital). We could conclude that regardless of gender, only a small proportion of patients died in hospital. Between men and women, we also see that the percentage of women that died in hospital is higher than that in men. This is a rather unusual. We could use mosaic plots for multiple factors. mosaicplot(table(SEX, DIED, DRG), color = rainbow(3)) # Figure 4.7 Figure 5.7: Mosaic plot of three factors. Here we nested the tabulate command inside the mosaic plot. As shown in Figure 5.7, we further divided each of the 4 quadrants of Figure 5.6 into three parts according to DRG codes, in red, green and blue. One thing we could tell is that a smaller proportion of surviving males developed complications, compared with females. Activity: interpret the mosaic plot of the Titanic dataset(built-in in R). ? Titanic # this leads you to information about the famous Titanic dataset. mosaicplot(~ Sex + Age + Survived, data = Titanic, color = rainbow(2)) This is a mosaic plot of the whole Titanic dataset mosaicplot(Titanic, color = rainbow(2)) Did men and women survived by equal proportion? Did girls and women survived by equal proportion? Exercise 5.4 The DIAGNOSIS column contains IDC codes that specifies the part of the heart that are affected. Are men and women equal in their frequencies of diagnoses? Use a stacked bar plot and a mosaic plot to compare the difference in frequency of DIAGNOSIS between men and women. Hint: Use table to generate the counts and then visualize. 5.4 Associations between a categorical and a numeric variables? Do women stay longer in the hospital? Does the charges differ for people with different diagnosis (part of the heart affected)? We should know by now how to answer these questions with T-test, and more generally ANOVA, following our examples commands used in our analysis of the Iris flower data set. For data visualization, boxplot is the most straight forward way. But beyond boxplot, we can use the ggplot2 package for more detailed examination of distribution of variables in two or more groups. library(ggplot2) ggplot(heartatk4R, aes(x = AGE, group = SEX, y = c(..count..[..group.. == 1]/sum(..count..[..group.. == 1]), ..count..[..group.. == 2]/sum(..count..[..group.. == 2])) * 100)) + geom_histogram(binwidth = 6, colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + facet_grid(SEX ~ .) + labs(y = &quot;Percent of Total&quot;) Figure 5.8: Histogram of AGE by SEX using ggplot2 package Now the two histograms are arranged, and it is very easy to see that women’s age are more skewed to the right, meaning women are considerably older than men. I am surprised at first by this huge difference, as the average age of women if bigger by 11. Further research shows that for women the symptoms of heart attacks are milder and often go unnoticed. We can further divide the population according to survival status by adding another factor: library(dplyr) heartatk4R %&gt;% mutate(GROUP = paste(DIED, SEX, sep = &quot;-&quot;)) %&gt;% ggplot(aes(x = AGE, group = GROUP, y = c(..count..[..group.. == 1]/sum(..count..[..group.. == 1]), ..count..[..group.. == 2]/sum(..count..[..group.. == 2]), ..count..[..group.. == 3]/sum(..count..[..group.. == 3]), ..count..[..group.. == 4]/sum(..count..[..group.. == 4])) * 100))+ geom_histogram(binwidth = 5, colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + facet_grid(DIED ~ SEX) + labs(y = &quot;Percent of Total&quot;) Figure 5.9: Histogram of AGE by SEX and DIED using ggplot2 package detach(&quot;package:dplyr&quot;) We can see that patients who did not survive heart attack tend to be older, for both men and women. This is perhaps better illustrated with density plot: ggplot(heartatk4R, aes(x = AGE, fill = SEX)) + geom_density(alpha = .3) Figure 5.10: Density plot of AGE by SEX using ggplot2 package The result is similar to Figure 5.8, but as a density plot. Now for each gender, we further divide the patients by their survival status. Instead of splitting into multiple panels, the curves are overlaid. ggplot(heartatk4R, aes(x = AGE, fill = DIED)) + geom_density(alpha = .3) + facet_grid(SEX ~ .) Figure 5.11: Density plot of AGE by SEX and DIED using ggplot2 package Exercise 5.5 Use the ggplot2 package to compare the distribution of lengths of stay among patients who survived and those who did not. Use both histograms and density plot. Interpret your results. Exercise 5.6 Use the ggplot2 package to compare the distribution of lengths of stay among patients who survived and those who did not, but compare men and women separately (similar to Figure 5.11). Exercise 5.7 Use student’s t-test, boxplot, histogram and density plots to compare the age distribution between survived and those who didn’t. Exercise 5.8 Use ANOVA, boxplot, and histogram and density plots to compare the charges among people who have different DRG codes. 5.5 Associations between multiple columns? We can use the ggplot2 package to investigate correlations among multiple columns by figures with multiple panels. ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_boxplot(color = &quot;blue&quot;) + facet_grid(SEX ~ .) Figure 5.12: Multiple boxplots of AGE by DRG and SEX using ggplot2 package Recall that 121 indicate survivors with complication, 122 survivors with no complication, and 123, died. As you could see this clearly indicate our previous observation people who died in hospital are older than survivors and that patients who developed complications seems to be older than those that did not. Did people with complications stayed longer in the hospital? Exercise 5.9 Are the surviving women younger than the women who died? Similar question can be asked for men. Produce a figure that compares, in a gender-specific way, age distribution between patients who died in the hospital and those who survived. Exercise 5.10 Use the ggplot2 package to produce boxplots to compare the length of stage of men vs. women for each of the DRG categories indicating complication status. You should produce a plot similar to Figure 5.13. Offer interpretation. Figure 5.13: Multiple boxplots using ggplot2 package All of these techniques we introduced so far enable us to LEARN about your dataset without any of priori hypothesis, ideas, and judgments. Many companies claim that they want to know their customers first as individuals and then do business. Same thing applies to data mining. You need to know you dataset as it is before making predictions, classifications etc. You should also INTERACT with your data by asking questions based on domain knowledge and common sense. Generates lots and lots of plots to support or reject hypothesis you may have. I demonstrated this by using the heart attack dataset in the last few pages. You should do the same thing when you have a new dataset. Sometimes, the thing that you discovered is more important than the initial objectives. # scatterplot of LOS vs. AGE ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() # scatterplot of LOS vs. AGE, divided by SEX ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() + facet_grid(SEX ~ .) # scatterplot colored by DIED ggplot(heartatk4R, aes(x = AGE, y = LOS, color = DIED)) + geom_point() + facet_grid(SEX ~ .) Note that ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() + facet_grid(SEX ~ .) generates multiple scatterplots of LOS ~ AGE according to different values of SEX, while color = DIED will add these two color-coded scatter plots into the same figure. Figure 5.14: A scatter plot of LOS vs. AGE, using SEX and DIED as factors. Figure 5.14 seems to suggest that the positive association between AGE and LOS is noticeable in patients who did not die in hospital, regardless of sex. This is a statistician’s language. Try this instead that could be understood by both the statistician and his/her grandmother. Older patients tend to stay longer in the hospital after surviving a heart attack. This is true for both men and women. Another way to visualize complex correlation is bubble plot. Bubble plot is an extension of scatter plot. It uses an additional dimension of data to determine the size of the symbols. Interesting video using bubble plot: http://youtu.be/jbkSRLYSojo y &lt;- x[sample(1:12844, 200), ] # randomly sample 200 patients plot(y$AGE, y$LOS, cex = y$CHARGES / 6000, col = rainbow(2)[y$SEX], xlab = &quot;AGE&quot;, ylab = &quot;LOS&quot;) legend(&quot;topleft&quot;, levels(y$SEX), col = rainbow(2), pch = 1) Figure 5.15: Bubble plot example. Figure 5.15 is a busy plot. Female patients are shown in red while males in blue. Size of the plot is proportional to charges. So on this plot we are visualizing 4 columns of data! Other common methods we can use to detect complex correlations and structures include principal component analysis (PCA), Multidimensional scaling (MDS), hierarchical clustering etc. Velleman, P. F.; Data Description Inc. ActivStats, 2000-2001 release.; A.W. Longman,: Glenview, IL, 2001. "],
["analyzing-heart-attack-data-set-ii.html", "Chapter 6 Analyzing heart attack data set II 6.1 Scatter plot in ggplot2 6.2 Histograms and density plots 6.3 Box plots and Violin plots 6.4 Bar plot with error bars 6.5 Statistical models are easy; interpretations and verifications are not!", " Chapter 6 Analyzing heart attack data set II We continue to investigate the heart attack dataset. First, let’s read the data in again and change several variables to factors. Note that you must set the working directory to where the data file is stored on your computer. I encourage you to define a project associated with a folder. # set working directory, not necessary if loading a project #setwd(&quot;C:/Ge working/RBook/learnR/datasets&quot;) df &lt;- read.table(&quot;heartatk4R.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) # read data df$DRG &lt;- as.factor(df$DRG) # convert DRG variable to factor df$DIED &lt;- as.factor(df$DIED) df$DIAGNOSIS &lt;- as.factor(df$DIAGNOSIS) df$SEX &lt;- as.factor(df$SEX) str(df) # double check ## &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : int 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41001&quot;,&quot;41011&quot;,..: 5 5 9 8 9 9 9 9 5 5 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : int 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : int 79 34 76 80 55 84 84 70 76 65 ... 6.1 Scatter plot in ggplot2 Hadley Wickham wrote the ggplot2 package in 2005 following Leland Wilkinson’s grammar of graphics, which provides a formal, structured way of visualizing data. Similarly, R was originally written by Robert Gentleman and Ross Ihaka in the 1990s; Linux was developed by Linus Torvalds, a student, at the same period. A few superheroes can make computing much easier for millions of people. ggplot2 uses a different approach to graphics. #install.packages(&quot;ggplot2&quot;) # install the package library(ggplot2) # load the package ggplot(df, aes(x = LOS, y = CHARGES)) # Specify data frame and aesthetic mapping This line does not finish the plot; it just specifies the name of the data frame, which is the required input data type, and defines the so-called aesthetic mapping: LOS maps to x-axis while CHARGES maps to the y-axis. To complete the plot, we use the geom_point function to add data points, which is called geometric objects. ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() # scatter plot Thus, it is a two-step process to generate a scatter plot. This seems cumbersome at first, but it is convenient to add additional features or customize the plot step by step. Let’s add a trend line with a confidence interval. ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() + stat_smooth(method = lm) As we keep adding elements to the plot, this line of code gets longer. So we break the code into multiple lines as below. The “+” at the end of the lines signifies that the plot is not finished and tell R to keep reading the next line. This code below does the same thing as the above, but it is easier to read. I often use the tab key in the second line to remind myself that it is continued from the above line. ggplot(df, aes(x = LOS, y = CHARGES)) + # aesthetic mapping geom_point() + # add data points stat_smooth(method = lm) # add trend line As you can see from this code, it also enables us to add comments for each step, making the code easy to read. This is important, as we often recycle our codes. We can also customize the plot by adding additional lines of code (Figure 6.1): ggplot(df, aes(x = LOS, y = CHARGES)) + # aesthetic mapping geom_point() + # add data points stat_smooth(method = lm) + # add trend line xlim(0, 25) + # change plotting limits of x-axis labs(x = &quot;Length of stay&quot;, y = &quot;Charges ($)&quot;) + # change x and y labels annotate(&quot;text&quot;, x = 3, y = 45000, label = (&quot;R = 0.74&quot;)) # add text to plot coordinates Figure 6.1: Scatter plot using ggplot2. You can learn other ways to customize your plot by googling. For example, try to find a way to add title to the plot by using keyword “ggplot2 add title to plot”. It is easy to represent other characteristics of data points (columns) using additional aesthetic mappings, such as linetype, color, size, fill (“inside” color). ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() # basic scatter plot ggplot(df, aes(x = LOS, y = CHARGES, color = DRG)) + geom_point() # map DRG to color ggplot(df, aes(x = LOS, y = CHARGES, color = DRG, shape = SEX)) + # map SEX to shape geom_point() Figure 6.2: Changing color and shape to represent multivariate data in ggplot2. At each step, we add additional information about these patients (Figure 6.2). With ggplot2, we can visualize complex, multivariate data by mapping different columns into diverse types of aesthetics. Figure 6.1 shows a strong positive correlation between charges and length of hospital stay, which is expected as many itemized costs are billed daily. Note there are over 12,000 data points on these plots, many are plotted near or exactly at the same places, especially at the lower left corner. stat_density2d() can be used to color code density. These plots are big in file size when you save it in a vector format (metafile), which often offer higher resolution. If you have many such plots in a paper or thesis, your file size can be big. These problems can be avoided if you use the bitmap format. Exercise 6.1 Use ggplot2 to generate a scatter plot of age vs. charges in the heart attack dataset. Use different shapes of data points to represent DRG and color-code data points based on diagnosis. 6.2 Histograms and density plots In addition to data points(geom_points), there are many other geometric objects, such as histogram(geom_histogram), lines (geom_line), and bars (geom_bar), and so on. We can use these geometric objects to generate different types of plots. To plot a basic histogram: ggplot(df, aes(x = AGE)) + geom_histogram() Then we can refine it and add a density curve (Figure 6.3): ggplot(df, aes(x = AGE, y = ..density..)) + geom_histogram(fill = &quot;cornsilk&quot;, colour = &quot;grey60&quot;, size = .2) + geom_density() Figure 6.3: Histogram with density curve. Combined density plots are useful in comparing the distribution of subgroups of data points. ggplot(df, aes(x = AGE)) + geom_density(alpha = 0.3) # all data Use color to differentiate sex: ggplot(df, aes(x = AGE, color = SEX)) + geom_density(alpha = 0.3) # Figure 5.4A Here, we split the dataset into two portions, men and women, and plotted their density distribution on the same plot. Figure 6.4A shows that age distribution is very different between men and women. Women’s distribution is skewed to the right and they are on average over ten years older than men. This is surprising, given that this dataset contains all heart attack admissions in New York state in 1993. The fill mapping changes the “inside” color: ggplot(df, aes(x = AGE, fill = SEX)) + geom_density(alpha = 0.3) # Figure 5.4B This plot shows the same information. It looks nicer, at least to me. We can split the plot into multiple facets: ggplot(df, aes(x = AGE, fill = SEX)) + geom_density(alpha = 0.3) + facet_grid(DRG ~.) # Figure 5.4C Figure 6.4: Density plots. Recall the DRG=121 represents patients who survived but developed complications; DRG=122 denotes those without complications and DRG=123 are patients who did not survive. Figure 6.4C suggests that women in all three groups are older than men counterparts. If we examine the distribution of male patients’ age distribution across facets, we can see that the distribution is skewed to the right in deceased patients (DRG=123), indicating that perhaps older people are less likely to survive a heart attack. Survivors without complications (DRG= 122) tend to be younger than survivors with complications. Exercise 6.2 Create density plots like Figure 6.5 to compare the distribution of length of hospital stay (LOS) for patients with different DRG groups, separately for men and women. Offer interpretation in the context of the dataset. Limit the x-axis to 0 to 20. Figure 6.5: Distribution of LOS by DRG groups grouped by SEX. 6.3 Box plots and Violin plots We can follow the same rule to generate boxplots using the geom_boxplot ( ). Let’s start with a basic version. ggplot(df, aes(x = SEX, y = AGE)) + geom_boxplot() # basic boxplot ggplot(df, aes(x = SEX, y = AGE)) + geom_boxplot() + facet_grid(DRG ~ .) # Figure 5.6A ggplot(df, aes(x = SEX, y = AGE)) + geom_violin() + facet_grid(DRG ~ .) # Figure 5.6B The last version is a violin plot. It shows more details about the distribution as it is essentially density plots on both left and right sides of the violins. Exercise 6.3 Generate a violin plot like Figure 6.6C to compare the distribution of length of hospital stay among patients with different prognosis outcomes (DRG), separately for men and women. Interpret your result. Note the axes labels are customized. Figure 6.6: Boxplot and violin plots using ggplot2. 6.4 Bar plot with error bars Suppose we are interested in examining whether people with certain diagnosis codes stays longer or shorter in the hospital after a heart attack. We can, of course, use the aggregate function to generate a table with means LOS by category. aggregate(df, by = list(df$DIAGNOSIS), FUN = mean, na.rm = TRUE) ## Group.1 Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE ## 1 41001 7130.229 NA NA NA NA 10868.030 7.775161 65.56317 ## 2 41011 6413.323 NA NA NA NA 10631.803 7.728618 65.81305 ## 3 41021 6482.808 NA NA NA NA 10666.687 7.556000 63.96000 ## 4 41031 6356.028 NA NA NA NA 9908.118 6.679715 62.93950 ## 5 41041 6259.752 NA NA NA NA 9985.722 7.307692 63.73884 ## 6 41051 6438.974 NA NA NA NA 9062.423 7.298701 67.22078 ## 7 41071 6949.627 NA NA NA NA 9750.309 7.834997 69.01996 ## 8 41081 8384.725 NA NA NA NA 9633.123 7.048780 68.04181 ## 9 41091 6165.481 NA NA NA NA 9511.093 7.625743 67.10359 We can be happy with this table and call it quits. However, tables are often not as easy to interpret as a nicely formatted graph. Instead of the aggregate function, we use the powerful dplyr package to summarize the data and then to generate a bar plot showing both the means and standard errors. I stole some code and ideas from R graphics cookbook and this website: http://environmentalcomputing.net/plotting-with-ggplot-bar-plots-with-error-bars/ #install.packages(&quot;dplyr&quot;) # dplyr package for summary statistics by group library(dplyr) # load the package To summarize data by groups/factors, the dplyr package uses a similar type of grammar like ggplot2, where operations are added sequentially. Similar to pipes in Unix, commands separated by “%&gt;%” are executed sequentially where the output of one step becomes the input of the next. The follow 6 lines are part of one command, consisting of three big steps. stats &lt;- df %&gt;% # names of the new data frame and the data frame to be summarized group_by(DIAGNOSIS) %&gt;% # grouping variable summarise(mean = mean(LOS), # mean of each group sd = sd(LOS), # standard deviation of each group n = n(), # sample size per group se = sd(LOS) / sqrt(n())) # standard error of each group The resultant data frame is a detailed summary of the data by DIAGNOSIS: stats ## # A tibble: 9 x 5 ## DIAGNOSIS mean sd n se ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 41001 7.78 5.69 467 0.263 ## 2 41011 7.73 5.34 1824 0.125 ## 3 41021 7.56 5.26 250 0.333 ## 4 41031 6.68 4.15 281 0.247 ## 5 41041 7.31 4.75 2665 0.0920 ## 6 41051 7.30 4.54 154 0.366 ## 7 41071 7.83 5.28 1703 0.128 ## 8 41081 7.05 5.32 287 0.314 ## 9 41091 7.63 5.14 5213 0.0712 Now we can use ggplot2 to plot these statistics (Figure 6.7): ggplot(stats, aes(x = DIAGNOSIS, y = mean)) + # data &amp; aesthetic mapping geom_bar(stat = &quot;identity&quot;) + # bars represent average geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) # error bars Figure 6.7: Average LOS by DIAGNOSIS group with error bars representing standard error. Note that the mean-se and mean+se refer to the two columns in the stats data frame and defines the error bars. Because we have an extremely large sample size, the standard errors are small. People with a diagnosis of 41031 stays shorter in the hospital. By looking up the IDC code in http://www.nuemd.com, we notice that this code represents “Acute myocardial infarction of inferoposterior wall, initial episode of care”, which probably makes sense. As we did previously, we want to define the LOS bars for men and women separately. We first need to go back and generate different summary statistics. stats2 &lt;- df %&gt;% group_by(DIAGNOSIS, SEX) %&gt;% # two grouping variables summarise(mean = mean(LOS), sd = sd(LOS), n = n(), se = sd(LOS) / sqrt(n())) The entire dataset was divided into 18 groups according to all possible combinations of DIAGNOSIS and SEX. For each group, the LOS numbers are summarized in terms of mean, standard deviation (sd), observations (n), and standard errors(se). Below is the resultant data frame with summary statistics: stats2 ## # A tibble: 18 x 6 ## # Groups: DIAGNOSIS [9] ## DIAGNOSIS SEX mean sd n se ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 41001 F 8.74 6.03 175 0.456 ## 2 41001 M 7.20 5.41 292 0.316 ## 3 41011 F 8.41 6.23 692 0.237 ## 4 41011 M 7.31 4.66 1132 0.139 ## 5 41021 F 8.54 5.82 100 0.582 ## 6 41021 M 6.9 4.76 150 0.389 ## 7 41031 F 6.41 4.67 96 0.477 ## 8 41031 M 6.82 3.86 185 0.283 ## 9 41041 F 7.79 5.18 998 0.164 ## 10 41041 M 7.02 4.45 1667 0.109 ## 11 41051 F 7.93 5.64 69 0.679 ## 12 41051 M 6.79 3.34 85 0.362 ## 13 41071 F 8.65 5.76 727 0.214 ## 14 41071 M 7.23 4.80 976 0.154 ## 15 41081 F 7.58 5.60 130 0.492 ## 16 41081 M 6.61 5.04 157 0.402 ## 17 41091 F 8.34 5.76 2078 0.126 ## 18 41091 M 7.15 4.63 3135 0.0826 Now we are ready to generate bar plot for men and women separately using the fill aesthetic mapping: ggplot(stats2, aes(x = DIAGNOSIS, y = mean, fill = SEX)) + # mapping geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + # bars for mean of LOS labs(x = &quot;Diagnosis codes&quot;, y = &quot;Length of Stay&quot;) + # axes labels geom_errorbar(aes(ymin = mean - se, ymax = mean + se), # error bars position = position_dodge(.9), width = 0.2) Figure 6.8: The length of stay summarized by diagnosis and sex. Error bars represent standard error. Figure 6.9: Mean ages for patients with different diagnosis and treatment outcome. Exercise 6.4 Generate Figure 6.9 and offer your interpretation. Hint: Modify both the summarizing script and the plotting script. Overall, our investigation reveals that women are much older than men when admitted to hospital for heart attack. Therefore, prognosis is also poor with high mortality and complication rates. It is probably not because they develop heart attack later in life. It seems that symptoms are subtler in women and often get ignored. Heart attack can sometimes manifest as back pain, numbness in the arm, or pain in the jaw or teeth! (Warning: statistics professor is talking about medicine!) As you could see from these plots, ggplot2 generates nicely-looking, publication-ready graphics. Moreover, it is a relatively structured way of customizing plots. That is why it is becoming popular among R users. Once again, there are many example codes and answered R coding questions online. Whatever you want to do, you can google it, try the example code, and modify it to fit your needs. It is all free! Enjoy coding! 6.5 Statistical models are easy; interpretations and verifications are not! By using pair-wised correlation analysis, we found that women have a much higher mortality rate than men due to heart attack. We also found that women are much older than men. These are obviously confounding effects. We need to delineate the effects of multiple factors using multiple linear regression. In our model below, we express the charges as a function of all other factors using multiple linear regression. fit &lt;- lm(CHARGES ~ SEX + LOS + AGE + DRG + DIAGNOSIS, data = df) summary(fit) ## ## Call: ## lm(formula = CHARGES ~ SEX + LOS + AGE + DRG + DIAGNOSIS, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31486 -2453 -674 1766 32979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6863.753 314.515 21.823 &lt; 2e-16 *** ## SEXM 183.085 84.131 2.176 0.02956 * ## LOS 989.717 8.268 119.700 &lt; 2e-16 *** ## AGE -55.255 3.216 -17.180 &lt; 2e-16 *** ## DRG122 -916.531 86.851 -10.553 &lt; 2e-16 *** ## DRG123 1488.187 141.771 10.497 &lt; 2e-16 *** ## DIAGNOSIS41011 -136.097 230.312 -0.591 0.55458 ## DIAGNOSIS41021 137.118 349.948 0.392 0.69520 ## DIAGNOSIS41031 201.021 334.056 0.602 0.54735 ## DIAGNOSIS41041 -349.932 223.352 -1.567 0.11720 ## DIAGNOSIS41051 -1162.651 408.621 -2.845 0.00444 ** ## DIAGNOSIS41071 -755.549 233.214 -3.240 0.00120 ** ## DIAGNOSIS41081 -353.290 332.747 -1.062 0.28838 ## DIAGNOSIS41091 -1042.968 215.038 -4.850 1.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4308 on 12131 degrees of freedom ## (699 observations deleted due to missingness) ## Multiple R-squared: 0.569, Adjusted R-squared: 0.5685 ## F-statistic: 1232 on 13 and 12131 DF, p-value: &lt; 2.2e-16 As we could see from above, one levels for factors (such as female) is used as a base for comparison, so it does not show. SEXM indicates that being male, we have a marginally significant effect on charges. Everything else being equal, a male patient will incur $183 dollars more cost than female for the hospital stay. This is really small number compared to overall charges and also the p value is just marginally significant for this large sample. This is in contrast to the t.test(CHARGES ~ SEX), when we got the opposite conclusion. This is because we did not control for other factors. The most pronounced effect is LOS. This is not surprising, as many hospitals have charges on daily basis. Since the p values are cutoff at 2e-16, the t value is an indicator of significance. LOS has a t value of 119.7, which is way bigger than all others. If I am an insurance company CEO, I will do anything I can to push the hospitals to discharge patients as early as possible. The coefficients tell us for the average patient, one extra day of stay in the hospital the charges will go up by $989.7. Age has a negative effect, meaning older people actually will be charged less, when other factors are controlled. So there is no reason to charge older people more in terms of insurance premiums. And there is a little evidence to charge female more than males. Compared with patients who had complications (DRG = 121, the baseline), people have no complications (DRG = 122) incurred less charges on average by the amount of $916. Patients who died, on the other hand, are likely to be charged more. People with diagnosis codes 41091, 41051 and 41071 incurred less charges compared with those with 41001. To investigate mortality, which is a binary outcome, we use logistic regression. fit &lt;- glm(DIED ~ SEX + LOS + AGE + DIAGNOSIS, family = binomial( ), data = df) summary(fit) ## ## Call: ## glm(formula = DIED ~ SEX + LOS + AGE + DIAGNOSIS, family = binomial(), ## data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7745 -0.4583 -0.2813 -0.1517 4.4996 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.359769 0.263001 -20.379 &lt; 2e-16 *** ## SEXM -0.345084 0.064978 -5.311 1.09e-07 *** ## LOS -0.248553 0.009197 -27.025 &lt; 2e-16 *** ## AGE 0.081122 0.002995 27.082 &lt; 2e-16 *** ## DIAGNOSIS41011 -0.404406 0.157454 -2.568 0.01022 * ## DIAGNOSIS41021 -0.336452 0.254026 -1.324 0.18534 ## DIAGNOSIS41031 -0.784819 0.262089 -2.994 0.00275 ** ## DIAGNOSIS41041 -0.991677 0.158059 -6.274 3.52e-10 *** ## DIAGNOSIS41051 -0.235784 0.279752 -0.843 0.39932 ## DIAGNOSIS41071 -1.719416 0.178522 -9.631 &lt; 2e-16 *** ## DIAGNOSIS41081 -0.484542 0.226713 -2.137 0.03258 * ## DIAGNOSIS41091 -0.677723 0.145463 -4.659 3.18e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8889.4 on 12843 degrees of freedom ## Residual deviance: 6844.2 on 12832 degrees of freedom ## AIC: 6868.2 ## ## Number of Fisher Scoring iterations: 6 Note that DIED is 1 for people died, and 0 otherwise. So a negative coefficient indicates less likely to die, hence more likely to survive. Males are more likely to survive heart attack, compared with females of the same age, and with the same diagnosis. People who stayed longer are less likely to die. Older people are more likely to die. Compared with people with diagnosis code of 41001, those diagnosed with 41071, 41041, and 41091, are more likely to survive. Exercise 6.5 Use multiple linear regression to investigate the factors associated with length of stay. Obviously we need to exclude charges in our model. Interpret your results. This is not part of the exercise, but another problem we could look into is who are more likely to have complications. We should only focuse on the surviving patients and do logistic regression. We still need pair-wised examinations, because we cannot put two highly correlated factors in the same model. "],
["advanced-topics.html", "Chapter 7 Advanced topics 7.1 Introduction to R Markdown 7.2 Tidyverse 7.3 Interactive plots made easy with Plotly 7.4 Shiny Apps", " Chapter 7 Advanced topics 7.1 Introduction to R Markdown Let’s start learning R Markdown from its cheat sheet. Click the RStudio IDE under Help \\(\\rightarrow\\) Cheatsheets \\(\\rightarrow\\) R Markdown Cheat Sheet. As the cheet sheet explained, the R Markdown (.Rmd) file can combine your code and the output in one file, and produce various formats, such as html, pdf, MS Word, or RTF documents. Figure 7.1: R Markdown Cheat Sheet To create a R Markdown file by the following process. Firstly, you can open a new .Rmd file at File \\(\\rightarrow\\) New File \\(\\rightarrow\\). In the open window, feel free to change the Title. For example, you can change the Untitled to MyFirstRmd. Then choose the default output format from the three options: HTML, PDF or WORD. Then click “OK”. Figure 7.2: Create a new RMarkdown file A new .Rmd file named MyFirstRmd.Rmd is created. The first part of the file is the YAML (Yet Another Markup Language) header. The YAML header is surrounded by dashes “-”. See the Figure 7.3. It specifies the key arguments for the document, such as title, author, date, and output format. The output format is html in the example. You may see the output: pdf_document, which means that the default format you have chosen is PDF when you created the file. Figure 7.3: YAML Header The second part of a .Rmd file is surrounded by three back ticks “`”. They are called chunks of R. Figure 7.4 is an example of a chunk. The “nm_cars” after r in the { } is the name of the chunk. Figure 7.4: Chunk of R A chunk is a collection of r code. You can run each chunk by clicking the right arrow on the top-right of the chunk or by pressing the shortcut Ctrl+Shift+Enter. Then R executes the code and displays the outputs inline with the code. The output of chunk in Figure 7.4 is shown below. ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 You can find manys options for chunks in the R Markdown Cheet Sheet. let’s take a look at some of the chunk options that you’ll use frequently. 1, eval = FALSE stops code from being evaluated. See the chunk below which displays the code only. The code won’t be evaluated and therefore does not have any results in the final document. 2, include = FALSE runs the code, but does not display the code or results in the final document. You can use the 3, echo = FALSE hides the code from the final document, and only presents the output obtained from the chunk. Another chunk below displays only the plot in the final document. The code does not show up in the final document since the echo = FALSE parameter was added to the code chunk. 4, message = FALSE prevents messages from apprearing in the final document. 5, warning = FALSE prevents warnings from showing up in the final document. 6, results = ‘hide’ hides printed output. 7, fig.show = ‘hide’ hides plots. 8, error = TRUE allows the render to continue even if code returns an error. The third part of a .Rmd file is the text mixed with symbols like # or *. You can type your text like how you do in MS Word files. The pound symbol # implies a heading. The number of pounders indicate the heading level, it runs from 1 to 6. One # presents the first level header, two #s implies the second level header, and etc. Here is an output of various level headers. The symbol * arounds a word before and after will give an italic text format. If two *s around a word before and after will bold the word. Such as, *Italic* and **bold** will be Italic and bold in the output file. The last step is to obtain the output by clicking the Knit button on the top of the .Rmd file. You will be asked to name the file and save it to your PC if it is the first time you knit the file. Do as instructed, then an output file will be opened automatically. Exercise 7.1 Do all the following Homework by using R Markdown. You should submit your .Rmd file and the final document with pdf format. (You do not submit any answer key for this exercise. ) 7.2 Tidyverse Tidyverse is collection of powerful R packages. The packages include ggplot2, dplyr, readr, purr, tidyr, and tibble. They were all written by Hadley Wickham, a true hero in the open-source R world. Following the same design philosophy and grammar, these powerful packages are designed to make R code easier to read. As they are more intuitive, some people argue that beginners should start by learning them, instead of the base R. Now Let’s explain the dplyr package in a little detail for manipulating iris data set. install.packages(&quot;dplyr&quot;) library(dplyr) In dplyr, we use the pipe operator %&gt;% to send data to the next stage. This is similar to the “+” operator we used in ggplot2. To create a new data frame for setosa with sepals longer than 4.0: iris %&gt;% filter(Species == &quot;setosa&quot;, Sepal.Length &gt; 4) Add a new column that contains the ratios of sepal length to sepal width: iris %&gt;% mutate(ratio = Sepal.Length / Sepal.Width) Sort by sepal length in ascending order: iris %&gt;% arrange(Sepal.Length) The power of dplyr is that we can connect these pipe operators to define a work flow. Suppose we want to see the Iris setosa flowers with the largest ratio of sepal length to sepal width. iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% # filter rows select(Sepal.Length, Sepal.Width) %&gt;% # select two columns mutate(ratio = Sepal.Length / Sepal.Width) %&gt;% # add a new column arrange(desc(ratio)) %&gt;% # sort in descending order head() # only show top rows. No more pipes, end of sequence. ## Sepal.Length Sepal.Width ratio ## 1 4.5 2.3 1.956522 ## 2 5.0 3.0 1.666667 ## 3 4.9 3.0 1.633333 ## 4 4.8 3.0 1.600000 ## 5 4.8 3.0 1.600000 ## 6 5.4 3.4 1.588235 filter( ), mutate( ) and arrange( ) are 3 “verbs” that operate on the data frame sequentially. head( ) is the function that only shows the top rows. Notice the pipe operator %&gt;% at the end of each line. This code is much easier to read by humans, as it defines a sequence of operations. Two other useful verbs are group_by( ) and summarise( ). They can be used to generate summary statistics. Below, we use group_by to split the data frame into 3 data frames by the species information, compute the mean of sepal lengths and width, and then combine. So it is “split-apply-combine”. iris %&gt;% group_by(Species) %&gt;% # split by Species summarise(avgSL = mean(Sepal.Length), avgSW = mean(Sepal.Width)) %&gt;% arrange(avgSL) ## # A tibble: 3 x 3 ## Species avgSL avgSW ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 ## 2 versicolor 5.94 2.77 ## 3 virginica 6.59 2.97 Here we created a new data frame with the mean sepal length and sepal width for each of the 3 species. Obviously, we can change mean( ) to many other functions. This makes it very easy to summarize large data sets. Exercise 7.2 Fill in the blanks. 1, Read in the heart attack data same as you used in chapter 4. 2, Calculate the average cost per day for patients with different DIAGNOSIS codes. 3, Restrict to females aged older than 20 and younger than 70 who stayed at least one day. 4, Sort the results in descending order. 5, Use one command with multiple steps. Hint: Build your code step by step. Test each step to make sure they work separately as desired. You can use the head or summary function at the end to examine if the desired data is produced. heartatk4R &lt;- read.csv(&quot;____Insert Your own Path_____/heartatk4R.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, colClasses = c(&quot;character&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;)) head(heartatk4R) library(dplyr) df &lt;- heartatk4R %&gt;% filter(SEX == &quot;______&quot; &amp; AGE &gt; 20 _____ AGE &lt; 70 &amp; ________) %&gt;% group_by(___________) %&gt;% ___________(CostPerDay = ___________) %&gt;% summarise(AvgCostPerDay = __________(CostPerDay, na.rm = _______)) %&gt;% ___________(-AvgCostPerDay) df 7.3 Interactive plots made easy with Plotly install.packages(&quot;plotly&quot;) library(plotly) g &lt;- ggplot(iris, aes(Petal.Width, Petal.Length , color = Species)) + geom_point() ggplotly(g) We first generated the plot using ggplot2 and stored it in an object g, which is rendered interactive with Plotly. If you mouse over the plot, the values are highlighted. You can also select an area on the chart to zoom in. The R community is uniquely supportive. There are lots of free online books, tutorials, example codes, etc. Here are some helpful websites and information. 7.4 Shiny Apps Recent developments in R made it easy to create interactive charts and even complex websites. Without any web development experience, I created a site entirely in R ( iDEP http://ge-lab.org/idep/ ) to enable biologists to analyze genomic data on their own. My blog (http://gex.netlify.com ) is also created in Rstudio. 7.4.1 Install the Shiny package by typing this in the console. install.packages(&quot;shiny&quot;) 7.4.2 Create a Shiny web app is a piece of cake Start a new Shiny app use the shortcut shown above. Or, select File -&gt; New File -&gt; Shiny Web App from the RStudio main menu. Give your app a name, such as test1. Save the app to your local PC. The nice nerds at Rstudio understand the power of an example. A small, but functional app is shown, defined in a file called test1.app.R. Click on Run App on the top-right of the script window, a histogram will pop up. We can custermize the number of bins of the histogram by moving the sliding bar. We can further custermize the outputs by editing the code. In the app.R file, there are two functions: ui() and server() . The ui() defines the user interface, and server() specifies the logic. 7.4.3 Let’s play! Change the color by changing the col = ‘darkgray’ in line 44 to your favorite color, such as ‘green’, ‘red’, etc. To make it colorful, set it to col = rainbow(10) so we can use ten colors on a rolling basis. See the highlignted code. After saving the correction, run the app again. Switch the data. To replace the original dataset Old Faithful Geyser to a new dataset iris, we need to change the columns and dataset specified on line 40. Meanwhile we need to change the content after titlePanel on line 16 to get a new title. See the hignlighted code in the following graphs. Save the changes and run the app again. Figure 7.5 is the output of histogram of the Sepal length in iris dataset. Figure 7.5: Output of the Sepal length in the iris. The default number of bins of 30, specified at line 25, is probably too big. We can change the defaul value to 12 by setting value = 12. Save the changes and run the app again, we have the histogram with 12 bins by default. We can change ‘Sepal.Length’ to other columns by assigning x to other variables. Would it be cool if the user can choose the variable from the output? To make it, we need to add a control widget by inserting this line of code after line 20: “selectInput(”cid“,”Column“, choices = colnames(iris)),”. Do not forget the comma at the very end! Save and run the app. A control widget Column presents above the Number of bins. We are able to select any of the variable in iris dataset. As you may have noticed that, the columns can be selected from the control widget Column, but the histogram does not change correspondingly. This is because that we have not changed the code with respect to the histogram. Take a look at the server function, on line 44, the histogram function is given as hist(x,…) where x &lt;- iris[, ‘Sepal.Length’] sepecified on line 40. That is why the histogram produces only the histogram of Sepal length. To let the histogram change according to the selection of columns, we replace x &lt;- iris[, ‘Sepal.Length’] by x &lt;- iris[, input$cid ] on the line 40. Note that the input$cid is consistant with the inserted code selectInput( “cid”,\\(\\cdots\\)) on line 20: “sidebarPanel(selectInput(“cid”, “Column”, choices = colnames(iris)),”. Save and run the code. Select a variable from the Column, say Petal.Width, then the histogram of Petal width shows up correctly. Exercise 7.3 What happens when you choose the last Column “Species” in the app? Solve the error by deleting the variable Species from the data iris. Hint: Modify the code on line 20: selectInput(“cid”, “Column”, choices = colnames(iris)). Exercise 7.4 Change the title from “Histogram of x” to “Histogram of &quot; where is the name of the selected variable like “Sepal.Length”. Complete the question by filling in the blanks. To change the title, let’s define a main title as titl &lt;- , then assign it to the option “main =___” in the hist() function. Your output should look like the Figure 7.6 titl &lt;- paste(&quot;Histogram of&quot;, **input$cid**, sep = &quot; &quot;) hist(x, breaks = bins, col = &#39;green&#39;, border = &#39;white&#39;, main = _______) Figure 7.6: Output of Exercise 7.4. We can build a more complex app by adding an approximation normal distribution line by replace the function hist(x, breaks = bins, col = ‘green’, border = ‘white’) by the code below: h &lt;- hist(x, breaks = bins, col = rainbow(10), border = &#39;white&#39;) yfit &lt;- dnorm(bins, mean = mean(x), sd = sd(x)) yfit &lt;- yfit * diff( h$mids[1:2]) * length(x) lines(bins, yfit, col = &quot;blue&quot;) The output should be similar to Figure 7.7. Figure 7.7: Output of adding an approximation normal distribution curve. Exercise 7.5 Fill in the blanks to solve the error message in this app by plotting a pie chart when the variable Species is selected in the Column. Steps: 1, Change the selectInput() line as its original with all columns selected. 2, Separate the variables into two groups, numeric variables (columns 1 throught 4) for histogram and categorical variables (5th column) for pie plot. 3, Use if() and }else{ for each group. Your output of the pie chart should be similar to Figure 7.8. Figure 7.8: Pie chart output of Exercise 7.5. library(shiny) # Define UI for application that draws a histogram ui &lt;- fluidPage( # Application title titlePanel(&quot;Interactive page for iris data&quot;), # Sidebar with a slider input for number of bins sidebarLayout( sidebarPanel( selectInput(&quot;cid&quot;, &quot;Select the column&quot;, choices = ___________), sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 12) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;Plot&quot;) ) ) ) # Define server logic required to draw a histogram and pie plot server &lt;- function(input, output) { output$Plot &lt;- renderPlot({ x &lt;- iris[, input$cid] # draw the histogram with the specified number of bins for variable columns ________ (input$cid %in% colnames(iris)[______]){ bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) titl &lt;- paste(&quot;Histogram of&quot;, input$cid, sep = &quot; &quot;) hist(x, breaks = bins, col = &#39;green&#39;, border = &#39;white&#39;, main = titl) # draw the pie plot for catogorical column Species __________ count &lt;- table(x) labl &lt;- paste(names(count), &quot;\\n&quot;, count, sep = &quot;&quot;) titl &lt;- paste(&quot;Pie plot of&quot;, input$cid, sep = &quot; &quot;) pie(count, labels = labl, main = titl) } }) } You can publish your app online by clicking on the Publish button at the top right of the app window and following the instructions. Show it off by sending an URL to your friends or your adviser. More instructions can be found in these excellent tutorials: https://docs.rstudio.com/shinyapps.io/getting-started.html, and https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/. Solutions to these challenges can be found at GitHub https://github.com/gexijin/teach/blob/master/app.R "],
["exploring-of-state-data-set.html", "Chapter 8 Exploring of state data set 8.1 Reading in and manipulating data 8.2 Basic information about data 8.3 Analyzing the relationship among variables 8.4 Peeking the whole picture of the data set 8.5 Linear model anylysis 8.6 Conclusion", " Chapter 8 Exploring of state data set 8.1 Reading in and manipulating data The state data sets include state information in early years around 1970s. We pick state.abb, state.x77, and state.region to form our data file. The detail information is listed here. state.abb: vector with 2-letter abbreviations for the state names. state.x77: matrix with 50 rows and 8 columns giving the following statistics in the respective columns. Population: population estimate as of July 1, 1975 Income: per capita income (1974) Illiteracy: illiteracy (1970, percent of population) Life Exp: life expectancy in years (1969-71) Murder: murder and non-negligent manslaughter rate per 100,000 population (1976) HS Grad: percent high-school graduates (1970) Frost: mean number of days with minimum temperature below freezing (1931-1960) in capital or large city Area: land area in square miles state.region: factor giving the region (Northeast, South, North Central, West) that each state belongs to. First we merge these three data sets to get data frame “sta” with 10 columns and 50 rows, and take a look of the data. tem &lt;- data.frame(state.x77) # transform matrix into data frame sta &lt;- cbind(state.abb, tem, state.region) colnames(sta)[1] &lt;- &quot;State&quot; colnames(sta)[10] &lt;- &quot;Region&quot; head(sta) ## State Population Income Illiteracy Life.Exp Murder HS.Grad ## Alabama AL 3615 3624 2.1 69.05 15.1 41.3 ## Alaska AK 365 6315 1.5 69.31 11.3 66.7 ## Arizona AZ 2212 4530 1.8 70.55 7.8 58.1 ## Arkansas AR 2110 3378 1.9 70.66 10.1 39.9 ## California CA 21198 5114 1.1 71.71 10.3 62.6 ## Colorado CO 2541 4884 0.7 72.06 6.8 63.9 ## Frost Area Region ## Alabama 20 50708 South ## Alaska 152 566432 West ## Arizona 15 113417 West ## Arkansas 65 51945 South ## California 20 156361 West ## Colorado 166 103766 West str(sta) ## &#39;data.frame&#39;: 50 obs. of 10 variables: ## $ State : Factor w/ 50 levels &quot;AK&quot;,&quot;AL&quot;,&quot;AR&quot;,..: 2 1 4 3 5 6 7 8 9 10 ... ## $ Population: num 3615 365 2212 2110 21198 ... ## $ Income : num 3624 6315 4530 3378 5114 ... ## $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... ## $ Life.Exp : num 69 69.3 70.5 70.7 71.7 ... ## $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... ## $ HS.Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... ## $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... ## $ Area : num 50708 566432 113417 51945 156361 ... ## $ Region : Factor w/ 4 levels &quot;Northeast&quot;,&quot;South&quot;,..: 2 4 4 2 4 4 1 2 2 2 ... summary(sta) ## State Population Income Illiteracy ## AK : 1 Min. : 365 Min. :3098 Min. :0.500 ## AL : 1 1st Qu.: 1080 1st Qu.:3993 1st Qu.:0.625 ## AR : 1 Median : 2838 Median :4519 Median :0.950 ## AZ : 1 Mean : 4246 Mean :4436 Mean :1.170 ## CA : 1 3rd Qu.: 4968 3rd Qu.:4814 3rd Qu.:1.575 ## CO : 1 Max. :21198 Max. :6315 Max. :2.800 ## (Other):44 ## Life.Exp Murder HS.Grad Frost ## Min. :67.96 Min. : 1.400 Min. :37.80 Min. : 0.00 ## 1st Qu.:70.12 1st Qu.: 4.350 1st Qu.:48.05 1st Qu.: 66.25 ## Median :70.67 Median : 6.850 Median :53.25 Median :114.50 ## Mean :70.88 Mean : 7.378 Mean :53.11 Mean :104.46 ## 3rd Qu.:71.89 3rd Qu.:10.675 3rd Qu.:59.15 3rd Qu.:139.75 ## Max. :73.60 Max. :15.100 Max. :67.30 Max. :188.00 ## ## Area Region ## Min. : 1049 Northeast : 9 ## 1st Qu.: 36985 South :16 ## Median : 54277 North Central:12 ## Mean : 70736 West :13 ## 3rd Qu.: 81163 ## Max. :566432 ## 8.2 Basic information about data Now let’s see whether the numeric variables are normally distributed or not. library(dplyr) a &lt;- colnames(sta)[2:9] # pick up the numeric columns according to the names par(mfrow = c(4, 4)) # layout in 4 rows and 4 columns for (i in 1:length(a)){ sub = sta[a[i]][,1] hist(sub, main = paste(&quot;Hist. of&quot;, a[i], sep = &quot; &quot;), xlab = a[i]) qqnorm(sub, main = paste(&quot;Q-Q Plot of&quot;, a[i], sep = &quot; &quot;)) qqline(sub) if (i == 1) {s.t &lt;- shapiro.test(sub) } else {s.t &lt;- rbind(s.t, shapiro.test(sub)) } } s.t &lt;- s.t[, 1:2] # take first two column of shapiro.test result s.t &lt;- cbind(a, s.t) # add variable name for the result s.t ## a statistic p.value ## s.t &quot;Population&quot; 0.769992 1.906393e-07 ## &quot;Income&quot; 0.9769037 0.4300105 ## &quot;Illiteracy&quot; 0.8831491 0.0001396258 ## &quot;Life.Exp&quot; 0.97724 0.4423285 ## &quot;Murder&quot; 0.9534691 0.04744626 ## &quot;HS.Grad&quot; 0.9531029 0.04581562 ## &quot;Frost&quot; 0.9545618 0.05267472 ## &quot;Area&quot; 0.5717872 7.591835e-11 From the histograms and QQplots we can see that the distribution of Population, Illiteracy and Area skewed to the left. Income and Life.Exp distributed close to normal. The shapiro tests show that Income, Life.Exp and Frost are normally distributed with p value greater than 0.05, while Murder and HS.Grad are almost normally distributed with p value really close to 0.05. There is no evidence that Population, Illiteracy and Area have normal distribution. As for the categorical variable region, here is the region information including the count and percentage of states. counts &lt;- sort(table(sta$Region), decreasing = TRUE) percentages &lt;- 100 * counts / length(sta$Region) barplot(percentages, ylab = &quot;Percentage&quot;, col = &quot;lightblue&quot;) text(x=seq(0.7, 5, 1.2), 2, paste(&quot;n=&quot;, counts)) Figure 8.1: State count in each region Bar plot tells us that we have relatively more states in South(16) and less states in Northeast(9). North Central and West have similar number of states(12 and 13). If we want to know whether the populations in California and New York are more than the other states like what we have in now days, or the population of South Dakota comparing with other states, we use Lollipop plot to show the population of all states. library(ggplot2) ggplot(sta, aes(x = State, y = Population)) + geom_point(size = 3) + geom_segment(aes(x = State, xend = State, y = 0, yend = Population)) + labs(title = &quot;Lollipop Chart for Population&quot;) + theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 65, vjust = 0.6)) Figure 8.2: Loppipop plot of population in each state From the plot we can see even in early days, California and New York are the top two states in population. South Dakota have little population even in 1970s. Other questions we may ask are: how about the murder rate distribution in early days? Is it the same for different states and different regions? What are the main effect factors to murder rate? Can we use model of other factors to explain their contribution to murder rate? library(maps) sta$region &lt;- tolower(state.name) # create new character vector with lowercase states names states &lt;- map_data(&quot;state&quot;) # extract state data map &lt;- merge(states, sta, by = &quot;region&quot;, all.x = T) # merge states and state.x77 data map &lt;- map[order(map$order), ] ggplot(map, aes(x = long, y = lat, group = group)) + geom_polygon(aes(fill = Murder)) + geom_path() + scale_fill_gradientn(colours = rev(heat.colors(10))) + coord_map() + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + guides(fill = guide_legend(title = &quot;Murder Rate&quot;)) + theme(plot.title = element_text(hjust = 0.5)) Figure 8.3: Map of murder rate distribution We can see from the map that the bottom and right of the map are close to red while the top middle and left are yellow. There is an area on top-right are yellow too. The map tells us that murder rate are higher in south and east states but less in north central, northwest and northeast states. library(ggridges) ggplot(sta, aes(x = Murder, y = Region, fill = Region)) + geom_density_ridges() + theme_ridges() + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5), axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5)) Figure 8.4: Ridgeline plot for murder rate in each region The ridgeline plot tells us that murder rate skewed to the left for region west, northeast and north central, but skewed to the right for region south, which confirm with map above that south has big murder rate than other regions. Exercise 8.1 Use lollipop plots to explore the distribution of Illiteracy in state.x77 data set and give brief interpretation. Hint: You can combine state.abb to state.x77 or use the row names of state.x77 data set directly. Exercise 8.2 Use ridgeline plot to explore the regional distribution of Illiteracy for state.x77 and state.region data sets and interpret your figure. 8.3 Analyzing the relationship among variables st &lt;- sta[, 2:9] #take numeric variables as goal matrix library(ellipse) library(corrplot) corMatrix &lt;- cor(as.matrix(st)) # correlation matrix col &lt;- colorRampPalette(c(&quot;#7F0000&quot;, &quot;red&quot;, &quot;#FF7F00&quot;, &quot;yellow&quot;, &quot;#7FFF7F&quot;, &quot;cyan&quot;, &quot;#007FFF&quot;, &quot;blue&quot;, &quot;#00007F&quot;)) corrplot.mixed(corMatrix, order = &quot;AOE&quot;, lower = &quot;number&quot;, lower.col = &quot;black&quot;, number.cex = .8, upper = &quot;ellipse&quot;, upper.col = col(10), diag = &quot;u&quot;, tl.pos = &quot;lt&quot;, tl.col = &quot;black&quot;) Figure 8.5: Corrplot for numeric variables On the top-right of correlation figure we can see the red and narrow shape between Murder and Life.Exp which shows high negative correlation, the blue narrow shape between Murder and Illiteracy which shows high positive correlation, the red-orange narrow shape between Murder and Frost, HS.Grad which show median negative correlation, also the orange shape between Murder and Income which shows small negative correlation and light-blue shape between Murder and both Area and Population which show small positive correlation. The pearson and spearman correlation matrix on the bottom-left gives us the r values between each pair of the variables, which confirm the correlation shape on the top-right. Positive correlation between Murder and Illiteracy with r value of 0.70, which means the lower education level the state have, the higher murder rate chance it will happen in that state; Negative correlations between Murder and Life.Exp, Frost, with r value of -0.78, and -0.54 illustrate that the more occurrence of murder, the shorter life expectation the state will have; And the colder of the weather, the lower chance the murder will occur: too cold to murder?! Exercise 8.3 According to the corrplot, Figure 8.5, explain the correlation between Illiteracy and other variables. Now let’s see the cluster situation of these variables. plot(hclust(as.dist(1 - cor(as.matrix(st))))) # hierarchical clustering Figure 8.6: Cluster dendrogram for state numeric variables The cluster Dendrogram tells us that there are two clusters for these variables. Murder is mostly close to Illiteracy, and then to Population and Area. Similar situation, HS.Grad is mostly close to Income, and then to Life.Exp and Frost. Though illiteracy and HS.Grad are in different cluster, we know for the same state, illiteracy is highly correlated with high school graduation rate , the lower the illiteracy, the higher the high school graduation rate. r value of -0.66 between Illiteracy and HS.Grad in the corrplot tells the same story. we can use density plot to see the distribution of Illiteracy by region. ggplot(sta, aes(x = Illiteracy, fill = Region)) + geom_density(alpha = 0.3) Figure 8.7: Illiteracy distribution by region We can see that north central region has narrow density distribution with most Illiteracy less than 1 percent of population. While south region has an open distribution with illiteracy covered from 0.5 to 3, and most south states have illiteracy between 1.5 and 2.2. Though region west has a spread out distribution too, but it’s left skewed, which means there are still lots of west states with illiteracy less than 1% of population. Most northeast region states have illiteracy less then 1.5% of population. Because of the relationship of Murder with both Population and Area, We add one more column of Pop.Density for the population per square miles of area to see the correlation between Murder and this density. sta$Pop.Density &lt;- sta$Population/sta$Area boxplot(sta$Pop.Density ~ sta$Region) Figure 8.8: Box plot of population by region model &lt;- aov(sta$Pop.Density ~ sta$Region, sta) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sta$Region 3 1.051 0.3502 12 6.3e-06 *** ## Residuals 46 1.343 0.0292 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The box plot shows that Pop.Density of Northeast is much more than the other regions, while West has lowest Pop.Density. ANOVA test with p value of 6.3e-06 also let us reject the null hypothesis that mean Pop.Densities are same for different regions, which means at least one of the regional population densities is different from the others. Here is the scatterplot for Illiteracy and Murder with Population per area. ggplot(sta, aes(x = Illiteracy, y = Murder)) + geom_point(aes(size = Pop.Density, color = Region)) + geom_smooth(method = &#39;lm&#39;,formula = y ~ x) + # add regression line theme(plot.title = element_text(hjust = 0.5)) Figure 8.9: Scatterplot for illiteracy and murder sized by population density and colored by region The plot shows that murder and illiteracy are positive correlated. All states in other three regions have murder rate less than 12 per 100,000 population except some of south states have murder over 12 per 100,000 population. All north central states(red) has illiteracy less than 1, all northeast states have less than 1.5 of illiteracy. The illiteracy of west and south states have much bigger variance. More Northeast states have big population density but middle illiteracy rate comparing with the states in the other three regions. Because of the high correlation of murder and Life.Exp, we will take a look of the distribution of Life.Exp. ggplot(sta, aes(x = Region, y = Life.Exp, fill = Region)) + geom_violin(trim=FALSE) + geom_boxplot(width=0.1) + theme(plot.title = element_text(hjust = 0.5)) Figure 8.10: Regional life expectancy On average, south has lower life expectancy than the other three regions. North Central has highest Life.Exp, while West has spread out distribution with two long tails on each ends, which means some west states have really long life expectancy, while some states expect short life though they are in the same region. Here is the plot for murder with the information of variables in the other cluster. According to the corrplot, we believe they affect the murder rate too, more or less. # group income into IncomeType first sta.income &lt;- sta %&gt;% mutate(IncomeType = factor(ifelse(Income &lt; 3500, &quot;Under3500&quot;, ifelse(Income &lt; 4000 &amp; Income &gt;= 3500, &quot;3500-4000&quot;, ifelse(Income &lt; 4500 &amp; Income &gt;= 4000, &quot;4000-4500&quot;, ifelse(Income &lt; 5000 &amp; Income &gt;= 4500, &quot;4500-5000&quot;, &quot;Above5000&quot;)))))) ggplot(sta.income, aes(x = Murder, y = Life.Exp)) + geom_point(aes(shape = IncomeType, color = Region, size = HS.Grad)) + geom_smooth(method = &#39;lm&#39;,formula = y ~ x) + #labs(title = &quot;Murder Vs Life.Exp with HS.Grad and IncomeType by region&quot;) + theme(plot.title = element_text(hjust = 0.5)) Figure 8.11: Relationship between murder rate and life expectancy, high school graduation and income Murder is negatively correlated with Life.Exp. Some states with higher murder rate over 12 have relatively small symbols, which means their high school graduation rates are as less as 40%; And these small symbols with murder rate bigger than 12 are all colored as green, which means they all belong to south region. It looks like the income type does not affect the murder rate a lot because all different symbols scatter around in different murder rates, especially between murder rate 8 and 10. Most southern states has lower HS.Grad high, low Life.Exp but higher murder frequency, while states in other three regions have relative higher HS.Grad and income but lower murder rate. Exercise 8.4 Use scatter plot to analyze the correlation between Illiteracy and those variables in the other cluster shown in Figure 8.6. Interpret your plot. 8.4 Peeking the whole picture of the data set library(gplots) st.matrix &lt;- as.matrix(st) # transfer the data frame to matrix s &lt;- apply(st.matrix, 2, function(y)(y - mean(y)) / sd(y)) # standardize data a &lt;- heatmap.2(s, col = greenred(75), #color green red density.info = &quot;none&quot;, trace = &quot;none&quot;, scale = &quot;none&quot;, RowSideColors = rainbow(4)[sta$Region], srtCol = 45, #column labels at 45 degree margins = c(5, 8), # bottom and right margins lhei = c(5, 15) ) legend(&quot;topright&quot;, levels(sta$Region), fill = rainbow(4), cex = 0.8) # add legend Figure 8.12: Heat map for whole state data set Same as cluster Dendrogram plot, Life.Exp, Income, HS.Grad, together with Frost build one cluster, while Illiteracy, Murder and Population and area build another cluster. Compare with other states, lots of south states with lower Life.Exp, Income, HS.Grad have higher Murder and Illiteracy, like Mississippi and Alabama. On the contrary, some northern and western states which have higher Life.Exp, Income, HS.Grad show lower Area, Population, Murder and Illiteracy, like Nebraska and South Dakota. Though the income of South Dakota show a little bit green. row.names(st) &lt;- sta$State stars(st, key.loc = c(13, 1.5), draw.segments = T) Figure 8.13: Segment diagram for all states The segment Diagram shows us different aspects of each state. For example, South Dakota has big Frost(yellow), big Life Expectancy(blue), relative high percentage of high school graduation rate(pink) and good income(red), but has small area and really tiny, almost nothing comparing with other states in population, illiteracy and murder. We use principal components analysis to explore the data a little bit more! pca = prcomp(st, scale = T) #scale = T to normalize the data pca ## Standard deviations (1, .., p=8): ## [1] 1.8970755 1.2774659 1.0544862 0.8411327 0.6201949 0.5544923 0.3800642 ## [8] 0.3364338 ## ## Rotation (n x k) = (8 x 8): ## PC1 PC2 PC3 PC4 PC5 ## Population 0.12642809 0.41087417 -0.65632546 -0.40938555 0.405946365 ## Income -0.29882991 0.51897884 -0.10035919 -0.08844658 -0.637586953 ## Illiteracy 0.46766917 0.05296872 0.07089849 0.35282802 0.003525994 ## Life.Exp -0.41161037 -0.08165611 -0.35993297 0.44256334 0.326599685 ## Murder 0.44425672 0.30694934 0.10846751 -0.16560017 -0.128068739 ## HS.Grad -0.42468442 0.29876662 0.04970850 0.23157412 -0.099264551 ## Frost -0.35741244 -0.15358409 0.38711447 -0.61865119 0.217363791 ## Area -0.03338461 0.58762446 0.51038499 0.20112550 0.498506338 ## PC6 PC7 PC8 ## Population -0.01065617 -0.062158658 -0.21924645 ## Income 0.46177023 0.009104712 0.06029200 ## Illiteracy 0.38741578 -0.619800310 -0.33868838 ## Life.Exp 0.21908161 -0.256213054 0.52743331 ## Murder -0.32519611 -0.295043151 0.67825134 ## HS.Grad -0.64464647 -0.393019181 -0.30724183 ## Frost 0.21268413 -0.472013140 0.02834442 ## Area 0.14836054 0.286260213 0.01320320 plot(pca) # plot the amount of variance each principal components captures. summary(pca) #shows the importance of the components ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 1.8971 1.2775 1.0545 0.84113 0.62019 0.55449 ## Proportion of Variance 0.4499 0.2040 0.1390 0.08844 0.04808 0.03843 ## Cumulative Proportion 0.4499 0.6539 0.7928 0.88128 0.92936 0.96780 ## PC7 PC8 ## Standard deviation 0.38006 0.33643 ## Proportion of Variance 0.01806 0.01415 ## Cumulative Proportion 0.98585 1.00000 percentVar &lt;- round(100 * summary(pca)$importance[2, 1:7], 0) # compute % variances percentVar ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## 45 20 14 9 5 4 2 The first two components account for 45% and 20%, together 65% of the variance. The third component attributes a little bit less but still over 10% of the variance. The barplot of each component’s variance shows how the each component dominate. library(ggfortify) row.names(sta) &lt;- sta$State autoplot(prcomp(st, scale = T), data = sta, colour = &#39;Region&#39;, shape = FALSE, label = TRUE, label.size = 3.5, loadings = TRUE, loadings.colour = &#39;blue&#39;, loadings.label = TRUE, loadings.label.size = 4, loadings.label.colour = &#39;blue&#39;) Figure 8.14: Bioplot for PCA The Biplot illustrate the special role of these variables to the first and second component of the variance. Illiteracy positively contribute to component of the variance PC1, while Life.Exp and Frost negatively contribute to component of the variance PC1. Area positively contribute to component of the variance PC2. The other four variables contribute to both component of the variance PC1 and PC2 positively or negatively. From the figure we also find that many states in south region such as Louisiana(LA) and Mississippi(MS) are mainly affected by Illiteracy and murder rate, while some north central states like Minnesota(MN) and North Dakota(ND) are mainly affected by life expectancy and frost. Area is the main effect for two states in West region, Alaska(AK) and California(CA). 8.5 Linear model anylysis According to the analysis above, we try to find a model to explain murder rate. Because of the high correlation of HS.Grad with Illiteracy, Life.Exp and Income, we will not put HS.Grad in the model. Similar reason, we leave Frost out too. lm.data &lt;- sta[, c(2:6, 9:10)] lm.data &lt;- within(lm.data, Region &lt;- relevel(Region, ref = &quot;South&quot;)) # set region South as reference model &lt;- lm(Murder ~ ., data = lm.data) summary(model) ## ## Call: ## lm(formula = Murder ~ ., data = lm.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8178 -0.9446 -0.1485 1.0406 3.5501 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.059e+02 1.601e+01 6.611 5.85e-08 *** ## Population 2.591e-04 5.439e-05 4.764 2.39e-05 *** ## Income 2.619e-04 4.870e-04 0.538 0.59362 ## Illiteracy 1.861e+00 5.567e-01 3.343 0.00178 ** ## Life.Exp -1.445e+00 2.275e-01 -6.351 1.37e-07 *** ## Area 1.133e-06 3.407e-06 0.333 0.74117 ## RegionNortheast -2.673e+00 8.020e-01 -3.333 0.00183 ** ## RegionNorth Central -7.182e-01 8.712e-01 -0.824 0.41451 ## RegionWest 2.358e-01 8.096e-01 0.291 0.77229 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.552 on 41 degrees of freedom ## Multiple R-squared: 0.852, Adjusted R-squared: 0.8232 ## F-statistic: 29.51 on 8 and 41 DF, p-value: 1.168e-14 Murder is most related to Life.Exp and Population of the state, also affected by Illiteracy of the state. Region is another relative smaller factor contributing to murder rate. The estimates illustrate that every unit of increasing in Life.Exp will decrease 1.445 unit of murder rate, while every unit of increasing in population and illiteracy will increase 0.000259 and 1.861 unit of murder rate. At the same time, if the state belongs to northeast region, the murder rate will be 2.673 unit less. The model will explain 82% of the variance of murder rate. If we know the population, Life.Exp, Illiteracy of the certain state in those years, we can estimate murder rate as follow: \\(Murder = 105.9 - 1.445 * Life.Exp + 0.000259 * Population + 1.861 * Illiteracy - 2.673 * RegionNortheast\\) Exercise 8.5 Do linear model analysis for Illiteracy and interpret your result. Hint: Check the corrplot figure 8.5 and pay attention to the high correlation between murder rate and life expectancy. 8.6 Conclusion -Southern region shows higher murder rate with lower life expectancy, income, and high school gradation rate but higher illiteracy, while northern region shows lower murder rate with higher population density, life expectancy, income, and high school gradation rate but lower illiteracy. -The information of life expectancy, population, illiteracy of the state in 1970s and whether the state belongs to northeast region will help to estimate the murder rate of the state at that time. "],
["exploring-of-game-sale-dataset.html", "Chapter 9 Exploring of game sale dataset 9.1 Reading in data and manage data 9.2 Visualization of categorical variables 9.3 Correlation among numeric variables 9.4 Analysis of score and count 9.5 Analysis of sales 9.6 Effect of platform type to priciple components 9.7 Models for global sales 9.8 Conclusion", " Chapter 9 Exploring of game sale dataset This is a video game sales data including game sales of North America, European, Japan and other area, together they make the global sale. The data also give the information about the critic score, user score and the number of critics or users who gave these two scores. This data is downloaded from https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings#Video_Games_Sales_as_at_22_Dec_2016.csv. The detail about the data is listed as follow: Name: Name of the game Platform: Console on which the game is running Year_of_Release: Year of the game released Genre: Game’s category Publisher: Publisher NA_Sales: Game sales in North America (in millions of units) EU_Sales: Game sales in the European Union (in millions of units) JP_Sales: Game sales in Japan (in millions of units) Other_Sales: Game sales in the rest of the world, i.e. Africa, Asia excluding Japan, Australia, Europe excluding the E.U. and South America (in millions of units) Global_Sales: Total sales in the world (in millions of units) Critic_Score: Aggregate score compiled by Meta critic staff Critic_Count: The number of critics used in coming up with the Critic_score User_Score: Score by Metacritic’s subscribers User_Count: Number of users who gave the user_score Developer: Party responsible for creating the game Rating: The ESRB ratings: E for “Everyone”; E10+ for “Everyone 10+”; T for “Teen”; M for “Mature”; AO for “Adults Only”; RP for “Rating Pending”; K-A for kids to adults. After downloading the data, We replace N/A with NA first in excel and save it as csv file and read in. We remove these observations with empty string of Rating and 6825 observations was left in our data set “game”. We notice that there are lots of zero sales value. To prepare these variables ready for being taken log value to make them normal or close to normal distribution, we transform the sales into its basic units and plus 1. We also divide critic score by 10 to march the scale unit of user score. 9.1 Reading in data and manage data tem &lt;- read.csv(&quot;datasets/video-game-sales-at-22-Dec-2016.csv&quot;) tem &lt;- na.omit(tem) #remove NA library(dplyr) game &lt;- tem %&gt;% filter(Rating != &quot;&quot;) %&gt;% droplevels() #remove empty rating observations #by multiplying 1000000 we get the actual sale, #adding 1 makes all sales positive which make log possible for all sales later game$Year_of_Release &lt;- as.factor(as.character(game$Year_of_Release)) game$NA_Sales &lt;- game$NA_Sales * 1000000 + 1 game$EU_Sales &lt;- game$EU_Sales * 1000000 + 1 game$JP_Sales &lt;- game$JP_Sales * 1000000 + 1 game$Other_Sales &lt;- game$Other_Sales * 1000000 + 1 game$Global_Sales &lt;- game$Global_Sales * 1000000 + 1 # By divide by 10 to make Critic Score the same decimal as User Score game$Critic_Score &lt;- as.numeric(as.character(game$Critic_Score)) / 10 game$User_Score &lt;- as.numeric(as.character(game$User_Score)) game$Critic_Count &lt;- as.numeric(game$Critic_Count) game$User_Count &lt;- as.numeric(game$User_Count) # format column names colnames(game) &lt;- c(&quot;Name&quot;, &quot;Platform&quot;, &quot;Year.Release&quot;, &quot;Genre&quot;, &quot;Publisher&quot;, &quot;NA.Sales&quot;, &quot;EU.Sales&quot;, &quot;JP.Sales&quot;, &quot;Other.Sales&quot;, &quot;Global.Sales&quot;, &quot;Critic.Score&quot;, &quot;Critic.Count&quot;, &quot;User.Score&quot;, &quot;User.Count&quot;, &quot;Developer&quot;, &quot;Rating&quot;) head(game) ## Name Platform Year.Release Genre Publisher ## 1 Wii Sports Wii 2006 Sports Nintendo ## 2 Mario Kart Wii Wii 2008 Racing Nintendo ## 3 Wii Sports Resort Wii 2009 Sports Nintendo ## 4 New Super Mario Bros. DS 2006 Platform Nintendo ## 5 Wii Play Wii 2006 Misc Nintendo ## 6 New Super Mario Bros. Wii Wii 2009 Platform Nintendo ## NA.Sales EU.Sales JP.Sales Other.Sales Global.Sales Critic.Score ## 1 41360001 28960001 3770001 8450001 82530001 7.6 ## 2 15680001 12760001 3790001 3290001 35520001 8.2 ## 3 15610001 10930001 3280001 2950001 32770001 8.0 ## 4 11280001 9140001 6500001 2880001 29800001 8.9 ## 5 13960001 9180001 2930001 2840001 28920001 5.8 ## 6 14440001 6940001 4700001 2240001 28320001 8.7 ## Critic.Count User.Score User.Count Developer Rating ## 1 51 8.0 322 Nintendo E ## 2 73 8.3 709 Nintendo E ## 3 73 8.0 192 Nintendo E ## 4 65 8.5 431 Nintendo E ## 5 41 6.6 129 Nintendo E ## 6 80 8.4 594 Nintendo E str(game) ## &#39;data.frame&#39;: 6825 obs. of 16 variables: ## $ Name : Factor w/ 4377 levels &quot; Tales of Xillia 2&quot;,..: 4203 2055 4205 2530 4201 2533 2054 4195 1820 4196 ... ## $ Platform : Factor w/ 17 levels &quot;3DS&quot;,&quot;DC&quot;,&quot;DS&quot;,..: 13 13 13 3 13 13 3 13 15 13 ... ## $ Year.Release: Factor w/ 25 levels &quot;1985&quot;,&quot;1988&quot;,..: 15 17 18 15 15 18 14 16 19 18 ... ## $ Genre : Factor w/ 12 levels &quot;Action&quot;,&quot;Adventure&quot;,..: 11 7 11 5 4 5 7 11 4 11 ... ## $ Publisher : Factor w/ 262 levels &quot;10TACLE Studios&quot;,..: 164 164 164 164 164 164 164 164 147 164 ... ## $ NA.Sales : num 41360001 15680001 15610001 11280001 13960001 ... ## $ EU.Sales : num 28960001 12760001 10930001 9140001 9180001 ... ## $ JP.Sales : num 3770001 3790001 3280001 6500001 2930001 ... ## $ Other.Sales : num 8450001 3290001 2950001 2880001 2840001 ... ## $ Global.Sales: num 82530001 35520001 32770001 29800001 28920001 ... ## $ Critic.Score: num 7.6 8.2 8 8.9 5.8 8.7 9.1 8 6.1 8 ... ## $ Critic.Count: num 51 73 73 65 41 80 64 63 45 33 ... ## $ User.Score : num 8 8.3 8 8.5 6.6 8.4 8.6 7.7 6.3 7.4 ... ## $ User.Count : num 322 709 192 431 129 594 464 146 106 52 ... ## $ Developer : Factor w/ 1289 levels &quot;10tacle Studios, Fusionsphere Systems&quot;,..: 779 779 779 779 779 779 779 779 468 779 ... ## $ Rating : Factor w/ 7 levels &quot;AO&quot;,&quot;E&quot;,&quot;E10+&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int 2 5 6 10 11 13 19 21 22 23 ... ## ..- attr(*, &quot;names&quot;)= chr &quot;2&quot; &quot;5&quot; &quot;6&quot; &quot;10&quot; ... summary(game) ## Name Platform ## LEGO Star Wars II: The Original Trilogy : 8 PS2 :1140 ## Madden NFL 07 : 8 X360 : 858 ## Need for Speed: Most Wanted : 8 PS3 : 769 ## Harry Potter and the Order of the Phoenix: 7 PC : 651 ## Madden NFL 08 : 7 XB : 565 ## Need for Speed Carbon : 7 Wii : 479 ## (Other) :6780 (Other):2363 ## Year.Release Genre Publisher ## 2008 : 592 Action :1630 Electronic Arts : 944 ## 2007 : 590 Sports : 943 Ubisoft : 496 ## 2005 : 562 Shooter : 864 Activision : 492 ## 2009 : 550 Role-Playing: 712 Sony Computer Entertainment: 316 ## 2006 : 528 Racing : 581 THQ : 307 ## 2003 : 498 Platform : 403 Nintendo : 291 ## (Other):3505 (Other) :1692 (Other) :3979 ## NA.Sales EU.Sales JP.Sales ## Min. : 1 Min. : 1 Min. : 1 ## 1st Qu.: 60001 1st Qu.: 20001 1st Qu.: 1 ## Median : 150001 Median : 60001 Median : 1 ## Mean : 394485 Mean : 236090 Mean : 64159 ## 3rd Qu.: 390001 3rd Qu.: 210001 3rd Qu.: 10001 ## Max. :41360001 Max. :28960001 Max. :6500001 ## ## Other.Sales Global.Sales Critic.Score Critic.Count ## Min. : 1 Min. : 10001 Min. :1.300 Min. : 3.00 ## 1st Qu.: 10001 1st Qu.: 110001 1st Qu.:6.200 1st Qu.: 14.00 ## Median : 20001 Median : 290001 Median :7.200 Median : 25.00 ## Mean : 82678 Mean : 777591 Mean :7.027 Mean : 28.93 ## 3rd Qu.: 70001 3rd Qu.: 750001 3rd Qu.:8.000 3rd Qu.: 39.00 ## Max. :10570001 Max. :82530001 Max. :9.800 Max. :113.00 ## ## User.Score User.Count Developer Rating ## Min. :0.500 Min. : 4.0 EA Canada : 149 AO : 1 ## 1st Qu.:6.500 1st Qu.: 11.0 EA Sports : 142 E :2082 ## Median :7.500 Median : 27.0 Capcom : 126 E10+: 930 ## Mean :7.186 Mean : 174.7 Ubisoft : 103 K-A : 1 ## 3rd Qu.:8.200 3rd Qu.: 89.0 Konami : 95 M :1433 ## Max. :9.600 Max. :10665.0 Ubisoft Montreal: 87 RP : 1 ## (Other) :6123 T :2377 Summary of these variables tell us that some of games were published in the same name; PS2 is the most popular platform; Action is the most popular Genre; Electronic Arts has the most high frequency among the publishers; Rating T and E are the two most released ratings; For these sales, though the minimums, several quantiles and medians are small, but the maximums are high, which means there are real good sale games among them; Extreme big maximum User count hints so many users score some special games. Our pre-analysis shows that these variables are not normally distributed, especially those sales and score counts variables. We take logs to transform these variables. NA.Sales.Log &lt;- log(game$NA.Sales) EU.Sales.Log &lt;- log(game$EU.Sales) JP.Sales.Log &lt;- log(game$JP.Sales) Other.Sales.Log &lt;- log(game$Other.Sales) Global.Sales.Log &lt;- log(game$Global.Sales) Critic.Count.Log &lt;- log(game$Critic.Count) User.Count.Log &lt;- log(game$User.Count) Then we combine the log variables with the original variables. game.log &lt;- cbind.data.frame(NA.Sales.Log, EU.Sales.Log, JP.Sales.Log, Other.Sales.Log, Global.Sales.Log, Critic.Count.Log, User.Count.Log) game &lt;- cbind.data.frame(game, game.log) # the data we use for analysis head(game) ## Name Platform Year.Release Genre Publisher ## 1 Wii Sports Wii 2006 Sports Nintendo ## 2 Mario Kart Wii Wii 2008 Racing Nintendo ## 3 Wii Sports Resort Wii 2009 Sports Nintendo ## 4 New Super Mario Bros. DS 2006 Platform Nintendo ## 5 Wii Play Wii 2006 Misc Nintendo ## 6 New Super Mario Bros. Wii Wii 2009 Platform Nintendo ## NA.Sales EU.Sales JP.Sales Other.Sales Global.Sales Critic.Score ## 1 41360001 28960001 3770001 8450001 82530001 7.6 ## 2 15680001 12760001 3790001 3290001 35520001 8.2 ## 3 15610001 10930001 3280001 2950001 32770001 8.0 ## 4 11280001 9140001 6500001 2880001 29800001 8.9 ## 5 13960001 9180001 2930001 2840001 28920001 5.8 ## 6 14440001 6940001 4700001 2240001 28320001 8.7 ## Critic.Count User.Score User.Count Developer Rating NA.Sales.Log ## 1 51 8.0 322 Nintendo E 17.53782 ## 2 73 8.3 709 Nintendo E 16.56790 ## 3 73 8.0 192 Nintendo E 16.56342 ## 4 65 8.5 431 Nintendo E 16.23854 ## 5 41 6.6 129 Nintendo E 16.45171 ## 6 80 8.4 594 Nintendo E 16.48551 ## EU.Sales.Log JP.Sales.Log Other.Sales.Log Global.Sales.Log ## 1 17.18143 15.14259 15.94968 18.22867 ## 2 16.36183 15.14788 15.00640 17.38561 ## 3 16.20702 15.00335 14.89732 17.30502 ## 4 16.02817 15.68731 14.87330 17.21002 ## 5 16.03254 14.89051 14.85931 17.18004 ## 6 15.75281 15.36307 14.62199 17.15908 ## Critic.Count.Log User.Count.Log ## 1 3.931826 5.774552 ## 2 4.290459 6.563856 ## 3 4.290459 5.257495 ## 4 4.174387 6.066108 ## 5 3.713572 4.859812 ## 6 4.382027 6.386879 str(game) ## &#39;data.frame&#39;: 6825 obs. of 23 variables: ## $ Name : Factor w/ 4377 levels &quot; Tales of Xillia 2&quot;,..: 4203 2055 4205 2530 4201 2533 2054 4195 1820 4196 ... ## $ Platform : Factor w/ 17 levels &quot;3DS&quot;,&quot;DC&quot;,&quot;DS&quot;,..: 13 13 13 3 13 13 3 13 15 13 ... ## $ Year.Release : Factor w/ 25 levels &quot;1985&quot;,&quot;1988&quot;,..: 15 17 18 15 15 18 14 16 19 18 ... ## $ Genre : Factor w/ 12 levels &quot;Action&quot;,&quot;Adventure&quot;,..: 11 7 11 5 4 5 7 11 4 11 ... ## $ Publisher : Factor w/ 262 levels &quot;10TACLE Studios&quot;,..: 164 164 164 164 164 164 164 164 147 164 ... ## $ NA.Sales : num 41360001 15680001 15610001 11280001 13960001 ... ## $ EU.Sales : num 28960001 12760001 10930001 9140001 9180001 ... ## $ JP.Sales : num 3770001 3790001 3280001 6500001 2930001 ... ## $ Other.Sales : num 8450001 3290001 2950001 2880001 2840001 ... ## $ Global.Sales : num 82530001 35520001 32770001 29800001 28920001 ... ## $ Critic.Score : num 7.6 8.2 8 8.9 5.8 8.7 9.1 8 6.1 8 ... ## $ Critic.Count : num 51 73 73 65 41 80 64 63 45 33 ... ## $ User.Score : num 8 8.3 8 8.5 6.6 8.4 8.6 7.7 6.3 7.4 ... ## $ User.Count : num 322 709 192 431 129 594 464 146 106 52 ... ## $ Developer : Factor w/ 1289 levels &quot;10tacle Studios, Fusionsphere Systems&quot;,..: 779 779 779 779 779 779 779 779 468 779 ... ## $ Rating : Factor w/ 7 levels &quot;AO&quot;,&quot;E&quot;,&quot;E10+&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ NA.Sales.Log : num 17.5 16.6 16.6 16.2 16.5 ... ## $ EU.Sales.Log : num 17.2 16.4 16.2 16 16 ... ## $ JP.Sales.Log : num 15.1 15.1 15 15.7 14.9 ... ## $ Other.Sales.Log : num 15.9 15 14.9 14.9 14.9 ... ## $ Global.Sales.Log: num 18.2 17.4 17.3 17.2 17.2 ... ## $ Critic.Count.Log: num 3.93 4.29 4.29 4.17 3.71 ... ## $ User.Count.Log : num 5.77 6.56 5.26 6.07 4.86 ... Now we plot histogram and QQ plot for the transformed data set. name &lt;- colnames(game)[c(11, 13, 17:23)] # pick up the numeric columns according to the names par(mfrow = c(5, 4)) # layout in 5 rows and 4 columns for (i in 1:length(name)){ sub &lt;- sample(game[name[i]][, 1], 5000) submean &lt;- mean(sub) hist(sub, main = paste(&quot;Hist. of&quot;, name[i], sep = &quot; &quot;), xlab = name[i]) abline(v = submean, col = &quot;blue&quot;, lwd = 1) qqnorm(sub, main = paste(&quot;Q-Q Plot of&quot;, name[i], sep = &quot; &quot;)) qqline(sub) if (i == 1) {s.t &lt;- shapiro.test(sub) } else {s.t &lt;- rbind(s.t, shapiro.test(sub)) } } s.t &lt;- s.t[, 1:2] # take first two column of shapiro.test result s.t &lt;- cbind(name, s.t) # add variable name for the result s.t ## name statistic p.value ## s.t &quot;Critic.Score&quot; 0.9643089 1.876624e-33 ## &quot;User.Score&quot; 0.9088868 1.173196e-47 ## &quot;NA.Sales.Log&quot; 0.6430895 4.978703e-73 ## &quot;EU.Sales.Log&quot; 0.7122343 1.168331e-68 ## &quot;JP.Sales.Log&quot; 0.6219809 3.21786e-74 ## &quot;Other.Sales.Log&quot; 0.7079277 5.895367e-69 ## &quot;Global.Sales.Log&quot; 0.9970675 2.689961e-08 ## &quot;Critic.Count.Log&quot; 0.9767009 7.465229e-28 ## &quot;User.Count.Log&quot; 0.9431856 3.581467e-40 From the histograms and qq plots we can see that two scores and and their count log values, and global sales log are close to normal distribution. Though the Shapiro test still deny the normality of these log values. We assume they are normally distributed in our analysis. There are lots of interest points in this data set such as the distribution of global and regional sales and their relationship, the correlation of critic score and user score and their counts, whether these scores are the main effect for sales, or the effect of other factors like genre, rating, platform, even publisher to sales, and so on. First let’s do visualization. 9.2 Visualization of categorical variables To simplify platform analysis, We regroup platform as Platform.type. #regroup platform as Platform.type pc &lt;- c(&quot;PC&quot;) xbox &lt;- c(&quot;X360&quot;, &quot;XB&quot;, &quot;XOne&quot;) nintendo &lt;- c(&quot;Wii&quot;, &quot;WiiU&quot;, &quot;N64&quot;, &quot;GC&quot;, &quot;NES&quot;, &quot;3DS&quot;, &quot;DS&quot;) playstation &lt;- c(&quot;PS&quot;, &quot;PS2&quot;, &quot;PS3&quot;, &quot;PS4&quot;, &quot;PSP&quot;, &quot;PSV&quot;) game &lt;- game %&gt;% mutate(Platform.type = ifelse(Platform %in% pc, &quot;PC&quot;, ifelse(Platform %in% xbox, &quot;Xbox&quot;, ifelse(Platform %in% nintendo, &quot;Nintendo&quot;, ifelse(Platform %in% playstation, &quot;Playstation&quot;, &quot;Others&quot;))))) library(ggplot2) ggplot(game, aes(x = Platform.type)) + geom_bar(fill = &quot;blue&quot;) Figure 9.1: Bar plot of platform type As the bar plot shown here, Playstation is the biggest group, then xbox and nintendo. While others are the smallest type. dat &lt;- data.frame(table(game$Genre)) dat$fraction = dat$Freq / sum(dat$Freq) dat = dat[order(dat$fraction), ] dat$ymax = cumsum(dat$fraction) dat$ymin = c(0, head(dat$ymax, n = -1)) names(dat)[1] &lt;- &quot;Genre&quot; library(ggplot2) ggplot(dat, aes(fill = dat$Genre, ymax = ymax, ymin = ymin, xmax = 4, xmin = 3)) + geom_rect(colour = &quot;grey30&quot;) + # background color coord_polar(theta = &quot;y&quot;) + # coordinate system to polar xlim(c(0, 4)) + labs(title = &quot;Ring plot for Genre&quot;, fill = &quot;Genre&quot;) + theme(plot.title = element_text(hjust = 0.5)) Action, Sports and Shooter are the first three biggest genre. Action occupies almost 25% genre. Three of them together contribute half of genre count. Puzzle, Adventure and Stratage have relative less count. We regroup rating AO, RP and K-A as “Others” because there are only few observations of these ratings. #regroup Rating as Rating.type rating &lt;- c(&quot;E&quot;, &quot;T&quot;, &quot;M&quot;, &quot;E10+&quot;) game &lt;- game %&gt;% mutate(Rating.type = ifelse(Rating %in% rating, as.character(Rating), &quot;Others&quot;)) counts &lt;- sort(table(game$Rating.type), decreasing = TRUE) names(counts)[1] &lt;- &quot;T - Teen&quot; # rename the names of counts for detail information names(counts)[2] &lt;- &quot;E - Everyone&quot; names(counts)[3] &lt;- &quot;M - Mature&quot; names(counts)[4] &lt;- &quot;E10+ - Everyone 10+&quot; pct &lt;- paste(round(counts/sum(counts) * 100), &quot;%&quot;, sep = &quot; &quot;) lbls &lt;- paste(names(counts), &quot;\\n&quot;, pct, sep = &quot; &quot;) # labels with count number pie(counts, labels = lbls, col = rainbow(length(lbls)), main=&quot;Pie Chart of Ratings with sample sizes&quot;) According to the order, the most popular ratings are T, E, M and E10+. Other ratings only occupy very little in the all games. library(ggmosaic) library(plotly) p &lt;- ggplot(game) + geom_mosaic(aes(x = product(Rating.type), fill = Platform.type), na.rm=TRUE) + labs(x=&quot;Rating.type&quot;, y = &quot;Platform Type&quot;, title=&quot;Mosaic Plot&quot;) + theme(axis.text.y = element_blank()) ggplotly(p) Figure 9.2: Mosaic plot between platform type and rating type For all platform and rating combination, Playstation games are released most in all other three different rating types except Everyone 10 age plus. Nintendo is the most popular game for Everyone 10+, it’s the second popular platform for rating Everyone. Xbox is the second popular platform for rating mature and teenage,and it’s the third favorite platform for rating everyone and everyone 10+. Most Other platform games are rated as Everyone. Exercise 9.1 Download the game sale data set and clean the data as similar as described in the beginning of this chapter, produce a masaic plot between genre and rating. Interpret your plot breifly. 9.3 Correlation among numeric variables st &lt;- game[, c(11, 13, 17:23)] # take numeric variables as goal matrix st &lt;- na.omit(st) library(ellipse) # install.packages(&quot;ellipses&quot;) library(corrplot) corMatrix &lt;- cor(as.matrix(st)) # correlation matrix col &lt;- colorRampPalette(c(&quot;#7F0000&quot;, &quot;red&quot;, &quot;#FF7F00&quot;, &quot;yellow&quot;, &quot;#7FFF7F&quot;, &quot;cyan&quot;, &quot;#007FFF&quot;, &quot;blue&quot;, &quot;#00007F&quot;)) corrplot.mixed(corMatrix, order = &quot;AOE&quot;, lower = &quot;number&quot;, lower.col = &quot;black&quot;, number.cex = .8, upper = &quot;ellipse&quot;, upper.col = col(10), diag = &quot;u&quot;, tl.pos = &quot;lt&quot;, tl.col = &quot;black&quot;) Figure 9.3: Corrplot among numeric variables There are high r values of 0.75, 0.65, 0.52 and 0.42 between the log value of Global.Sales and regional sales, we will consider to use Global.Sales.Log as our target sales to analyze the relationship of sales with other variables later. On the other hand, there are good positive correlation between regional sales too. User Score is positive correlated to Critic Score with r of 0.58. There is little correlation between User Count log value and User Score. plot(hclust(as.dist(1 - cor(as.matrix(st))))) # hierarchical clustering Figure 9.4: Cluster dendrogram for numeric variables All sales’ log value except JP.Sales.Log build one cluster, scores build second cluster, and log value of counts and JP.Sales build another one. In sales cluster, Other.Sales.Log is the closest to Global.Sales.Log, then NA.Sales.Log, and EU.Sales.Log is the next. 9.4 Analysis of score and count library(ggpmisc) #package for function stat_poly_eq formula &lt;- y ~ x p1 &lt;- ggplot(game, aes(x = User.Score, y = Critic.Score)) + geom_point(aes(color = Platform), alpha = .8) + geom_smooth(method = &#39;lm&#39;, se = FALSE, formula = formula) + #add regression line theme(legend.position = &quot;none&quot;) + stat_poly_eq(formula = formula, #add regression equation and R square value eq.with.lhs = &quot;italic(hat(y))~`=`~&quot;, # add ^ on y aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;*plain(\\&quot;,\\&quot;)~&quot;)), label.x.npc = &quot;left&quot;, label.y.npc = 0.9, # position of the equation label parse = TRUE) p2 &lt;- ggplot() + geom_density(data = game, aes(x = Critic.Score), color = &quot;darkblue&quot;, fill = &quot;lightblue&quot;) + geom_density(data = game, aes(x = User.Score), color = &quot;darkgreen&quot;, fill = &quot;lightgreen&quot;, alpha=.5) + labs(x = &quot;Critic.Score-blue, User.Score-green&quot;) + theme(plot.title = element_text(hjust = 0.5)) library(gridExtra) grid.arrange(p1, p2, nrow = 1, ncol = 2) Figure 9.5: Scatter and density plot for critic score and user score There is positive correlation between Critic.Score and User.Score. In total, Critic score is lower than user score. t.test(game$Critic.Score, game$User.Score) ## ## Welch Two Sample t-test ## ## data: game$Critic.Score and game$User.Score ## t = -6.5463, df = 13629, p-value = 6.108e-11 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2058518 -0.1109834 ## sample estimates: ## mean of x mean of y ## 7.027209 7.185626 T-test with p value of much less than 0.05 let us accept the alternative hypothesis with 95% confidence that there is significant difference in the means of critic score and user score. The mean of critic score is 7.03, and mean of user score is 7.19. p1 &lt;- ggplot(game, aes(x = Critic.Count.Log, y = Critic.Score)) + stat_binhex() + # Bin 2d plane into hexagons scale_fill_gradientn(colours = c(&quot;black&quot;, &quot;red&quot;), name = &quot;Frequency&quot;) # Adding a custom continuous color palette p2 &lt;- ggplot(game, aes(x = User.Count.Log, y = User.Score)) + stat_binhex() + scale_fill_gradientn(colours = c(&quot;black&quot;, &quot;red&quot;), name = &quot;Frequency&quot;) # color legend grid.arrange(p1, p2, nrow = 1, ncol = 2) Figure 9.6: Binhex plot for critic count and user count Critic.Score has a pretty good correlation to Critic.Count.Log, with an r value of 0.41 in the correlation analysis above, though Critic.Count.Log doesn’t have impact over Critic.Score. While User.Score looks like independent on User.Count.Log. Exercise 9.2 Use ggplot2 package to get a scatter plot with smooth line between Global_Sales and NA_Sales. Use plain sentence to explain what you find in the plot. Exercise 9.3 Use density plot of Global_Sales, NA_Sales, EU_Sales, JP_Sales and Other_Sales to illustrate the relationship among these sales. Interpret your plot. 9.5 Analysis of sales 9.5.1 By Year.Release Year.Release &lt;- game$Year.Release counts &lt;- data.frame(table(Year.Release)) p &lt;- game %&gt;% select(Year.Release, Global.Sales) %&gt;% group_by(Year.Release) %&gt;% summarise(Total.Sales = sum(Global.Sales)) q &lt;- cbind.data.frame(p, counts[2]) names(q)[3] &lt;- &quot;count&quot; q$count &lt;- as.numeric(q$count) ggplot(q, aes(x = Year.Release, y = Total.Sales, label = q$count)) + geom_col(fill = &quot;green&quot;) + geom_point(y = q$count * 500000, size = 3, shape = 21, fill = &quot;Yellow&quot; ) + geom_text(y = (q$count + 50) * 500000) + # position of the text: count of games each year theme(axis.text.x = element_text(angle = 90), panel.background = element_rect(fill = &quot;purple&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + scale_x_discrete(&quot;Year.Release&quot;, labels = as.character(Year.Release), breaks = Year.Release) Figure 9.7: Total sales by year labs(title = &quot;Global Sales Each Year&quot;, x = &quot;Year Release&quot;, y = &quot;Global Sales&quot;) ## $x ## [1] &quot;Year Release&quot; ## ## $y ## [1] &quot;Global Sales&quot; ## ## $title ## [1] &quot;Global Sales Each Year&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;labels&quot; We can see from the histogram of total sales that there is very little sales before 1996, only one game was released for each year. For several years between 1996 and 2000 the sales increased slowly. The count of games too. After that there is a big climbing in total sales and the number of released games. The top sales happened in 2008, and the most count games was released in that year too. After that both total sales and count of games sloped down. 9.5.2 By Region library(reshape2) game %&gt;% select(Year.Release, NA.Sales.Log, EU.Sales.Log, JP.Sales.Log, Other.Sales.Log, Global.Sales.Log) %&gt;% melt(id.vars = &quot;Year.Release&quot;) %&gt;% group_by(Year.Release, variable) %&gt;% summarise(total.sales = sum(value)) %&gt;% ggplot(aes(x = Year.Release, y = total.sales, color = variable, group = variable)) + geom_point() + geom_line() + labs(title = &quot;Regional Global Sales Log Distribution Each Year&quot;, x = &quot;Year Release&quot;, y = &quot;Total Sales Log Value&quot;, color = &quot;Region&quot;) + theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90), panel.background = element_rect(fill=&quot;pink&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Figure 9.8: Total sales by region The pattern of log value for these regional sales in those years are similar for Global, North America, Europe, and Others. Japan is much different from them. Same conclusion as cluster analysis. 9.5.3 By Rating game$Rating.type &lt;- as.factor(game$Rating.type) x &lt;- game[, c(6:10)] matplot(t(x), type = &quot;l&quot;, col = rainbow(5)[game$Rating.type]) legend(&quot;center&quot;, levels(game$Rating.type), fill = rainbow(5), cex = 0.8, pt.cex = 1) text(c(1.2, 2, 3, 3.9, 4.8), 80000000, colnames(x)) Figure 9.9: Sales by rating type The figure shows one E game(for everyone) which was sold mainly in North America and Europe produced a sale tale of over 80 millions’ global sale, while North America contributed half of the global sales. We can check the game data and know it’s Wii Sports released in 2006. We also noticed that Mature game is popular in North America(green), which contributed a lot to global sales, Everyone games(red) have good sale in Europe, while Japanese like Teen(purple) and Everyone(red) games. It’s balance in rating for “other” region. 9.5.4 By Genre game %&gt;% select(Year.Release, Global.Sales.Log, Genre) %&gt;% group_by(Year.Release, Genre) %&gt;% summarise(Total.Sales.Log = sum(Global.Sales.Log)) %&gt;% ggplot(aes(x = Year.Release, y = Total.Sales.Log, group = Genre, fill = Genre)) + geom_area() + theme(legend.position = &quot;right&quot;, axis.text.x = element_text(angle = 90), panel.background = element_rect(fill = &quot;blue&quot;), panel.grid.major = element_blank(), panel.grid.minor=element_blank()) + theme(plot.title = element_text(hjust = 0.5)) Figure 9.10: Year wise log global sales by Genre The figure shows the golden year for games are from 2007 to 2009, these games together occur above 7000 total.sales.log each of those years. Action and sports keeps on the top sale for almost all of those 20 years, occupying biggest portion of the total global sales log. Adventure, Puzzle and Strategy are on the bottom of the sale log list. 9.5.5 by Score p1 &lt;- ggplot(game, aes(x = Critic.Score, y = Global.Sales.Log)) + geom_point(aes(color = Genre)) + geom_smooth() p2 &lt;- ggplot(game, aes(x = User.Score, y = Global.Sales.Log)) + geom_point(aes(color = Rating)) + geom_smooth() grid.arrange(p1, p2, nrow = 1,ncol = 2) Figure 9.11: Global sales by critic and user score Independent from Genre and Rating, the higher of Score, the better of Global.Sales.Log. Especially for Critic.Score bigger than 9, Global.Sales straight rising. Global.Sales rise slowly with User.Score. game$Name &lt;- gsub(&quot;Brain Age: Train Your Brain in Minutes a Day&quot;, #shorten the game name &quot;Brain Age: Train Your Brain&quot;, game$Name) p1 &lt;- game %&gt;% select(Name, User.Score, Critic.Score, Global.Sales) %&gt;% group_by(Name) %&gt;% summarise(Total.Sales = sum(Global.Sales), Avg.User.Score = mean(User.Score), Avg.Critic.Score = mean(Critic.Score)) %&gt;% arrange(desc(Total.Sales)) %&gt;% head(20) ggplot(p1, aes(x = factor(Name, levels = Name))) + geom_bar(aes(y = Total.Sales/10000000), stat = &quot;identity&quot;, fill = &quot;green&quot;) + geom_line(aes(y = Avg.User.Score, group = 1, colour = &quot;Avg.User.Score&quot;), size = 1.5) + geom_point( aes(y = Avg.User.Score), size = 3, shape = 21, fill = &quot;Yellow&quot; ) + geom_line(aes(y = Avg.Critic.Score, group = 1, colour = &quot;Avg.Critic.Score&quot;), size = 1.5) + geom_point(aes(y = Avg.Critic.Score), size = 3, shape = 21, fill = &quot;white&quot;) + theme(axis.text.x = element_text(angle = 90, size = 8)) + labs(title = &quot;Top Global Sales Game with Score&quot;, x = &quot;Name of the top games&quot; ) + theme(plot.title = element_text(hjust = 0.5)) Among these 20 top sale games, the first two games, Wii Sports and Grand Theft Auto V have much better sales than the others. For most games, average critic score is higher than average user score, which agree with our density plot Figure9.5. Two Call of Duty games got really lower average user score comparing with other top sales games. 9.5.6 By Rating &amp; Genre &amp; Critic score p1 &lt;- game %&gt;% select(Rating.type, Global.Sales, Genre, Critic.Score) %&gt;% group_by(Rating.type, Genre) %&gt;% summarise(Total.Sales = sum(Global.Sales)/10^8, Avg.Score = mean(Critic.Score)) p2 &lt;- p1 %&gt;% group_by(Genre) %&gt;% summarise(Avg.Critic.Score = mean(Avg.Score)) ggplot() + geom_bar(data = p1, aes(x = Genre, y = Total.Sales, fill = Rating.type), stat = &quot;Identity&quot;, position = &quot;dodge&quot;) + geom_line(data = p2, aes(x = Genre, y = Avg.Critic.Score, group = 1, color = &quot;Avg.Critic.Score&quot;), size = 2) + geom_point(data = p2, aes(x = Genre, y = Avg.Critic.Score, shape = &quot;Avg.Critic.Score&quot;), size = 3, color = &quot;Blue&quot;) + scale_colour_manual(&quot;Score&quot;, breaks = &quot;Avg.Critic.Score&quot;, values = &quot;yellow&quot;) + scale_shape_manual(&quot;Score&quot;, values = 19) + theme(axis.text.x = element_text(angle = 90), plot.title = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, panel.background = element_rect(fill = &quot;black&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Figure 9.12: Total sales for genre and rating with critic score For genre &amp; rating &amp; Global.sale combination, Everyone sports game are so popular that it occupy the first in global sales in this group. Rating Mature contribute big portion in both Action and Shooter global sales. On average these three top sales genres show relatively higher critic score. Fighting, adventure and racing games got relatively lower average critic score. We can also see from the figure that adventure, puzzle and stratage do sell less comparing with other genres. 9.5.7 By Platform library(viridis) ## Loading required package: viridisLite library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:viridis&#39;: ## ## viridis_pal ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor p &lt;- game %&gt;% group_by(Platform.type, Year.Release) %&gt;% summarise(total = sum(Global.Sales)) p$Year.Release. &lt;- as.numeric(as.character(p$Year.Release)) ggplot(p, aes(x = Year.Release., fill = Platform.type)) + geom_density(position = &quot;fill&quot;) + labs(y = &quot;Market Share&quot;) + theme(plot.title = element_text(hjust = 0.5)) + scale_fill_viridis(discrete = TRUE) + scale_y_continuous(labels = percent_format()) Figure 9.13: Yearly market share by platform type Nintendo and Xbox came after 1990. Before that PC and Playstation occupied the game market, PC are the main platform at that time. After 1995, the portion of PC and Playstation shrinked, while Nintendo and Xbox grew fast and took over more portion than Playstation and PC in the market. Together with Nintendo and Xbox, there were other game platform sprouting out in early 1990s, but they last for 20 years and disappeared. From around 2010, the portions of these 4 platforms keep relatively evenly and stablly. #compute 1-way ANOVA test for log value of global sales by Platform Type model &lt;- aov(Global.Sales.Log ~ Platform.type, data = game) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Platform.type 4 1283 320.8 181.2 &lt;2e-16 *** ## Residuals 6820 12077 1.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 tukey &lt;- TukeyHSD(model) par(mar = c(4, 10, 2, 1)) plot(tukey, las = 1) ANOVA test shows that there is at lease one of the platform type is significant different from the others. In detail, the plot of Turkey tests tells us that there is significant difference between all other pairs of platform types but between Xbox and Nintendo, others and Nintendo. game$Platform.type &lt;- as.factor(game$Platform.type) ggplot(game, aes(x = Platform.type, y = Global.Sales.Log, fill = Rating.type)) + geom_boxplot() Figure 9.14: Global sales log by platform and rating type In total, PC has lower Global sales log comparing with other platform type, while Playstation and Xbox have higher sale mediums for different rating types. Rating of Everyone sold pretty well in all platform type, while rating Mature sold better in PC, Playstation and Xbox. ggplot(game, aes(Critic.Score, Global.Sales.Log, color = Platform.type)) + geom_point() + facet_wrap(~ Genre) + theme(plot.title = element_text(hjust = 0.5)) Figure 9.15: Global sales log by critic score for different platform type and genre Most genre plots in Figure 9.15 illustrate that there are positive correlation between Global.Sales.Log and Critic Score, the higher the critic score, the better the global sales log value. Most puzzle games were from Nintendo, while lots of stratage games are PC. For other genres, all platforms shared the portion relatively evenly. Lots of PC(green) shared lower market portion in different genres, while some of Nintendo(red) games in sports, racing, platform, and misc were sold really well. At the same time, Playstation action and racing games, and Xbox misc, action and shooter games show higher global sales log too. 9.6 Effect of platform type to priciple components st &lt;- game[, c(11, 13, 17:23)] pca = prcomp(st, scale = T) #scale = T to normalize the data percentVar &lt;- round(100 * summary(pca)$importance[2, 1:3], 0) # compute % variances #head(pca$x) # the new coordinate values for each observation pcaData &lt;- as.data.frame(pca$x[, 1:2]) #First and Second principal component value pcaData &lt;- cbind(pcaData, game$Platform.type) #add platform type as third col. for cluster purpose colnames(pcaData) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;Platform&quot;) ggplot(pcaData, aes(PC1, PC2, color = Platform, shape = Platform)) + geom_point(size = 0.8) + xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + # x label ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) + # y label theme(aspect.ratio = 1) # width and height ratio Figure 9.16: PCA plot colored with platform type PC, Xbox, Playstation and Nintendo occupy in their own positions in the PCA figure, which illustrate that they play different important role in components of the variance of PC1 and PC2. library(ggfortify) set.seed(1) autoplot(kmeans(st, 3), data = st, label = FALSE, label.size = 0.1) Figure 9.17: Kmeans PCA figure using ggfortify Together with PCA Figure 9.16, we will find that the first cluster is contributed mainly by PC. The second cluster is contributed mainly by Xbox and Playstation. Xbox, Nintendo, and Playstation together build the third cluster. 9.7 Models for global sales Because there are too many of levels in Publisher and Developer, and there is apparent correlation between them, we use only top 12 levels of Publisher and classified the other publishers as “Others”; Because of the good correlation between Critic.Score and User.Score, we use only critic score; Also we use only log value of user score count because of it’s closer correlation to global sales log. We will not put other sales log variables in our model because their apparent correlation with global sales log. #re-categorize publisher into 13 groups Publisher. &lt;- head(names(sort(table(game$Publisher), decreasing = TRUE)), 12) game &lt;- game %&gt;% mutate(Publisher.type = ifelse(Publisher %in% Publisher., as.character(Publisher), &quot;Others&quot;)) game.lm &lt;- game[, c(3:4, 11, 21, 23:26)] #game.log$Genre.type &lt;- as.factor(game.log$Genre.type) #game.lm$Publisher.type &lt;- as.factor(game.lm$Publisher.type) model &lt;- lm(Global.Sales.Log ~ ., data = game.lm) summary(model) ## ## Call: ## lm(formula = Global.Sales.Log ~ ., data = game.lm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9221 -0.5558 0.0254 0.5789 4.1009 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 10.187752 0.922104 11.048 ## Year.Release1988 -3.088268 1.300919 -2.374 ## Year.Release1992 -1.796072 1.300851 -1.381 ## Year.Release1994 2.930510 1.302000 2.251 ## Year.Release1996 1.488582 0.987461 1.507 ## Year.Release1997 1.188658 0.955613 1.244 ## Year.Release1998 0.744742 0.939532 0.793 ## Year.Release1999 0.653088 0.936406 0.697 ## Year.Release2000 0.785921 0.925467 0.849 ## Year.Release2001 0.772488 0.922728 0.837 ## Year.Release2002 0.570865 0.922008 0.619 ## Year.Release2003 0.340110 0.921843 0.369 ## Year.Release2004 0.550812 0.921901 0.597 ## Year.Release2005 0.210534 0.921694 0.228 ## Year.Release2006 0.141533 0.921703 0.154 ## Year.Release2007 0.373361 0.921629 0.405 ## Year.Release2008 0.529829 0.921677 0.575 ## Year.Release2009 0.446508 0.921648 0.484 ## Year.Release2010 0.596658 0.921889 0.647 ## Year.Release2011 0.389514 0.921944 0.422 ## Year.Release2012 -0.031458 0.922540 -0.034 ## Year.Release2013 -0.170252 0.922985 -0.184 ## Year.Release2014 -0.383671 0.923096 -0.416 ## Year.Release2015 -0.478979 0.923302 -0.519 ## Year.Release2016 -0.826950 0.923192 -0.896 ## GenreAdventure -0.335989 0.063305 -5.307 ## GenreFighting 0.072835 0.055338 1.316 ## GenreMisc 0.357196 0.054323 6.575 ## GenrePlatform -0.184139 0.054854 -3.357 ## GenrePuzzle -0.345253 0.091582 -3.770 ## GenreRacing -0.097943 0.048451 -2.021 ## GenreRole-Playing -0.357837 0.043093 -8.304 ## GenreShooter -0.142624 0.040372 -3.533 ## GenreSimulation 0.405629 0.060887 6.662 ## GenreSports 0.011217 0.046986 0.239 ## GenreStrategy -0.483702 0.063838 -7.577 ## Critic.Score 0.114744 0.009865 11.632 ## User.Count.Log 0.615776 0.011589 53.134 ## Platform.typeOthers -0.188799 0.066011 -2.860 ## Platform.typePC -2.442802 0.051704 -47.246 ## Platform.typePlaystation 0.135254 0.033868 3.994 ## Platform.typeXbox -0.106733 0.036927 -2.890 ## Rating.typeE10+ -0.058393 0.040759 -1.433 ## Rating.typeM -0.467923 0.045704 -10.238 ## Rating.typeOthers 0.144593 0.545092 0.265 ## Rating.typeT -0.331696 0.035692 -9.293 ## Publisher.typeAtari -0.506235 0.082442 -6.141 ## Publisher.typeCapcom -0.759855 0.078983 -9.620 ## Publisher.typeElectronic Arts -0.114754 0.053315 -2.152 ## Publisher.typeKonami Digital Entertainment -0.705932 0.073042 -9.665 ## Publisher.typeNamco Bandai Games -0.569621 0.074819 -7.613 ## Publisher.typeNintendo 0.111592 0.074409 1.500 ## Publisher.typeOthers -0.618870 0.046433 -13.328 ## Publisher.typeSega -0.517708 0.069222 -7.479 ## Publisher.typeSony Computer Entertainment -0.585506 0.069125 -8.470 ## Publisher.typeTake-Two Interactive -0.276947 0.070848 -3.909 ## Publisher.typeTHQ -0.117032 0.068824 -1.700 ## Publisher.typeUbisoft -0.377192 0.059101 -6.382 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## Year.Release1988 0.017628 * ## Year.Release1992 0.167420 ## Year.Release1994 0.024432 * ## Year.Release1996 0.131733 ## Year.Release1997 0.213591 ## Year.Release1998 0.427996 ## Year.Release1999 0.485550 ## Year.Release2000 0.395791 ## Year.Release2001 0.402522 ## Year.Release2002 0.535836 ## Year.Release2003 0.712180 ## Year.Release2004 0.550211 ## Year.Release2005 0.819326 ## Year.Release2006 0.877964 ## Year.Release2007 0.685409 ## Year.Release2008 0.565410 ## Year.Release2009 0.628070 ## Year.Release2010 0.517516 ## Year.Release2011 0.672679 ## Year.Release2012 0.972799 ## Year.Release2013 0.853660 ## Year.Release2014 0.677690 ## Year.Release2015 0.603940 ## Year.Release2016 0.370418 ## GenreAdventure 1.15e-07 *** ## GenreFighting 0.188157 ## GenreMisc 5.22e-11 *** ## GenrePlatform 0.000793 *** ## GenrePuzzle 0.000165 *** ## GenreRacing 0.043269 * ## GenreRole-Playing &lt; 2e-16 *** ## GenreShooter 0.000414 *** ## GenreSimulation 2.91e-11 *** ## GenreSports 0.811327 ## GenreStrategy 4.01e-14 *** ## Critic.Score &lt; 2e-16 *** ## User.Count.Log &lt; 2e-16 *** ## Platform.typeOthers 0.004248 ** ## Platform.typePC &lt; 2e-16 *** ## Platform.typePlaystation 6.58e-05 *** ## Platform.typeXbox 0.003859 ** ## Rating.typeE10+ 0.152006 ## Rating.typeM &lt; 2e-16 *** ## Rating.typeOthers 0.790814 ## Rating.typeT &lt; 2e-16 *** ## Publisher.typeAtari 8.69e-10 *** ## Publisher.typeCapcom &lt; 2e-16 *** ## Publisher.typeElectronic Arts 0.031403 * ## Publisher.typeKonami Digital Entertainment &lt; 2e-16 *** ## Publisher.typeNamco Bandai Games 3.04e-14 *** ## Publisher.typeNintendo 0.133735 ## Publisher.typeOthers &lt; 2e-16 *** ## Publisher.typeSega 8.44e-14 *** ## Publisher.typeSony Computer Entertainment &lt; 2e-16 *** ## Publisher.typeTake-Two Interactive 9.36e-05 *** ## Publisher.typeTHQ 0.089092 . ## Publisher.typeUbisoft 1.86e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9177 on 6767 degrees of freedom ## Multiple R-squared: 0.5735, Adjusted R-squared: 0.5699 ## F-statistic: 159.6 on 57 and 6767 DF, p-value: &lt; 2.2e-16 model &lt;- aov(Global.Sales.Log ~ ., data = game.lm) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year.Release 24 521 21.7 25.78 &lt;2e-16 *** ## Genre 11 527 47.9 56.87 &lt;2e-16 *** ## Critic.Score 1 1771 1771.3 2103.34 &lt;2e-16 *** ## User.Count.Log 1 1344 1344.0 1595.92 &lt;2e-16 *** ## Platform.type 4 2944 736.1 874.11 &lt;2e-16 *** ## Rating.type 4 165 41.2 48.93 &lt;2e-16 *** ## Publisher.type 12 389 32.4 38.53 &lt;2e-16 *** ## Residuals 6767 5699 0.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Global sales log is mostly effected by factors of critic score, user count log, platform type, Publisher type and genre in glm analysis. ANOVA shows every factors are in the contribution to global sales log, critic score and user count log are the most important factors. Critic score and User.Count.Log positively affect the global sales log, while other factors like Platform type and Genre either lift up or pull down the global sales according to their types. This model will explain the global sales log with R-Square of 0.57. Because of the curve smooth line at global sale ~ critic score plot in our previous analysis and its big contribution in linear model analysis, We try a polynomial fit of critic score only. The first two levels are not statistically significant according to our pre-analysis, we use the third and fourth levels only here. model &lt;- lm(Global.Sales.Log ~ I(Critic.Score^3) + I(Critic.Score^4), data = game.lm) summary(model) ## ## Call: ## lm(formula = Global.Sales.Log ~ I(Critic.Score^3) + I(Critic.Score^4), ## data = game.lm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8634 -0.7892 0.0950 0.8837 5.5807 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.193e+01 7.115e-02 167.722 &lt; 2e-16 *** ## I(Critic.Score^3) -2.989e-03 7.972e-04 -3.749 0.000179 *** ## I(Critic.Score^4) 6.076e-04 8.224e-05 7.387 1.67e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.283 on 6822 degrees of freedom ## Multiple R-squared: 0.1597, Adjusted R-squared: 0.1595 ## F-statistic: 648.3 on 2 and 6822 DF, p-value: &lt; 2.2e-16 In total, the coefficients are statistically significant, the model of two levels of critic score itself will explain the data with R square 0.16. ModelFunc &lt;- function(x) {model$coefficients[1] + x^3*model$coefficients[2] + x^4*model$coefficients[3]} ggplot(data = game.lm, aes(x = Critic.Score, y = Global.Sales.Log)) + geom_point() + stat_function(fun = ModelFunc, color = &#39;blue&#39;, size = 1) Here is the scatter plot of Global.Sales.Log ~ Critic Score and the model line which predict the global sales log with critic score. Exercise 9.4 Use different plots to visualize the distribution of NA_Sales by Year_of_Release, Genre, Rating and Platform individually or combinedly. Explain the relationship between NA_Sales with these factors. Hint: Take log value for NA_Sales first. It’s better to regroup platform first. Exercise 9.5 What’s the correlation between NA_Sales and Critic_Score? Use scatter plot with smooth line or polynomial model line to show the trend. Give your interpretation. Exercise 9.6 Use linear model and ANOVA to analyze NA_Sales with all the factors which contribute to its variance. Interpret your result breifly. Hint: Check the corrplot in Figure 9.3 and pay attention to the high correlation among those sales and between those scores. 9.8 Conclusion Global and regional sales are not distributed normally, while their log values are close to normal distribution. Most regional sales have the similar pattern as global sales. There is positive correlation between critic score and user score. In total, Critic score is lower than user score. No apparent correlation was found between scores and their counts. Critic score, user score count log, genre, rating, platform, and publisher together affect the global sales log. Critic score is the most important contributors. "],
["exploring-of-employee-salary-data-set.html", "Chapter 10 Exploring of employee salary data set 10.1 Read in and clean data 10.2 Information about the variables 10.3 Analysis of gross income 10.4 Analysis of gross income type 10.5 LM and ANOVA analysis 10.6 Conclusion", " Chapter 10 Exploring of employee salary data set This original data set is taken from the Payroll Data from online “https://data.ct.gov/Government/State-Employee-Payroll-Data-Calendar-Year-2015-thr/virr-yb6n/data”, Which include Calendar Year 2015 through the most recent pay period for Connecticut state employees. We take the subset only for 2017 fiscal year, which include the information of more than 1.77 million observations and 38 variables. There are 102101 employees listed there. For convenience of analysis, we randomly took 8500 employees from the original data. For our sampled data set (“state_empoyee_salary_data_2017.csv”), we are interested in the relationship between income with other variables like age, sex, ethnic, union and so on. We will focus on the full time employees who got 26 pay checks in 2017 fiscal year. First we will clean our sampled data set. 10.1 Read in and clean data raw0 &lt;- read.csv(&quot;datasets/State_Employee_Payroll_Data_Calendar_Year_2015_through_Present.csv&quot;) str(raw0) employID &lt;- unique(raw0$EmplId.Empl.Rcd) sample &lt;- sample(employID, 8500) raw &lt;- raw0[raw0$EmplId.Empl.Rcd %in% sample, ] %&gt;% droplevels() str(raw) write.csv(raw, file = &quot;datasets/state_employee_salary_data_2017.csv&quot;, row.names = FALSE) From the sampled data set, We will pick full time employees as our goal and focus on the interested variables. library(dplyr) raw &lt;- read.csv(&quot;datasets/state_employee_salary_data_2017.csv&quot;) raw1 &lt;- filter(raw, Full.Part == &quot;F&quot;) # pick full time employee raw2 &lt;- raw1[, -c(1:2, 5:9, 11:12, 19:23, 31:33, 35:36)] # keep only the interested variables We remove the variables we are not interested in. Now we will check the situation of employees. EmplId.Empl.Rcd &lt;- as.character(raw2$EmplId.Empl.Rcd) counts &lt;- data.frame(table(EmplId.Empl.Rcd)) PersonOccurCount &lt;- table(counts[, 2]) plot(PersonOccurCount, col = rainbow(30), xlab = &quot;Occurance of employee&quot;, ylab = &quot;Count of employee&quot;) Figure 10.1: Situation of check occurance of employees The most employees occurrence is 26. We will focus on the employees who have 26 paychecks for whole year in 2017. subEmpl26 &lt;- counts[which(counts$Freq != 26), ] len &lt;- length(subEmpl26$EmplId.Empl.Rcd) for(i in 1:len){ j &lt;- which(raw2$EmplId.Empl.Rcd == as.character(subEmpl26$EmplId.Empl.Rcd)[i]) if(i == 1) id = j else if(i &gt; 1) id = c(id, j) } raw26 &lt;- raw2[-id, ] # remove employees with check count not equal 26 We removed employees with check count unequal to 26. Check.Dt &lt;- gsub(&quot; 12:00:00 AM&quot;, &quot;&quot;, raw26$Check.Dt) counts &lt;- table(Check.Dt) barplot(counts, col = rainbow(26), axis.lty = 1, xlab = &quot;Check Date&quot;, ylab = &quot;Count&quot;) Figure 10.2: The counts of checks for check date in 2017 The counts of checks for bi week almost the same, which means the employees kept same all the time in 2017 except 03/08/2017. We will remove the employee in this check date. empl38 &lt;- raw26[which(raw26$Check.Dt == &quot;03/08/2017 12:00:00 AM&quot;), ] raw26 &lt;- raw26[-which(raw26$EmplId.Empl.Rcd == empl38$EmplId.Empl.Rcd), ] We check the situation whether different genders are indexed for same employee. df &lt;- raw26 %&gt;% group_by(EmplId.Empl.Rcd, Sex) %&gt;% summarise(Total.Gross = sum(Tot.Gross )) dupliSex &lt;- df[duplicated(df$EmplId.Empl.Rcd), ] ## Warning: Factor `EmplId.Empl.Rcd` contains implicit NA, consider using ## `forcats::fct_explicit_na` dupliSex ## Warning: Factor `EmplId.Empl.Rcd` contains implicit NA, consider using ## `forcats::fct_explicit_na` ## # A tibble: 0 x 3 ## # Groups: EmplId.Empl.Rcd [1] ## # ... with 3 variables: EmplId.Empl.Rcd &lt;fct&gt;, Sex &lt;fct&gt;, ## # Total.Gross &lt;dbl&gt; There is no employee having different sex recorded. We will check whether sex “U” employee is still in our data raw26[which(raw26$Sex == &quot;U&quot;), ] ## [1] DeptID EmplId.Empl.Rcd Check.Dt ## [4] Annnual.Rate Bi.Weekly.Comp.Rate Other ## [7] Fringe Overtime Salaries...Wages ## [10] Tot.Gross Age Job.Cd.Descr ## [13] EE.Class.Descr Job.Indicator Ethnic.Grp ## [16] Sex City Union.Descr ## [19] Agency ## &lt;0 rows&gt; (or 0-length row.names) sex “U” employees are already removed. Then we will check about the duplication situation of ethnic group. df &lt;- raw26 %&gt;% group_by(EmplId.Empl.Rcd, Ethnic.Grp) %&gt;% summarise(Total.Gross = sum(Tot.Gross )) dupliEthnic.Grp &lt;- df[duplicated(df$EmplId.Empl.Rcd), ] ## Warning: Factor `EmplId.Empl.Rcd` contains implicit NA, consider using ## `forcats::fct_explicit_na` dupliEthnic.Grp ## Warning: Factor `EmplId.Empl.Rcd` contains implicit NA, consider using ## `forcats::fct_explicit_na` ## # A tibble: 0 x 3 ## # Groups: EmplId.Empl.Rcd [1] ## # ... with 3 variables: EmplId.Empl.Rcd &lt;fct&gt;, Ethnic.Grp &lt;fct&gt;, ## # Total.Gross &lt;dbl&gt; There is no different ethnic recorder for the same employee. Now we notice that some employees even have gross income or biweekly rate less then zero. We consider to remove those employees. salary &lt;- raw26 subGross0 &lt;- salary[which(salary$Tot.Gross &lt;= 0 | salary$Bi.Weekly.Comp.Rate &lt;= 0), ] len &lt;- length(unique(subGross0$EmplId.Empl.Rcd)) for(i in 1:len){ j &lt;- which(salary$EmplId.Empl.Rcd == unique(subGross0$EmplId.Empl.Rcd)[i]) if(i == 1) id = j else if(i &gt; 1) id = c(id, j) } salary &lt;- salary[-id, ] #remove employees with negative or zero Tot.Gross and Bi.Weekly.Comp.Rate We removed employees with negative or zero Tot.Gross and biweekly rate too. Now let’s check the distribution of total gross. plot(salary$Tot.Gross, ylab = &quot;Total gross income&quot;) Figure 10.3: Scatterplot of total gross income Most total gross point scatter under $20000, some of them are even above $60000. We will not keep those employees with Job.Indicator as “S” too. subIndicator &lt;- salary[which(salary$Job.Indicator == &quot;S&quot;), ] len &lt;- length(unique(subIndicator$EmplId.Empl.Rcd)) for(i in 1:len){ j &lt;- which(salary$EmplId.Empl.Rcd == unique(subIndicator$EmplId.Empl.Rcd)[i]) if(i == 1) id = j else if(i &gt; 1) id = c(id, j) } salary &lt;- salary[-id, ] #remove employees with Job indicator as &quot;S&quot; Employees with Job indicator as “S” are moved. We also will not consider those employees with duplicated union descriptions. df &lt;- salary %&gt;% group_by(EmplId.Empl.Rcd, Union.Descr) %&gt;% summarise(Total.Gross = sum(Tot.Gross )) dupliUnion &lt;- df[duplicated(df$EmplId.Empl.Rcd), ] len &lt;- length(dupliUnion$EmplId.Empl.Rcd) for(i in 1:len){ j &lt;- which(salary$EmplId.Empl.Rcd == dupliUnion$EmplId.Empl.Rcd[i]) if(i == 1) id = j else if(i &gt; 1) id = c(id, j) } salary &lt;- salary[-id, ] # remove employees with different Union descreption Finally, we cleaned our data according to our interest and we will group our left observations according to employee ID “EmplId.Empl.Rcd”, and just select variables like sex, ethnic group, union description, age, total gross and biweekly rate. We take round age for each employee. salary &lt;- salary %&gt;% group_by(EmplId.Empl.Rcd, Sex, Ethnic.Grp, Union.Descr) %&gt;% summarise(Age = round(mean(Age), 0), Tot.Gross = mean(Tot.Gross), Bi.Weekly.Rate = mean(Bi.Weekly.Comp.Rate)) %&gt;% group_by(EmplId.Empl.Rcd) %&gt;% filter(n() == 1) %&gt;% droplevels() str(data.frame(salary)) ## &#39;data.frame&#39;: 3441 obs. of 7 variables: ## $ EmplId.Empl.Rcd: Factor w/ 3441 levels &quot;00034CF9004E4E0D7872FEB52CB5933F&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 2 1 1 2 2 1 1 2 ... ## $ Ethnic.Grp : Factor w/ 8 levels &quot;&quot;,&quot;AMIND&quot;,&quot;ASIAN&quot;,..: 8 8 8 8 8 8 6 5 8 8 ... ## $ Union.Descr : Factor w/ 54 levels &quot;Admin and Residual (P-5)&quot;,..: 9 2 7 24 24 43 35 41 37 44 ... ## $ Age : num 73 64 65 54 50 46 71 47 52 48 ... ## $ Tot.Gross : num 4176 1799 1935 2751 2195 ... ## $ Bi.Weekly.Rate : num 3995 1776 1931 2713 2195 ... There is one level marked as “” in ethnic group, which means unknown for us. levels(salary$Ethnic.Grp)[levels(salary$Ethnic.Grp) == &quot;&quot;] &lt;- &quot;unknown&quot; We replace level of empty space in Ethnic.Grp as “unknown”. str(data.frame(salary)) ## &#39;data.frame&#39;: 3441 obs. of 7 variables: ## $ EmplId.Empl.Rcd: Factor w/ 3441 levels &quot;00034CF9004E4E0D7872FEB52CB5933F&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 2 1 1 2 2 1 1 2 ... ## $ Ethnic.Grp : Factor w/ 8 levels &quot;unknown&quot;,&quot;AMIND&quot;,..: 8 8 8 8 8 8 6 5 8 8 ... ## $ Union.Descr : Factor w/ 54 levels &quot;Admin and Residual (P-5)&quot;,..: 9 2 7 24 24 43 35 41 37 44 ... ## $ Age : num 73 64 65 54 50 46 71 47 52 48 ... ## $ Tot.Gross : num 4176 1799 1935 2751 2195 ... ## $ Bi.Weekly.Rate : num 3995 1776 1931 2713 2195 ... summary(salary) ## EmplId.Empl.Rcd Sex Ethnic.Grp ## 00034CF9004E4E0D7872FEB52CB5933F: 1 F:1804 WHITE :2101 ## 001D621CF1C85C8F46EFD935CD685BFD: 1 M:1637 BLACK : 485 ## 004C3FFCCFC10A09D65CA3DF60CB69FD: 1 HISPA : 283 ## 0059143835BC7A37482ABF4B933A4642: 1 NSPEC : 236 ## 0059CD55728E16BC6BCFD35EDBA9DA32: 1 unknown: 211 ## 007CE08D82395FFD546943D382911009: 1 ASIAN : 109 ## (Other) :3435 (Other): 16 ## Union.Descr Age Tot.Gross ## Service/Maintenance (NP-2) : 299 Min. :22.00 Min. : 568.7 ## Correctional Officers (NP-4) : 289 1st Qu.:40.00 1st Qu.: 2414.2 ## Social and Human Services(P-2): 263 Median :49.00 Median : 3041.0 ## Administrative Clerical (NP-3): 252 Mean :47.71 Mean : 3222.5 ## Admin and Residual (P-5) : 214 3rd Qu.:56.00 3rd Qu.: 3754.7 ## Health Professional (P-1) : 198 Max. :81.00 Max. :17530.3 ## (Other) :1926 ## Bi.Weekly.Rate ## Min. : 25.78 ## 1st Qu.: 2145.94 ## Median : 2656.14 ## Mean : 2894.14 ## 3rd Qu.: 3443.95 ## Max. :12884.62 ## Now 3441 employees are included in our data set. These employees include both male and female, aged from 22 to 81, group into 8 different ethnics. They belong to 54 different Unions. The minimum total gross is $568.7 and maximum is $17530.3. Biweekly rate range from $25.78 to $12884.62. As for this salary data, we will focus on the total gross income of these state employees. We are interested in the points such as: How about the distribution of the total gross income? Are male and female paid equally? Is there any difference in total gross for different age and age group? Do some ethnic group get better pay than the others? What’s the most important factor effect the gross income for these state employees? From previous plots of total gross, we need to transform total gross first. salary$Gross.Log &lt;- log(salary$Tot.Gross) #write.csv(salary, file = &quot;datasets/salary.csv&quot;, row.names = FALSE) #salary &lt;- read.csv(&quot;datasets/salary.csv&quot;) 10.2 Information about the variables 10.2.1 Income and age variable &lt;- c(&quot;Tot.Gross&quot;, &quot;Gross.Log&quot;, &quot;Age&quot;) # pick up the numeric columns according to the names par(mfrow = c(3, 2)) # layout in 3 rows and 3 columns for (i in 1:length(variable)){ sub &lt;- unlist(salary[variable[i]]) submean &lt;- mean(sub) hist(sub, main = paste(&quot;Hist. of&quot;, variable[i], sep = &quot; &quot;), xlab = variable[i]) abline(v = submean, col = &quot;blue&quot;, lwd = 1) qqnorm(sub, main = paste(&quot;Q-Q Plot of&quot;, variable[i], sep = &quot; &quot;)) qqline(sub) if (i == 1) {s.t &lt;- shapiro.test(sub) } else {s.t &lt;- rbind(s.t, shapiro.test(sub)) } } Figure 10.4: Histogram and QQ plot for total gross, log of gross and age s.t &lt;- s.t[, 1:2] # take first two columns of shapiro.test result s.t &lt;- cbind(variable, s.t) # add variable name for the result s.t ## variable statistic p.value ## s.t &quot;Tot.Gross&quot; 0.840075 2.490722e-50 ## &quot;Gross.Log&quot; 0.9888379 7.88885e-16 ## &quot;Age&quot; 0.9848966 1.023745e-18 Tot gross is not statistically normally distributed. It’s extremely left skewed distributed. The distribution of Gross.Log, transformation of Tot.Gross is closer to normality than Tot.Gross. The highest frequency of Gross.Log is round 8. Highest frequency employees are aged between 45 and 50 years old. The distribution is close to bell shape. Shapiro tests deny the normality of all these three numeric variables. 10.2.2 Ethnic group library(ggplot2) counts &lt;- data.frame(sort(table(salary$Ethnic.Grp), decreasing = TRUE)) perc &lt;- paste(round(counts$Freq/sum(counts$Freq), 2)*100, &quot;%&quot;, sep = &quot;&quot;) ggplot(counts, aes(x = reorder(Var1, Freq), y = Freq)) + geom_bar(stat = &#39;identity&#39;, fill = rainbow(8)) + geom_text(aes(x = c(8:1), y = Freq + 100 , label = perc), size = 3.5) + labs(x = &#39;Ethnic Group&#39;, y = &#39;Freqency&#39;, title = &#39;Ethnic group by count&#39;) + theme_bw() + # classic dark-on-light coord_flip() + # flip the plot theme(plot.title = element_text(hjust = 0.5)) 61% of state employees are white, 14% are black. AMIND and PACIF are very limited. 10.2.3 Sex library(plotrix) ## ## Attaching package: &#39;plotrix&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## rescale ## The following object is masked from &#39;package:gplots&#39;: ## ## plotCI counts &lt;- sort(table(salary$Sex), decreasing = TRUE) #labels with count number and percentage p &lt;- paste(round(counts/sum(counts), 2)*100, &quot;%&quot;, sep = &quot;&quot;) lbls &lt;- paste(names(counts), &quot;\\n&quot;, counts, p, sep = &quot; &quot;) pie3D(counts, labels = lbls, explode = 0.05, main = &quot;Pie chart of sex&quot;, labelcex = 1.0, labelpos=c(1.8, 5.0)) There are around 4% more female than male in the state employees. 10.2.4 Union descreption top10Union &lt;- data.frame(sort(table(salary$Union.Descr), decreasing = TRUE)[1:10]) names(top10Union)[1] &lt;- &quot;Union.Descr&quot; topUnion &lt;- salary %&gt;% group_by(Union.Descr) %&gt;% summarise(Avg.Gross.Log = round(mean(Gross.Log), 3)) %&gt;% arrange(desc(Avg.Gross.Log)) %&gt;% head(10) df &lt;- data.frame(top10Union, topUnion) names(df)[1]&lt;- &quot;Top union by count&quot; names(df)[3]&lt;- &quot;Top union by log of gross&quot; library(knitr) kable(df, booktabs=TRUE, caption = &quot;Top union by count and average log value of gross&quot;) Table 10.1: Top union by count and average log value of gross Top union by count Freq Top union by log of gross Avg.Gross.Log Service/Maintenance (NP-2) 299 UCHC - Faculty - AAUP 8.927 Correctional Officers (NP-4) 289 UCHC - Faculty 8.710 Social and Human Services(P-2) 263 Comm College Mgmt Exclusions 8.693 Administrative Clerical (NP-3) 252 UConn - Law School Faculty 8.605 Admin and Residual (P-5) 214 Amercan Fed of School Admin 8.523 Health Professional (P-1) 198 Connecticut Innovations Inc 8.511 Health NonProfessional (NP-6) 189 Exempt/Elected/Appointed 8.510 Engineer Scien Tech (P-4) 181 StatePoliceLts&amp;Captains (NP-9) 8.473 UCHC Univ Hlth Professionals 121 Managerial 8.416 Managerial 119 Crim Justice Managerial Exempt 8.389 The top unions by count are Service Maintain, Correctional officers, and Social &amp; Human Services. But according to average gross, the first four are all come from university or college. Except Managerial, the top unions by count are totally different from top unions by average gross. 10.3 Analysis of gross income 10.3.1 Age # p1 get bar plot p1 &lt;- salary %&gt;% group_by(Age) %&gt;% summarise(Avg.Gross = mean(Tot.Gross)) %&gt;% ggplot(aes(x = Age, y = Avg.Gross)) + geom_bar(stat = &#39;identity&#39;, col = &quot;lightblue&quot;) + scale_x_continuous(breaks = seq(20, 90, 10)) + labs(y = &quot;Average gross income&quot;) # p2 get density2D plot p2 &lt;- ggplot(salary, aes(x = Age, y = Gross.Log)) + stat_density2d(aes(fill = ..density..^0.25), geom = &quot;tile&quot;, contour = FALSE, n = 200) + scale_fill_continuous(low = &quot;white&quot;, high = &quot;dodgerblue3&quot;) + scale_x_continuous(breaks = seq(10, 100, 10)) + scale_y_continuous(breaks = seq(2, 12, 1)) + theme(legend.position = &quot;none&quot;, panel.background = element_blank()) # display p1 and p2 in same row library(gridExtra) grid.arrange(p1, p2, nrow = 1) Figure 10.5: Average gross and log of gross distribution by age In total, the older the employees, the higher their gross income. Lots of employees between age 50 and 60 have the similar log gross income around 8. Now we separate age into groups with age gap of 10. salary$Age. &lt;- cut(salary$Age, breaks = c(10, 20, 30, 40, 50, 60, 70, 80, 90), right = FALSE) stat &lt;- salary %&gt;% group_by(Age.) %&gt;% summarise(Avg.Gross.Log = mean(Gross.Log)) ggplot() + geom_boxplot(data = salary, aes(x = Age., y = Gross.Log, fill = Age.)) + geom_point(data = stat, aes(x = Age., y = Avg.Gross.Log), color = &quot;brown&quot;) + geom_text(data = stat, aes(label = round(Avg.Gross.Log, 2), x = Age., y = Avg.Gross.Log - 0.1)) + theme(plot.title = element_text(hjust = 0.5)) Figure 10.6: Distribution of log of gross by age group With average 10 years of age increasing, log value of gross increase step by step until after age 80. model &lt;- aov(salary$Gross.Log~salary$Age.) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## salary$Age. 6 33.0 5.500 54.53 &lt;2e-16 *** ## Residuals 3434 346.4 0.101 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 With p value of much less than 0.05, we reject the null hypothesis that there is no significant difference among these means of gross for different age groups. library(agricolae) # LSD.test need provide degrees of freedom and mean square for errors. We can got it from aov. res &lt;- LSD.test(salary$Gross.Log, salary$Age., DFerror = model$df.residual, MSerror=anova(model)[[&quot;Mean Sq&quot;]][2]) res$groups ## salary$Gross.Log groups ## [70,80) 8.284744 a ## [60,70) 8.122599 b ## [50,60) 8.068892 c ## [40,50) 8.032668 d ## [30,40) 7.913289 e ## [20,30) 7.756642 f ## [80,90) 6.343428 g From the information of pairwise comparison, the total different alpha beta index indicates that these 8 age groups are significantly different from each other in mean gross. The elder the age, the higher the gross income, except for age group between 80 and 90, its mean gross is the lowest comparing with all other age group. library(scales) library(plotly) p &lt;- salary %&gt;% group_by(Union.Descr, Age) %&gt;% summarise(Avg.Gross = mean(Gross.Log)) %&gt;% arrange(desc(Avg.Gross)) %&gt;% ggplot(aes(x = Age, y = Avg.Gross, fill = Union.Descr)) + geom_bar(stat = &quot;identity&quot;, position = &quot;fill&quot;) + scale_y_continuous(labels = percent_format()) + scale_x_continuous(breaks = seq(10, 100, 10)) + labs(y = &quot;Percentage&quot;) + theme(panel.background = element_rect(fill = &quot;black&quot;), panel.grid.major = element_blank(), panel.grid.minor=element_blank()) ggplotly(p) Figure 10.7: Percentage of union descriptions by age group This is the distribution of average gross percentage of Union Descriptions by age group. Use mouse we can find out the information of percentage occupation of union description for certain age. For example, for age 22, Administrative Clerical, Exempt/Elected/Appointed, Service/Maintenance and UCHC Univ Hlth Professional these four union share the total Log of gross almost equally, while for age 81, the only union is Legislative Management. 10.3.2 Sex par(mfrow = c(1, 2)) # density plot for each sex df1 &lt;- salary[which(salary$Sex == &quot;F&quot;), ] df2 &lt;- salary[which(salary$Sex == &quot;M&quot;), ] plot(density(df1$Gross.Log), col = &quot;red&quot;, xlab = &quot;Log of Gross&quot;, main = &quot;Density plot for each sex&quot;) lines(density(df2$Gross.Log), col = &quot;blue&quot;) legend(&quot;topright&quot;, c(&quot;M&quot;,&quot;F&quot;), lty = c(1, 1), col = c(&quot;blue&quot;, &quot;red&quot;)) # LSD comparison for sex grouop model &lt;- aov(salary$Gross.Log~salary$Sex) res &lt;- LSD.test(salary$Gross.Log, salary$Sex, DFerror = model$df.residual, MSerror=anova(model)[[&quot;Mean Sq&quot;]][2]) gross &lt;- round(res$groups[, &quot;salary$Gross.Log&quot;], 2) plot(res, xlab = &quot;Sex Group&quot;, ylab = &quot;Range between max and min&quot;, main = &quot;Sex groups and variation range&quot;) text(x = c(seq(from = 1, to = 10, by = 1.2)), y = 8, gross) Figure 10.8: Log of gross comparison by sex group Density plot shows blue male line is later than red female line, which means male has bigger log of gross than female in total. The density peak of female is earlier than male also tell the same story. The total different alpha beta index indicates that these 2 sex groups are significantly different from each other in mean log of gross, male get higher gross income than female. # p1 produce the point and line plot p1 &lt;- salary %&gt;% group_by(Ethnic.Grp, Sex) %&gt;% summarise(Avg.Gross = mean(Gross.Log)) %&gt;% ggplot(aes(x = Ethnic.Grp, y = Avg.Gross, colour = Sex, group = Sex)) + geom_line(size = 1) + geom_point(size = 4, shape = 19) + theme(axis.text.x = element_text(angle = 45, hjust = 1), panel.grid.major = element_blank(), legend.position = &quot;bottom&quot;) + labs(x = &quot;Ethnic Group&quot;, y = &quot;Average Gross Log&quot;) # p2 produce the ensity ridge plot library(ggridges) p2 &lt;- salary %&gt;% group_by(Ethnic.Grp, Sex) %&gt;% ggplot(aes(x = Gross.Log, y = Ethnic.Grp, fill = Sex)) + geom_density_ridges(alpha = 0.55) + theme(legend.position = &quot;bottom&quot;) + xlim(6.0, 10.0) # Lay out p1 and p2 in same row library(gridExtra) grid.arrange(p1, p2, nrow = 1) Figure 10.9: Log of gross comparison by ethnic and sex group Both plots tell us that for other ethnic group except Asian, male have higher average gross than female. On average, Asian female earn similar as male. Amind male have more bigger log of gross than Amind female. From the point and line plot we can see the same situation for Pacif ethnic. Because of the tiny amount of PACIF employees, there is no ridge showing in the plot. # Standardize Gross.Log first. Separate into two groups &quot;Below average&quot; and &quot;Above average&quot; Sd.Gross &lt;- round((salary$Gross.Log - mean(salary$Gross.Log))/sd(salary$Gross.Log), 2) Gross.Type &lt;- ifelse(Sd.Gross &lt; 0, &quot;Below average&quot;, &quot;Above average&quot;) Sex &lt;- salary$Sex Category &lt;- salary$Union.Descr df &lt;- data.frame(Category, Sd.Gross, Gross.Type, Sex) ggplot(df, aes(x = Category, y = Sd.Gross)) + geom_bar(stat = &#39;identity&#39;, aes(fill = Gross.Type), width = .5) + facet_wrap(~ Sex) + labs(x = &quot;Union Descreption&quot;, y = &quot;Standardized log of gross&quot;) + coord_flip() + theme(plot.title = element_text(hjust = 0.5), legend.position = &quot;top&quot;) Figure 10.10: Diverging bars of stanardized log of gross income From the figure we can see that for both male and female, the pattern of above and below average for almost all of the union description are similar. The only difference is the difference in total standardized log value of gross for male and female. For example, for union UCHC Univ Hlth Professionals on top, administrative Clerical on the bottom and other two unions like service/maintenance and Correctional Officers, all these four unions have more standardized log value of gross below average, on the same time, female UCHC Univ Hlth Professionals, administrative Clerical and male service/maintenance and Correctional Officers get this value less comparing with the other gender in the same union. But for union Uconn Faculty and Engineer Scien Tech, or Health professional and Judicial Professional, more standardized log value of gross above average, but female Uconn Faculty and Engineer Scien Tech and male Health professional and Judicial Professional get less than the other gender in the same union. 10.3.3 Ethnic Group # Bar plot with error bar for different ethnic group p1 &lt;- salary %&gt;% group_by(Ethnic.Grp) %&gt;% summarise(mean = mean(Gross.Log), sd = sd(Gross.Log), se = sd(Gross.Log)/sqrt(n())) %&gt;% ggplot(aes(x = Ethnic.Grp, y = mean, color = Ethnic.Grp, fill = Ethnic.Grp)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin =mean - se, ymax =mean + se), color = &quot;black&quot;, width = 0.3) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Density ridges for different ethnic group library(ggridges) p2 &lt;- salary %&gt;% group_by(Ethnic.Grp) %&gt;% ggplot(aes(x = Gross.Log, y = Ethnic.Grp, color = Ethnic.Grp, fill = Ethnic.Grp)) + geom_density_ridges(alpha = 0.5) + xlim(6.0, 10.0) + coord_flip() + theme(legend.position = &quot;none&quot;) # display two plots in same row grid.arrange(p1, p2, nrow = 1) Figure 10.11: Bar and density ridge plot of log of gross by ethnic From the bar figure we can see that Asian and White have relatively high gross incomes, Pacif and unknown get lower gross incomes. There is no big difference of variance for all other ethnic groups but Pacif, next Amind. Density ridge plot shows White, Asian and Nespec have biggest density above log of gross of 8.5, while unknown, Amind and Hispa have more density below 8. Ethnic Black’s density keep even at both sides of log of gross of 8. # get ethnic order according frequency of count CountEthnic &lt;- data.frame(sort(table(salary$Ethnic.Grp), decreasing = TRUE)) # get ethnic order according log of gross income GrossEthnic &lt;- salary %&gt;% group_by(Ethnic.Grp) %&gt;% summarise(Avg.Gross = mean(Gross.Log)) %&gt;% arrange(desc(Avg.Gross)) %&gt;% pull(Ethnic.Grp) # build data frame for rank of these two ethnic order EthnicGroup &lt;- unlist(list(CountEthnic[, 1], GrossEthnic)) Rank &lt;- rep(1:8, 2) EthnicType &lt;- rep(c(&quot;CountEthnic&quot;,&quot;GrossEthnic&quot;), each = 8) dat &lt;- data.frame(Rank, EthnicType, EthnicGroup) # point plot two set of ethnics according rank and connect same ethnic by line ggplot(dat, aes(y = Rank, x = EthnicType, label = EthnicGroup, group = EthnicGroup) )+ geom_line(size = 1) + geom_point(aes(color = EthnicGroup), size = 5) + scale_y_continuous(breaks = seq(0,10,1)) + annotate(&quot;text&quot;, x = 0.7, y = Rank[1:8], label = EthnicGroup[1:8], hjust = 0, cex = 3.5) + annotate(&quot;text&quot;, x = 2.1, y = Rank[9:16], label = EthnicGroup[9:16], hjust = 0, cex = 3.5) Figure 10.12: Ethnic rank plot by count and log of gross According to count, most of the state employees are white and Black. Hispa and Nspec occupy the third and fourth in the list. By average gross, the first Ethnic group is Asian, then Nspec and White. Black is in 4th place by gross. Pacif keeps the last for both count and average gross rank. model &lt;- aov(Gross.Log ~ Ethnic.Grp, data = salary) tukey &lt;- TukeyHSD(model) par(mar = c(4, 10, 2, 1)) psig = as.numeric(apply(tukey$Ethnic.Grp[, 2:3], 1, prod) &gt;= 0) + 1 plot(tukey, col = psig, las = 1, cex.axis = 0.7, yaxt = &quot;n&quot;) for (j in 1:length(psig)){ axis(2, at = j,labels = rownames(tukey$Ethnic.Grp)[length(psig) - j + 1], las = 1, cex.axis = .8, col.axis = psig[length(psig) - j + 1]) } There is no significant difference between the mean grosses of black pairs but the red pairs in plot: unknown with Asian, Black, Nspec and White; Asian with Black, Hispa and White; Black with Hispa and White; Hispa with Nespa and White. ggplot(salary, aes(x = Age, y = Gross.Log, colour = Sex, shape = Sex)) + geom_point() + ggtitle(&quot;Scatterplot of log of gross vs Age by Sex and Ethnic&quot;) + facet_wrap(~ Ethnic.Grp) + # make subplots by Ethnic Group geom_smooth(aes(colour = Sex), method = &#39;lm&#39;,formula = y ~ x) + # add the regression line theme(plot.title = element_text(hjust = 0.5), legend.position = c(0.85, 0.15)) In our data set, we have much more employees in white, but really little in Pacif. For both male and female, with increasing in age, log of gross increase slightly for all other ethnics but Nspec. For most ethnics, male and female have similar pattern for the relation between age and log of gross. For ethnic unknown and Amind, male’s log of gross increase faster than female, but for Asian, male’s log of gross increase slower than female. 10.4 Analysis of gross income type 10.4.1 Gross Type According to the average log of gross for each employee, we separate the employees into three group: High.Gross with average log of gross greater than 8.5, Middle.Gross with average log of gross greater than 7.8, and Low.Gross with average log of gross equal or less than 7.8. #regroup gross by average log of gross for each EmplId.Empl.Rcd as Gross.Type High.Gross &lt;- as.vector(salary$EmplId.Empl.Rcd[which(salary$Gross.Log &gt; 8.5)]) Middle.Gross &lt;- as.vector(salary$EmplId.Empl.Rcd[which(salary$Gross.Log &gt; 7.8)]) salary &lt;- salary %&gt;% mutate(Gross.Type = ifelse(EmplId.Empl.Rcd %in% High.Gross, &quot;High&quot;, ifelse(EmplId.Empl.Rcd %in% Middle.Gross, &quot;Middle&quot;, &quot;Low&quot;))) For each gross type, we try to get information about total number of observation, percentage, employee number, average log of gross, average of total gross and standardized total gross. library(janitor) df1 &lt;- salary %&gt;% group_by(Gross.Type) %&gt;% tabyl(Gross.Type) df2 &lt;- salary %&gt;% group_by(Gross.Type) %&gt;% summarise(Avg.Log.Gross = mean(Gross.Log), Avg.Tot.Gross = mean(Tot.Gross)) df &lt;- merge(df1, df2, by=&quot;Gross.Type&quot;) %&gt;% arrange(desc(Avg.Log.Gross)) df &lt;- round(df[, 2:5], 2) # round all numeric variables to decimal 2 Gross.Type = c(&quot;High, Gross.Log&gt;8.5&quot;, &quot;Middle, Gross.Log&gt;7.8&quot;,&quot;Low, Gross.Log&lt;=7.8&quot;) df &lt;- cbind(Gross.Type, df) kable(df, booktabs=TRUE, caption = &quot;Table of gross type&quot;) Table 10.2: Table of gross type Gross.Type n percent Avg.Log.Gross Avg.Tot.Gross High, Gross.Log&gt;8.5 217 0.06 8.72 6307.37 Middle, Gross.Log&gt;7.8 2329 0.68 8.11 3379.69 Low, Gross.Log&lt;=7.8 895 0.26 7.62 2065.61 df$Gross.Type &lt;- factor(df$Gross.Type, levels = df$Gross.Type) ggplot(df, aes(x = Gross.Type, label = paste(Avg.Tot.Gross, paste(percent*100, &quot;%&quot;, sep = &quot;&quot;), sep = &quot;, &quot;))) + geom_bar(aes(y = n), stat = &quot;identity&quot;, fill = rainbow(3)) + geom_line(aes(y = Avg.Log.Gross*250, group = 1, colour = &quot;Avg.Tot.Gross*250&quot;), size = 2) + geom_point(aes(y = Avg.Log.Gross*250, group = 1, colour = &quot;Avg.Tot.Gross*250&quot;), size = 4) + labs(x = &quot;Gross Type&quot;, y = &quot;Employee Number&quot;, title = &quot;Average gross and employee distribution for each gross type&quot;) + theme(plot.title = element_text(hjust = 0.5)) + geom_text(x= c(1, 2, 3), y = c(2000, 1900, 1800), size = 4, color = &quot;black&quot;) High gross group occupy 6% of the total employees. Around 68% employees earned middle gross, and around 26% employees earned low. The gross decrease straightly. The real average total gross change from 6307 dollars/Bi-weekly to 3380, then to 2066 for high, middle and low gross group. The average real gross income for high gross group is more than 3 times of that for low gross group. 10.4.2 Ethnic and Sex and age salary %&gt;% mutate(EthnicSex = paste(Ethnic.Grp, Sex, sep = &quot;-&quot;)) %&gt;% tabyl(EthnicSex, Gross.Type, show_missing_levels = FALSE) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_totals(&quot;col&quot;) %&gt;% adorn_percentages(&quot;all&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% adorn_ns %&gt;% adorn_title ## Gross.Type ## EthnicSex High Low Middle Total ## AMIND-F 0.0% (0) 0.1% (5) 0.1% (3) 0.2% (8) ## AMIND-M 0.0% (1) 0.0% (0) 0.1% (5) 0.2% (6) ## ASIAN-F 0.3% (10) 0.3% (11) 1.0% (36) 1.7% (57) ## ASIAN-M 0.2% (7) 0.2% (6) 1.1% (39) 1.5% (52) ## BLACK-F 0.3% (10) 2.5% (86) 5.8% (199) 8.6% (295) ## BLACK-M 0.3% (11) 1.2% (42) 4.0% (137) 5.5% (190) ## HISPA-F 0.1% (3) 1.7% (60) 2.8% (95) 4.6% (158) ## HISPA-M 0.1% (3) 1.4% (47) 2.2% (75) 3.6% (125) ## NSPEC-F 0.2% (7) 1.0% (33) 2.9% (101) 4.1% (141) ## NSPEC-M 0.2% (8) 0.8% (26) 1.8% (61) 2.8% (95) ## PACIF-F 0.0% (0) 0.0% (1) 0.0% (0) 0.0% (1) ## PACIF-M 0.0% (0) 0.0% (0) 0.0% (1) 0.0% (1) ## unknown-F 0.3% (10) 1.9% (64) 1.7% (57) 3.8% (131) ## unknown-M 0.2% (6) 1.2% (41) 1.0% (33) 2.3% (80) ## WHITE-F 1.1% (37) 7.6% (261) 20.8% (715) 29.4% (1013) ## WHITE-M 3.0% (104) 6.2% (212) 22.4% (772) 31.6% (1088) ## Total 6.3% (217) 26.0% (895) 67.7% (2329) 100.0% (3441) In high gross type, white male are occupy much bigger portion than any other male or female ethnic group. For other ethnic group with high gross income, female portion is equal or higher than male. Situation keeps similar as high gross type for middle gross type, white and Asian male occupy more portion than white and Asian female, while for other ethnics, female is more than male. For low gross type, no matter the ethnic the employees belong, female is more than male. In total, only for white, its male is more than female, for all other ethnics, female is more than male. For all other ethnics, over half of the employees belong to middle gross type, but unknown group have more people in low gross type. library(ggmosaic) h_mosaic &lt;- ggplot(data = salary) + geom_mosaic(aes(x = product(Sex, Ethnic.Grp), fill = Age.), na.rm=T, divider=mosaic(&quot;h&quot;)) + theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, size=6), legend.position=&quot;top&quot;,legend.text=element_text(size=8), panel.background = element_rect(fill=&quot;black&quot;), panel.grid.major = element_blank()) + labs(x = &quot;&quot;, y = &quot;&quot;, title= &quot;Mosaic Plot for Gross Group by Ethnic and Sex Group&quot;) + facet_grid(Gross.Type ~ .) ggplotly(h_mosaic, width = 800, height = 500) %&gt;% layout(legend = list(orientation = &quot;h&quot;, y = 1)) Here is the mosaic plot of each sub group we discussed in last chunk. We add age group as another factor. More elder employees above 50 get higher gross income, especially the male elders aged between 70 and 80. 10.5 LM and ANOVA analysis According to our previous analysis we try to fit a model for log of gross with Age., Ethnic.Grp, Sex, and Union description. Here we try use the average log of gross for each employee. salaryglm &lt;- salary[, -c(1, 6:7, 9:10)] # remove first col. of EmplId.Empl.Rcd model0 &lt;- lm(Gross.Log ~ ., data = salaryglm) ss &lt;- coef(summary(model0)) # take only coeficient of summary ss.sig &lt;- ss[ss[, &quot;Pr(&gt;|t|)&quot;] &lt; 0.05, ][1:20, ] # show first 20 coeficient with p value less than 0.05 ss.sig ## Estimate Std. Error ## (Intercept) 7.53153247 0.03449654 ## SexM 0.06950292 0.00926251 ## Ethnic.GrpAMIND 0.21998688 0.06774550 ## Ethnic.GrpASIAN 0.19194372 0.03354914 ## Ethnic.GrpBLACK 0.15780719 0.02657445 ## Ethnic.GrpHISPA 0.11448845 0.02776897 ## Ethnic.GrpNSPEC 0.13227360 0.02551390 ## Ethnic.GrpWHITE 0.14666022 0.02461445 ## Union.DescrAdministrative Clerical (NP-3) -0.39303463 0.02210547 ## Union.DescrAmercan Fed of School Admin 0.41117127 0.11893504 ## Union.DescrComm College Admin - CCCC -0.22235053 0.03617872 ## Union.DescrComm College Faculty CCCC -0.09661379 0.03681822 ## Union.DescrComm College Mgmt Exclusions 0.54115377 0.16743739 ## Union.DescrConfidential -0.16454268 0.04305299 ## Union.DescrConn Assoc Prosecutors 0.29935059 0.05673950 ## Union.DescrConnecticut Innovations Inc 0.55456110 0.09265495 ## Union.DescrCorrectional Officers (NP-4) -0.09952380 0.02206728 ## Union.DescrCorrectional Supervisor (NP-8) 0.16602156 0.04387205 ## Union.DescrCriminal Justice Residual -0.38863702 0.08485137 ## Union.DescrEducation A (P-3A) 0.21950893 0.06301334 ## t value Pr(&gt;|t|) ## (Intercept) 218.327176 0.000000e+00 ## SexM 7.503682 7.890072e-14 ## Ethnic.GrpAMIND 3.247255 1.176634e-03 ## Ethnic.GrpASIAN 5.721272 1.149428e-08 ## Ethnic.GrpBLACK 5.938305 3.171674e-09 ## Ethnic.GrpHISPA 4.122891 3.831477e-05 ## Ethnic.GrpNSPEC 5.184375 2.294737e-07 ## Ethnic.GrpWHITE 5.958298 2.810636e-09 ## Union.DescrAdministrative Clerical (NP-3) -17.779971 1.118520e-67 ## Union.DescrAmercan Fed of School Admin 3.457108 5.527464e-04 ## Union.DescrComm College Admin - CCCC -6.145893 8.878783e-10 ## Union.DescrComm College Faculty CCCC -2.624075 8.727566e-03 ## Union.DescrComm College Mgmt Exclusions 3.231977 1.241181e-03 ## Union.DescrConfidential -3.821864 1.348327e-04 ## Union.DescrConn Assoc Prosecutors 5.275877 1.404436e-07 ## Union.DescrConnecticut Innovations Inc 5.985229 2.386971e-09 ## Union.DescrCorrectional Officers (NP-4) -4.510017 6.702718e-06 ## Union.DescrCorrectional Supervisor (NP-8) 3.784222 1.568656e-04 ## Union.DescrCriminal Justice Residual -4.580209 4.812871e-06 ## Union.DescrEducation A (P-3A) 3.483531 5.011306e-04 # show Adjusted R-squared value statistic &lt;- paste(&quot;Adjusted R-squared&quot;,round(summary(model0)$adj.r.squared, 4), sep = &quot;: &quot;) statistic ## [1] &quot;Adjusted R-squared: 0.497&quot; model1 &lt;- aov(Gross.Log ~ ., data = salaryglm) summary(model1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Sex 1 4.83 4.833 87.14 &lt;2e-16 *** ## Ethnic.Grp 7 13.90 1.986 35.81 &lt;2e-16 *** ## Union.Descr 53 153.71 2.900 52.29 &lt;2e-16 *** ## Age 1 19.54 19.544 352.35 &lt;2e-16 *** ## Residuals 3378 187.37 0.055 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 LM model indicate that Average Gross is related with all factors including Age, Ethnic group, Sex, and Union description. ANOVA test illustrates Average Gross is significantly affected by all four factors, especially by age and sex. 10.6 Conclusion Total Gross is not statistically normally distributed. It skewed to left. After taking log transformation, it is more close to normality. The elder the age, the higher the gross income, except for age group between 80 and 90. On average, males earn more than females in the same age, ethnic, union. The means of gross are significantly different for different ethnic groups. On average, Asian and white are the top two groups. PACIF and unknown are the last. The real average total gross change from 6307 dollars bi-weekly to 3380, and to 2066 for high , middle and low gross group. High, middle and low gross group occupy 6%, 68%, and 26% of the total employees. White male occupy much bigger portion in high gross group, especially the elder white male. Age group, Union description, Sex, and Ethnic group together will explained 50% of log variance of gross. Exercise 10.1 Create a folder, download the data file “state_empoyee_salary_data_2017.csv” into it, and create an Rstudio project in the folder. Read in the data file, peak the structure of your data set and clean it step by step: Select only columns of “EmplId.Empl.Rcd”, “Bi.Weekly.Comp.Rate”, “Age”, “Ethnic”, “Sex”, “Full.Part”, “City”, get summary information for all these variables. Make sure EmplId.Empl.Rcd, sex, ethnic to be factor, age and biweekly rate to be numeric. Pick up full time employees in city “Hartford”. Select only those employees with 26 pay checks for 2017 ficial year(Hint:Each employee has unique ID of “EmplId.Empl.Rcd”). Remove sex group other than “F” and “M” if there is any. Hint:Use str() after droplevels(). Replace level of empty space in Ethnic.Grp as “unknown”. Remove emplolyees with Bi.Weekly.Comp.Rate equal or less than zero if there is any. Group your data set by EmplId.Empl.Rcd, Ethnic.Grp, Sex. For each employee, take round of mean age and mean of Bi.Weekly.Comp.Rate as Age and Bi.Weekly.Rate for your data set. Show structure and summary of your data set. Show the distribution of “Bi-Weekly Comp Rate” and “Age” using histogram and QQ plot. Test the normality. Transform by taking log value if it’s necessary. Using pie plot to show the distribution of Sex. List the count number and percent of employees in each ethnic group. Using scatter plot to view the relationship between average Bi-Weekly Comp Rate and Age, add regression line and give your interpretation. Using box plot to view the relationship between average Bi-Weekly Comp Rate by Ethnic group. Using density plot to view the distribution of Bi-Weekly Comp Rate for different sex group, package ggplot2 is refered. Do ANOVA test to check whether male are the same as female in Bi-Weekly Comp Rate in city “Hartford”. Explore the interaction of ethnic and sex to Bi-Weekly Comp Rate using density ridges plot and interpret your plot. "]
]
